Dataset: Digits
--------------------------------------------
Model Name: MVAE_EM_Prior_10_Latent_dim_8_Soft_Digits
==================================================================
MVAE_EM_Prior_10_Latent_dim_8_Soft_Digits Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 10
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 1500
--------------------------------------------
Learning Rate = 0.001
--------------------------------------------
Step Size = 30
--------------------------------------------
Input Dimension = 64
--------------------------------------------
Encoder Hidden Dimension 1 = 256
--------------------------------------------
Encoder Hidden Dimension 2 = 256
--------------------------------------------
Latent Dimension = 8
--------------------------------------------
Decoder Hidden Layer 1 = 256
--------------------------------------------
Decoder Hidden Layer 2 = 256
--------------------------------------------
Output Dimension = 64
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 50
--------------------------------------------
Batch Size (Test) = 50
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 1501
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-24.6465, -28.8905, -17.2595, -17.8347, -15.9862, -26.1952, -16.2432,
         -23.8761, -16.1032, -17.3448],
        [-24.4686, -24.7711, -26.2645, -21.9664, -26.2432, -20.7997, -23.2338,
         -22.5951, -20.9083, -23.9048],
        [-21.0417, -31.2541, -21.3603, -21.8918, -21.2556, -30.8897, -24.2156,
         -21.2887, -26.4612, -21.3453],
        [-25.0746, -28.5008, -22.5648, -23.2675, -26.5428, -24.0336, -22.1989,
         -21.9917, -24.9557, -24.1793],
        [-23.3615, -26.8021, -23.0742, -29.5767, -25.7652, -28.4302, -22.2664,
         -25.4007, -30.3061, -23.6286],
        [-50.3246, -20.4939, -48.4414, -40.2926, -20.4060, -26.3061, -36.4486,
         -29.4340, -33.8002, -32.3206],
        [-30.9651, -31.8452, -19.0655, -44.1948, -31.2061, -37.3862, -32.5455,
         -13.1691, -44.3897, -48.3296],
        [-30.7665, -39.3621, -45.7933, -21.6310, -21.7707, -24.1690, -21.3814,
         -44.4853, -21.0790, -20.7817],
        [-33.0553, -18.5555, -49.3801, -27.5529, -17.4920, -23.9897, -25.4140,
         -28.4523, -30.9824, -47.6755],
        [-21.3523, -23.9037, -22.4186, -21.2099, -18.9674, -22.4520, -23.7668,
         -25.5363, -22.2559, -21.5912],
        [-43.6327, -37.0161, -23.3127, -34.1047, -32.0464, -36.0097, -28.3937,
         -18.7543, -29.3430, -31.2501],
        [-20.6531, -18.0378, -20.1052, -19.0512, -17.6198, -59.8578, -19.2607,
         -34.1604, -30.9131, -22.0652],
        [-20.8438, -29.0534, -21.5321, -26.6425, -28.8113, -29.9583, -28.9928,
         -31.6052, -24.6894, -26.9091],
        [-48.9320, -49.4792, -34.1395, -21.5638, -26.1022, -21.2149, -58.2218,
         -85.4332, -22.1478, -62.2315],
        [-20.9692, -20.8773, -43.6081, -22.5545, -22.0359, -64.9284, -37.2225,
         -30.4480, -37.5926, -26.1097],
        [-41.8874, -71.0583, -30.7776, -19.9553, -21.9869, -18.9467, -22.5410,
         -26.0154, -21.5500, -21.4143],
        [-42.0320, -18.4275, -53.5749, -31.7088, -19.2938, -29.1903, -32.2289,
         -20.8351, -37.5860, -38.7842],
        [-50.2675, -18.6676, -35.8374, -38.2057, -21.1981, -72.1617, -22.7695,
         -27.5353, -42.6835, -35.6453],
        [-18.0775, -24.7734, -18.7209, -19.5120, -26.8364, -31.7300, -15.9093,
         -18.1000, -20.9559, -16.0080],
        [-30.0737, -36.5453, -22.9901, -23.7535, -53.6569, -16.4407, -33.9080,
         -30.6138, -29.1829, -34.9646],
        [-28.4727, -25.4155, -18.8281, -23.1611, -22.1429, -29.4133, -26.1905,
         -18.5279, -58.6646, -19.6023],
        [-25.8711, -38.7235, -30.8721, -20.8640, -25.1445, -26.1001, -27.8832,
         -42.3004, -22.4201, -24.9143],
        [-25.1229, -29.0680, -23.3627, -30.9769, -23.1030, -29.1125, -21.9948,
         -20.0751, -24.2507, -27.9348],
        [-21.9362, -28.7559, -22.3445, -19.8408, -22.2688, -20.3638, -23.0484,
         -21.3447, -20.5250, -24.8094],
        [-26.4837, -16.4017, -25.1329, -23.1783, -20.1997, -54.6667, -24.2883,
         -30.2885, -21.1503, -34.2573],
        [-17.3060, -16.6972, -19.7906, -39.9487, -16.1210, -20.6771, -18.8143,
         -29.6574, -22.5903, -27.7770],
        [-23.1784, -21.2930, -15.5709, -20.8649, -25.3496, -30.5684, -20.2331,
         -12.4084, -23.0294, -20.8908]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([27, 10, 8]), scale: torch.Size([27, 10, 8]))
=============================================================================================
Decoder Loss (Test) = tensor([[-21.3409, -19.1364, -22.1472, -22.3274, -20.0789, -32.7360, -19.2606,
         -38.6385, -32.9178, -22.7430],
        [-29.9698, -20.7398, -45.7738, -39.7304, -26.4270, -93.3137, -35.9234,
         -78.5726, -50.0798, -44.4440],
        [-17.2039, -17.2596, -17.8205, -19.8358, -18.8597, -18.1639, -24.9388,
         -18.7033, -33.1514, -18.7594],
        [-35.5322, -41.3427, -20.7017, -27.8527, -30.8241, -30.7155, -23.1362,
         -20.1611, -33.0622, -26.6830],
        [-33.1878, -41.7251, -26.2234, -28.1912, -31.2697, -26.9525, -29.5495,
         -24.8644, -31.1009, -27.0981],
        [-31.9828, -38.8025, -16.9329, -30.0427, -29.1268, -31.8544, -19.1312,
         -16.5661, -38.5016, -25.3625],
        [-35.3293, -40.7502, -20.9612, -28.9405, -32.3770, -29.8862, -25.5595,
         -19.5289, -32.3295, -28.9272],
        [-27.4742, -32.4765, -23.4903, -20.9418, -24.4979, -20.6295, -22.0459,
         -22.7174, -20.5629, -21.5258],
        [-16.9924, -17.2795, -17.0031, -18.1996, -19.0454, -19.4926, -24.8142,
         -17.6423, -25.9983, -17.6676],
        [-35.2428, -38.0853, -30.0017, -21.6531, -28.4937, -21.4544, -23.5153,
         -30.2317, -23.0483, -24.5240],
        [-21.7024, -27.7936, -22.5343, -19.6068, -20.4157, -20.2066, -19.9676,
         -24.6351, -20.1313, -19.6755],
        [-20.2646, -17.2057, -29.6270, -24.3020, -17.7954, -38.9136, -20.5109,
         -40.8934, -29.9131, -27.4884],
        [-30.8993, -28.3658, -19.1112, -22.8527, -22.0205, -25.4093, -20.0569,
         -19.1105, -26.1382, -20.3926],
        [-29.3679, -25.6142, -20.4163, -21.4949, -25.8810, -24.8499, -20.2725,
         -20.2725, -27.8107, -21.9068],
        [-17.6742, -15.7849, -22.4448, -19.0090, -17.2571, -34.7684, -17.4239,
         -35.1361, -26.4935, -20.5808],
        [-28.9634, -30.4622, -23.5233, -17.9753, -24.1099, -18.3166, -26.8214,
         -21.3966, -20.0669, -21.3248],
        [-44.5481, -20.6437, -20.5357, -21.7162, -19.5129, -25.8708, -20.8432,
         -22.4297, -22.6570, -21.1712],
        [-35.0352, -35.2188, -18.2782, -20.4922, -25.7341, -19.9978, -20.7934,
         -18.3831, -30.5876, -19.9065],
        [-34.5613, -30.5072, -25.3195, -19.9142, -28.6127, -20.3938, -29.1439,
         -23.2029, -25.0532, -23.8108],
        [-59.8645, -60.3417, -26.9792, -30.6065, -40.7103, -29.4778, -30.0101,
         -27.6785, -35.6517, -29.2236]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([20, 10, 8]), scale: torch.Size([20, 10, 8]))
=============================================================================================
