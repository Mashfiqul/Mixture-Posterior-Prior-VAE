Dataset: USPS
--------------------------------------------
Model Name: MVAE_EM_Prior_20_Latent_dim_5_Soft_USPS
==================================================================
MVAE_EM_Prior_20_Latent_dim_5_Soft_USPS Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 20
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 30
--------------------------------------------
Input Dimension = 256
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 256
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[ -200.7715,  -104.8591,  -222.7312,  -104.2030,  -194.3178,  -374.5953,
          -106.4644,  -125.7616,  -109.3141,  -104.8089],
        [  -44.9240,   -54.5901,   -40.1642,   -50.3847,  -270.3869,   -60.3926,
           -40.1407,  -268.6932,   -43.7110,   -47.7696],
        [ -200.5146,  -103.7294,  -160.2409,  -110.2703,  -137.5916,  -318.2083,
          -107.5086,  -102.3031,  -107.4569,  -105.0838],
        [  -79.2222,   -74.9463,   -97.1727,   -79.1439,  -177.7782,  -260.0204,
           -76.1000,  -373.6104,   -79.8920,   -76.3400],
        [ -134.8445,   -74.0728,  -106.0141,   -75.8168,  -152.8450,  -288.5858,
           -71.4189,  -342.9917,   -72.8524,   -71.4311],
        [ -128.5078,  -118.3483,  -186.5619,  -123.1863,  -196.2545,  -407.5756,
          -119.6795,  -880.3878,  -108.5727,  -106.2183],
        [ -222.6126,  -109.3631,  -202.2345,  -108.3051,  -217.4977,  -269.8611,
          -113.5140,  -111.3344,  -119.1887,  -107.5235],
        [  -65.9219,   -64.8860,   -71.9317,   -67.1001,  -194.0688,  -124.0927,
           -64.9922, -1241.3201,   -71.1663,   -65.9723],
        [  -88.3730,   -80.9086,   -99.8256,   -83.9877,  -181.6132,  -260.1483,
           -83.3116, -3362.6167,   -85.5518,   -83.6996],
        [ -109.2475,   -81.2557,  -115.0874,   -82.4774,  -261.2507,  -317.9562,
           -81.8538, -1178.8256,   -88.3554,   -90.8060],
        [ -144.0208,   -96.5822,  -133.3794,   -97.8286,  -228.5510,  -288.3253,
           -95.9385,  -554.7188,   -96.6585,  -100.8594],
        [ -197.6730,  -101.8193,  -175.2577,  -112.2450,  -152.0015,  -227.1456,
          -106.0062,  -103.9811,  -106.0453,  -102.5757],
        [ -157.3892,  -104.2556,  -159.3125,  -104.0524,  -225.3292,  -359.9003,
          -107.7451,  -539.1304,  -103.9653,  -104.3701],
        [  -89.0654,   -89.6961,   -92.7916,   -90.9437,  -205.7189,  -233.5985,
           -87.3236, -5363.1562,   -86.1795,   -88.1956],
        [ -150.0587,  -116.6218,  -163.5193,  -119.9562,  -309.2583,  -336.7820,
          -116.6518,  -821.8973,  -116.8090,  -116.8851],
        [ -155.1627,   -90.5309,  -164.5397,   -95.5338,  -164.5088,  -289.6406,
           -90.8784,   -92.8659,   -91.5234,   -90.5533],
        [  -67.0205,   -71.7262,   -73.2706,   -81.2855,  -142.6138,  -142.5179,
           -81.8552,  -214.4950,   -72.1001,   -74.3067],
        [  -76.8939,   -73.7748,   -79.8584,   -74.1779,  -222.2426,  -157.7651,
           -74.8232, -5321.3618,   -74.9694,   -74.7357],
        [  -84.8463,   -79.8591,   -85.7590,   -81.7961,  -181.6571,  -149.3676,
           -84.7316, -1210.1300,   -86.2563,   -79.6105],
        [ -117.6669,   -86.1523,  -127.0374,   -87.7466,  -219.8058,  -462.7961,
           -99.8690, -2654.4131,   -86.5739,   -89.6665],
        [ -217.2192,  -105.3260,  -176.0432,  -108.6527,  -168.1186,  -172.3709,
          -110.2728,  -107.9311,  -106.9337,  -106.3348],
        [ -208.6913,   -93.5463,  -123.6955,   -97.8202,  -154.8402,  -260.8240,
           -97.1478,   -92.3672,   -93.1890,   -93.8252],
        [ -223.7665,  -109.9569,  -159.0212,  -111.7948,  -160.6820,  -208.1534,
          -109.6821,  -112.9374,  -117.7724,  -110.7031],
        [ -199.5966,   -94.5484,  -187.2553,   -98.3437,  -187.8981,  -235.5869,
          -100.4273,  -109.9066,   -98.7189,   -96.5626],
        [ -123.8430,   -79.0279,  -124.5287,   -83.7211,  -155.6288,  -402.0925,
           -80.8024,  -103.8496,   -81.3279,   -80.1330],
        [  -65.3788,   -62.5958,   -61.4377,   -64.1948,  -216.9894,  -140.0958,
           -70.0209,  -318.6060,   -61.1453,   -60.4797],
        [ -220.0859,  -108.5640,  -164.8411,  -107.5035,  -167.0325,  -257.6174,
          -108.3840,  -100.7863,  -103.6643,  -100.9524],
        [ -232.5629,   -96.0462,  -188.0086,  -103.4393,  -202.5335,  -296.4187,
           -96.4824,  -112.2533,  -101.8565,   -94.3848],
        [ -222.9662,   -99.6535,  -162.0405,  -100.4768,  -149.0080,  -268.7329,
          -101.3089,  -102.7907,   -99.5384,   -99.6295],
        [  -87.7440,   -85.7257,   -95.3018,   -84.7927,  -166.6212,  -242.5360,
           -83.9227, -2309.9517,   -84.1639,   -88.5516],
        [  -65.1624,   -81.8533,   -67.9219,   -66.0637,  -215.8327,  -122.8514,
           -66.7283, -4295.1377,   -70.9856,   -86.5530],
        [  -49.1894,   -52.8030,   -46.6996,   -59.5232,  -137.5291,   -59.3106,
           -51.0306,  -458.7238,   -54.6076,   -54.0846],
        [  -93.7856,   -84.9087,  -107.3909,   -85.5661,  -183.8340,  -211.5660,
           -84.1254,  -493.1721,   -83.0425,   -83.0900],
        [  -73.1942,   -75.4439,   -74.0798,   -75.9537,  -184.4262,  -205.9623,
           -73.7481, -2839.3281,   -75.5758,   -73.0402],
        [  -53.4120,   -55.2602,   -48.9926,   -60.1816,  -178.3291,   -59.2457,
           -54.6531,  -314.4931,   -59.2948,   -53.4633],
        [ -159.0646,   -88.5077,  -164.7671,   -87.9983,  -151.0562,  -390.8661,
           -85.0593,   -96.2766,   -83.5551,   -86.1099],
        [  -67.4334,   -67.3777,   -63.8288,   -92.9466,  -150.8052,  -157.5557,
           -82.7999,  -292.7712,   -71.7632,   -70.7128],
        [ -116.4338,   -80.2673,  -115.7353,   -85.8705,  -205.1323,  -257.7853,
           -82.5828,  -157.5435,   -80.0887,   -81.8554],
        [ -158.3232,   -97.1558,  -175.5569,   -97.0992,  -146.3148,   -96.9001,
           -99.3083,   -98.4256,  -104.4355,   -96.5978],
        [  -96.9549,   -91.7189,   -94.6253,   -90.6703,  -181.7017,  -208.2811,
           -90.1052, -5215.2158,   -92.0779,   -90.7980],
        [  -98.5223,   -79.4677,   -88.9333,   -79.5103,  -224.4258,  -185.0389,
           -79.6152, -3470.2563,   -84.4244,   -80.6915],
        [ -184.3877,   -90.1401,  -128.6645,   -93.6783,  -188.8319,  -347.6713,
           -95.4734,   -91.7234,   -90.5593,   -91.8204],
        [  -73.3363,   -72.7094,   -75.3723,   -73.8215,  -198.8458,  -275.5180,
           -71.7236, -1313.5981,   -84.4630,   -72.4434],
        [  -86.4865,   -84.2823,  -109.9961,   -89.3771,  -221.1517,  -390.5147,
           -84.2538,  -368.0632,   -89.9120,   -88.2209],
        [ -141.1496,   -80.1481,  -126.1363,   -89.0758,  -167.5744,  -246.0856,
           -77.9861,   -87.4645,   -76.6895,   -78.3782],
        [  -71.3532,   -75.3528,   -87.7616,   -77.2010,  -183.6604,  -272.2994,
           -72.5617,  -623.2499,   -75.5347,   -76.3834],
        [ -151.7824,   -94.2379,  -183.4963,   -95.2285,  -251.5604,  -468.4872,
           -98.2196,  -479.1736,  -106.5867,  -100.3883],
        [ -212.0368,  -101.1693,  -212.0258,  -104.6855,  -177.9948,  -171.5348,
          -107.1055,  -103.5563,  -104.9106,  -102.4078],
        [  -44.1704,   -41.3387,   -40.4795,   -46.0189,  -137.0529,   -63.7098,
           -49.9415,  -321.8209,   -43.4512,   -44.4389],
        [  -78.3743,   -79.9511,   -78.0788,   -77.5513,  -171.3035,  -190.6439,
           -76.3642, -2189.8726,   -78.0504,   -79.1472],
        [ -131.1961,   -95.3132,  -166.0104,   -99.4567,  -270.3353,  -419.4839,
           -98.9589,  -620.3145,  -114.1833,   -95.5719],
        [  -72.6822,   -80.2111,   -88.7982,   -74.7581,  -135.4477,  -169.5121,
           -74.2265, -1520.5305,   -78.2581,   -75.2095],
        [  -48.7491,   -57.4126,   -47.1367,   -69.2621,  -187.2777,   -60.0253,
           -54.0321, -3153.8472,   -57.2689,   -53.0577],
        [  -92.1481,   -95.7832,  -116.0395,  -115.3556,  -263.9606,  -274.6644,
           -97.3762, -3127.5559,   -98.6862,   -98.5466],
        [ -105.2729,   -95.7274,  -110.0711,   -96.9489,  -198.7051,  -237.1964,
           -97.1043, -1281.5181,   -97.9143,   -95.8160],
        [ -128.2619,   -73.2695,   -94.4199,   -77.6537,  -155.9672,  -300.5161,
           -74.6085,  -391.9674,   -74.5267,   -74.4475],
        [ -121.3273,   -77.0552,  -104.8723,   -75.2787,  -174.9523,  -283.3870,
           -77.7252,  -174.5540,   -76.8012,   -78.4304],
        [  -70.4796,   -69.3460,   -69.3621,   -74.9273,  -135.5364,  -188.4705,
           -69.9799,  -283.3895,   -68.9789,   -74.6642],
        [  -93.9041,   -59.2928,   -94.3932,   -73.4000,  -168.2672,  -136.5703,
           -68.1713,  -140.1496,   -61.1814,   -57.9627],
        [ -205.8082,  -102.7769,  -146.6492,  -105.0536,  -158.0190,  -210.1806,
          -103.8944,  -101.3513,  -102.5639,  -102.7605],
        [ -153.4376,   -93.2039,  -190.6361,   -94.9615,  -232.9104,  -355.7173,
           -92.4898,  -423.9383,  -108.7237,   -99.1212],
        [  -74.9978,   -76.6901,   -74.7457,   -77.3819,  -201.8976,  -167.4786,
           -75.4870, -2231.4778,   -76.0874,   -75.0794],
        [  -46.2853,   -53.0077,   -43.4390,   -61.0383,  -297.4406,   -66.9772,
           -47.8436,  -532.4568,   -54.2044,   -47.6601],
        [  -90.8819,   -87.2275,   -88.8798,   -86.1073,  -175.0544,  -258.9305,
           -86.6850,  -362.8981,   -86.4631,   -93.0880],
        [ -117.4734,   -92.2149,  -123.8042,   -95.2339,  -184.4334,  -291.6640,
           -95.0330, -4182.6636,   -93.6417,   -92.7901],
        [ -109.0361,   -90.5206,  -109.1904,   -90.9470,  -193.0132,  -283.4845,
           -88.4313, -2344.0942,   -90.6421,   -88.1709],
        [ -216.7522,  -104.7452,  -137.4137,  -105.6475,  -152.1772,  -216.2588,
          -108.8244,  -107.1820,  -104.5314,  -102.6167],
        [  -67.2415,   -70.3687,   -78.8435,   -69.5038,  -169.1922,  -197.5293,
           -69.6247,  -497.0023,   -72.3441,   -71.5769],
        [ -127.4315,   -91.3654,   -94.7228,   -97.4855,  -205.5876,  -289.8983,
           -91.4692, -1173.2698,   -91.2800,   -88.8154],
        [ -138.4316,  -119.7346,  -169.1759,  -118.9534,  -225.1027,  -413.2235,
          -112.8276,  -811.1743,  -114.0225,  -110.8018],
        [  -75.8782,   -76.3703,   -82.6765,   -75.5820,  -129.3126,  -227.7399,
           -77.2714,  -320.3454,   -82.3065,   -82.6531],
        [  -45.6059,   -48.3977,   -43.7192,   -53.4358,  -231.0350,   -67.0678,
           -45.3973,  -270.5831,   -47.5930,   -57.4068],
        [ -128.7183,  -100.1266,  -128.5119,  -110.4168,  -194.8283,  -238.3276,
           -98.7563,  -441.8192,  -104.7186,   -99.3521],
        [ -171.1499,   -88.1006,  -150.6777,   -88.4850,  -170.4268,  -304.9101,
           -91.9753,   -91.8215,   -91.1283,   -89.1150],
        [ -179.9379,  -110.7122,  -178.7322,  -112.0797,  -190.4167,  -159.0827,
          -119.2304,  -126.7730,  -113.5817,  -114.5378],
        [ -113.9759,   -73.9189,  -105.7905,   -76.0439,  -163.3094,  -242.1591,
           -75.5686,  -114.2751,   -81.8108,   -76.0372],
        [  -50.0375,   -48.9474,   -47.8427,   -55.5778,  -146.4928,   -62.1078,
           -49.7211,  -776.2440,   -54.6324,   -52.2047],
        [ -119.1154,  -113.2902,  -265.5733,  -113.3492,  -231.8605,  -543.4408,
          -110.7369,  -393.9780,  -110.4578,  -110.3702],
        [  -46.5535,   -59.3229,   -44.2148,   -54.7027,  -142.3834,   -73.2137,
           -49.4702,  -370.3306,   -56.2170,   -51.0382],
        [  -67.3548,   -67.5380,   -76.6073,   -68.0873,  -118.1691,  -207.5718,
           -66.7143,  -475.4893,   -69.3234,   -69.3216],
        [  -82.9546,   -86.2063,   -84.4646,   -84.3243,  -315.3644,  -150.5312,
           -83.6448, -1532.7048,   -83.9002,   -82.1592],
        [ -127.6157,   -83.3851,  -118.1900,   -81.3610,  -225.1405,  -286.3349,
           -81.8150,  -798.1901,   -80.8377,   -80.6965],
        [  -71.0451,   -65.3048,   -68.9155,   -78.9216,  -157.9760,  -109.6775,
           -65.7880,  -351.5414,   -65.0954,   -66.1370],
        [  -56.8393,   -56.9151,   -57.6692,   -58.0123,  -119.4309,  -134.6114,
           -59.9142,  -333.3612,   -56.4546,   -61.9819],
        [  -99.6874,   -76.0946,  -172.1488,   -75.0861,  -170.1027,  -177.7098,
           -69.9564,  -259.8929,   -74.0786,   -74.2217],
        [ -101.2230,  -103.4852,  -127.2354,  -103.3218,  -181.7972,  -379.3802,
           -97.5437,  -537.5485,  -101.6284,   -96.5021],
        [  -70.6274,   -72.5676,   -69.8952,   -74.7769,  -334.5079,  -176.3430,
           -73.1933,  -771.5388,   -72.9405,   -77.2052],
        [ -148.6568,  -123.2447,  -175.5007,  -125.1105,  -385.8452,  -388.9480,
          -121.9185, -1828.9041,  -121.2180,  -123.1077],
        [  -82.8385,   -91.6640,   -83.3543,  -108.6459,  -193.3799,  -231.1617,
          -100.0877,  -453.8845,   -96.9511,   -99.7103],
        [ -164.7975,   -83.5076,  -144.1628,   -80.5760,  -192.2062,  -272.9229,
           -90.7754,   -86.7283,   -82.0062,   -81.8902],
        [ -124.6919,   -78.1670,  -118.9407,   -82.2652,  -179.9005,  -252.3566,
           -78.1812,  -121.4238,   -84.9896,   -78.7147]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([91, 10, 5]), scale: torch.Size([91, 10, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-152.6660, -104.1392, -162.5717, -105.5092, -156.1670, -104.4364,
         -104.6479, -104.2807, -104.4038, -104.0991],
        [ -55.0162,  -55.3610,  -53.9429,  -54.3700, -118.7732, -120.9090,
          -55.1882, -218.9837,  -56.2038,  -55.2230],
        [ -93.7669,  -78.7029, -100.1133,  -80.7065, -202.8838, -277.3199,
          -78.8654, -236.5960,  -78.0604,  -78.3235],
        [ -68.7860,  -69.7398,  -69.2612,  -70.7998, -146.4427, -196.5174,
          -68.8598, -262.4943,  -70.5808,  -69.1280],
        [ -99.4412,  -96.8704, -101.2655,  -96.1869, -253.5989, -279.7167,
          -96.8067, -240.1564,  -98.4232,  -97.8901],
        [-203.1750,  -95.2248, -142.2406,  -96.4162, -154.2206, -173.4437,
          -96.2077,  -95.4933,  -95.4732,  -95.1166],
        [ -50.9615,  -56.5832,  -47.8844,  -74.9571, -154.1629,  -57.3490,
          -56.1788, -220.7807,  -54.9924,  -55.7777]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([7, 10, 5]), scale: torch.Size([7, 10, 5]))
=============================================================================================
