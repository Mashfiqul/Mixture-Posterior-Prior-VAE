Dataset: USPS
--------------------------------------------
Model Name: MVAE_EM_Prior_100_Latent_dim_5_Hard_USPS
==================================================================
MVAE_EM_Prior_100_Latent_dim_5_Hard_USPS Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 100
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 30
--------------------------------------------
Input Dimension = 256
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 256
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-153.5225, -121.3244, -109.8151, -101.5709,  -97.1101, -174.9451,
          -99.0703,  -83.9271,  -95.4862,  -87.2065],
        [-125.0546, -125.7312, -130.6030, -145.9570, -112.6128, -124.3337,
          -93.8774, -100.4828, -106.9261,  -96.5107],
        [-148.8013, -140.8194, -118.0113, -144.2145, -118.2717, -147.7954,
         -102.3040, -118.0132, -106.3991, -101.7058],
        [-144.3290, -121.2571, -104.9080, -148.1829, -104.0675, -123.0649,
          -81.3330,  -84.0938,  -89.7277,  -80.5433],
        [-133.6698,  -83.1357,  -84.5082,  -79.2959,  -87.8998,  -76.2252,
          -99.9831,  -85.9579,  -90.8898,  -87.9493],
        [-241.1407, -180.3447, -188.7454, -155.4415, -150.6230, -223.9132,
         -138.8507, -122.7990, -134.3696, -109.2056],
        [-109.9483, -102.5165, -100.9030, -115.3383, -100.4556, -110.3745,
          -92.7552,  -92.4991,  -96.0766,  -91.8960],
        [-148.8148, -158.9251, -132.9771, -156.1498, -122.0815, -186.9165,
         -114.6004, -109.3257, -116.4175, -107.8867],
        [ -76.7333,  -65.8353,  -64.8475,  -68.7504,  -76.7904,  -59.1483,
          -65.8636,  -68.5196,  -73.0565,  -67.0047],
        [-129.8589, -107.6537, -104.3464,  -99.0615, -142.8409, -112.6113,
         -110.9191,  -86.2323,  -95.8403, -105.4784],
        [-113.9881, -109.8864, -117.8186, -138.9692, -109.1652, -109.8131,
         -106.0491, -108.7735, -104.2272, -103.5571],
        [-154.5859, -158.6401, -122.8000, -128.0404, -132.3612, -193.4281,
         -102.8959, -105.1106, -112.4160, -103.8482],
        [-124.4775, -129.9880, -112.6232, -134.7359, -111.9230, -124.1129,
         -101.3992, -105.6553, -101.9535,  -99.2039],
        [-120.7671,  -79.8448,  -73.2039,  -76.5412,  -75.8143,  -72.2591,
          -77.5520,  -77.4162,  -76.3927,  -83.0056],
        [-135.7178, -168.7773, -156.9147, -147.0939, -123.6072, -143.1237,
         -116.5159, -114.2680, -116.9219, -111.6933],
        [-131.7210, -155.6168, -122.5766, -156.1215, -125.3483, -163.7420,
         -102.6873, -112.7416, -104.6594,  -96.4791],
        [ -38.9016,  -38.6026,  -38.4722,  -39.3765,  -43.6170,  -39.3788,
          -47.5117,  -39.3887,  -38.7602,  -40.2113],
        [-127.5786,  -98.3405,  -99.6477, -114.9541,  -99.5994,  -96.3321,
          -77.3014, -111.6788,  -83.2038,  -77.7908],
        [ -98.8024,  -76.1875,  -72.4815,  -77.8800,  -79.8511,  -70.0281,
          -78.0507,  -79.0458,  -72.3163,  -73.4673],
        [ -40.2161,  -40.7945,  -40.6226,  -40.8294,  -46.2643,  -41.4633,
          -42.4621,  -41.3945,  -40.9716,  -41.4206],
        [-116.3345, -109.8554, -103.0047, -137.5264, -112.2510,  -98.2547,
          -83.0958,  -86.5974,  -83.5204,  -87.1642],
        [-119.7582,  -88.5492,  -90.7057,  -90.4037,  -89.0553,  -78.2666,
         -100.3758,  -93.1765,  -86.6920,  -93.2360],
        [-190.0137, -207.8319, -144.4429, -153.8444, -154.5474, -221.8289,
         -112.9611, -111.2521, -112.3250, -114.0046],
        [-158.0377, -132.8627, -133.4226, -169.3718, -130.6754, -107.3259,
         -177.8480, -122.9269, -110.4990, -118.7980],
        [ -98.5146,  -63.4477,  -66.1947,  -64.0993,  -77.3109,  -58.3654,
          -67.1843,  -60.3717,  -59.2298,  -61.7816],
        [-200.7480, -144.3490, -134.4791, -134.7439, -133.7913, -122.5249,
         -136.2465, -108.8330, -106.2564,  -99.5731],
        [-108.6838,  -90.9215,  -79.4797,  -80.9311,  -86.1932,  -67.2424,
          -72.8143,  -71.9283,  -73.9997,  -68.8113],
        [ -51.4183,  -55.0482,  -47.7727,  -47.7052,  -47.9614,  -52.2668,
          -54.1539,  -64.5592,  -53.6807,  -52.5696],
        [-254.5087, -257.6484, -158.1911, -217.1053, -190.5284, -232.1578,
         -108.6764, -106.5816, -108.2361, -103.8724],
        [-194.2756, -103.4649,  -97.9634, -116.6616,  -90.0199,  -85.2968,
          -74.9440, -100.1307,  -83.6794,  -81.8685],
        [-171.7028, -127.6545, -125.4848, -123.4299, -135.9115, -110.0280,
         -136.8768, -106.9334, -103.5946, -104.1995],
        [-183.0255, -126.5591, -109.6845, -190.2513, -108.9339, -131.4926,
          -87.4737, -103.1937,  -98.1058,  -86.9519],
        [-191.6952, -194.4892, -145.5455, -185.8119, -146.6036, -206.8292,
         -102.2704, -105.4834, -107.2395, -104.4690],
        [ -44.2320,  -44.9000,  -46.0654,  -44.8875,  -44.4920,  -48.2542,
          -57.4708,  -49.6064,  -54.6125,  -51.7513],
        [ -97.5653, -104.1853,  -95.5724,  -90.7590, -109.2826,  -87.9964,
          -78.8874,  -99.0027,  -89.9888,  -83.5673],
        [-103.1812,  -75.5952,  -81.6440,  -76.7815,  -75.9689,  -71.1428,
         -104.4837,  -86.3709,  -73.9438,  -80.5893],
        [ -38.7269,  -40.0974,  -39.8959,  -43.7642,  -46.1875,  -40.7409,
          -41.8019,  -43.9791,  -40.8757,  -55.0323],
        [ -89.4892,  -80.4419,  -83.4202,  -75.0172, -101.0043,  -64.9028,
          -70.3426,  -73.4065,  -77.9200,  -68.9840],
        [-158.9785, -133.4436, -118.7968, -126.2837, -130.2191, -162.0229,
          -97.5785,  -91.9585,  -94.1940,  -92.8788],
        [-187.4650, -169.1880, -153.9772, -164.4027, -153.2953, -188.1044,
         -137.3689, -122.5331, -120.8795, -118.9770],
        [-181.1120, -195.8554, -152.1372, -155.9056, -179.3558, -191.7079,
         -130.7917, -131.3082, -133.3365, -125.9306],
        [-121.5504, -119.1547, -110.6265, -121.3611, -115.6990, -113.6709,
         -106.3604, -110.3255, -104.2289, -108.9295],
        [ -75.1645,  -74.3883,  -76.8869,  -83.3859,  -75.3098,  -70.4563,
          -93.9829,  -78.0899,  -73.8773,  -87.2752],
        [-191.9362, -184.8620, -141.2629, -150.6864, -155.8857, -220.2180,
         -140.2088, -131.8329, -128.6165, -122.3521],
        [-136.2693, -150.7726, -111.8156, -144.5760, -109.7378, -127.1495,
          -90.8749,  -94.0396,  -89.9591,  -88.4912],
        [-153.2771, -125.3698, -111.2255, -138.3661, -110.2840, -139.5460,
          -99.5535, -111.1278, -101.5022,  -98.9144],
        [ -91.3966,  -74.2617,  -77.0938,  -72.5179,  -73.0790,  -73.7857,
          -97.0614,  -85.2095,  -72.6153,  -78.2304],
        [-200.5017, -133.0750, -113.4083, -170.3837, -119.5004, -175.4087,
          -90.6102,  -91.4963,  -92.7806,  -90.0305],
        [ -87.4518,  -79.0037,  -79.4740,  -84.6395,  -93.9128,  -88.5758,
          -76.2799,  -80.4903,  -80.2419,  -74.6216],
        [-119.3875,  -99.7304, -104.3279,  -99.7005, -107.9687, -111.1740,
          -86.3250,  -82.8530,  -83.6222,  -83.3472],
        [ -36.8034,  -37.6833,  -36.6075,  -38.0659,  -40.7428,  -38.2542,
          -38.1370,  -37.1807,  -44.1308,  -39.1137],
        [-112.5799,  -88.2616,  -86.1687,  -92.7086, -100.3570,  -93.8912,
         -104.8864,  -82.6404,  -86.2030,  -81.9274],
        [-110.7026,  -99.2593,  -92.4925, -177.8282, -101.2987, -131.4164,
          -98.8656,  -95.6895,  -84.9894,  -86.1882],
        [-198.5891, -205.9270, -163.1066, -188.2377, -160.1815, -233.1968,
         -132.9193, -120.2292, -127.8435, -120.1536],
        [-123.6426, -114.1123, -107.5807, -118.6042, -107.2070, -110.6935,
         -100.8986, -106.9451,  -99.5005,  -96.5967],
        [-229.6623, -173.1096, -150.9058, -155.2005, -146.5022, -225.5883,
         -136.2980, -105.6189, -106.1597, -128.1654],
        [-150.2327, -133.2388, -129.0627, -138.9583, -119.6898, -148.0270,
         -109.6976, -108.3117, -110.3543, -105.3538],
        [-131.0254, -123.0971, -124.6880, -126.9117, -121.2253, -145.1228,
         -113.3539, -109.6443, -101.5035,  -98.4415],
        [-131.2262,  -83.8154,  -83.7711,  -84.7470,  -85.3636,  -75.7297,
          -97.3621,  -94.5217,  -85.8339,  -90.2888],
        [ -38.9139,  -40.0600,  -39.2922,  -41.7312,  -41.5312,  -42.2212,
          -42.9336,  -41.3884,  -40.3423,  -46.3568],
        [-128.9915, -125.4873, -131.1672, -152.4977, -115.9533, -166.9138,
         -107.2514, -101.0203, -100.3074,  -90.0773],
        [-140.7138, -122.9646, -100.3186, -116.6296, -106.7622, -125.7467,
          -85.4480,  -98.2329,  -90.2257,  -82.4613],
        [-172.2804, -113.3334, -121.7791, -132.0791, -135.8036, -134.9589,
          -96.5759,  -92.9820,  -94.0163, -103.1250],
        [ -48.0518,  -48.0604,  -51.0395,  -48.8078,  -47.9066,  -52.2683,
          -54.7009,  -62.2236,  -56.0148,  -52.0153],
        [-120.1039, -120.3408, -113.9133, -157.9460, -115.1895, -110.2403,
          -98.1293, -100.9678,  -99.6760,  -97.1520],
        [-124.8817, -107.7958, -106.1788,  -99.8580, -118.2912, -104.2935,
          -79.3247,  -76.8378,  -81.0163,  -77.0509],
        [-196.1584, -112.5359, -102.8931, -128.1642, -106.7667, -132.4989,
          -89.1781,  -89.5311,  -89.4308,  -98.6396],
        [-159.0132, -188.8649, -143.4850, -133.0201, -140.8981, -243.9384,
         -110.1981, -113.1823, -116.1741, -105.1242],
        [-192.3095, -154.9514, -142.2918, -159.7061, -145.8775, -261.9034,
         -107.0701, -111.8117, -114.2707, -118.0363],
        [-220.2328, -155.4937, -133.8745, -171.8123, -151.1665, -188.8482,
         -110.0718, -105.1481, -111.6504, -107.0394],
        [-160.2852, -134.5152, -127.1341, -131.7615, -127.2301, -111.3226,
         -152.3186,  -96.7116, -101.2073,  -90.9976],
        [-130.4081, -107.6958, -100.0980, -150.1187, -102.8203, -104.4443,
          -84.5589,  -85.9415,  -90.8354,  -82.6887],
        [-138.4739, -131.2276, -117.1707, -151.4166, -113.1050, -142.8528,
          -97.5304, -106.9871, -100.9338,  -97.0952],
        [-149.1775, -114.8579, -107.5473,  -89.2566, -120.4317, -135.3994,
          -81.1455,  -81.2898,  -89.2456,  -85.1611],
        [-256.1788, -180.2974, -181.6056, -177.9114, -179.7057, -144.6375,
         -134.7389, -120.1283, -119.5119, -118.1522],
        [ -50.0038,  -51.6477,  -48.3741,  -50.4675,  -57.6731,  -56.8073,
          -49.3752,  -59.8546,  -60.3613,  -51.7671],
        [ -50.6710,  -48.8775,  -49.0902,  -48.9859,  -48.8235,  -53.3839,
          -58.9251,  -64.4972,  -58.5426,  -50.9982],
        [-132.9658,  -77.4741,  -86.6777,  -81.6366, -133.6105,  -70.0219,
          -76.5501,  -76.1385,  -70.0270,  -75.3323],
        [-146.9506, -134.7144, -116.5756, -134.5322, -115.8480, -143.8681,
         -110.2003, -117.1620, -113.6346, -103.9859],
        [-122.4658, -104.2085, -109.6450, -138.7477, -101.3842, -114.6605,
          -85.0929,  -93.5745,  -82.9513,  -81.9467],
        [-125.9632,  -96.1647, -111.1691, -106.7645,  -99.2438,  -99.5981,
         -132.9830, -128.5771, -102.3611, -131.1673],
        [-229.1764, -120.1606, -128.1171, -123.4805, -150.0983,  -99.3158,
         -204.1157, -154.9802, -146.0081, -141.4856],
        [ -97.1916,  -84.0568,  -92.1268, -100.2770,  -97.6895,  -84.0383,
         -115.4533, -107.7977,  -85.8330,  -90.8005],
        [-117.2322,  -89.2428, -115.8501,  -93.8154,  -88.6965,  -86.1564,
         -114.3761, -110.6233,  -98.2415, -105.1559],
        [ -48.0567,  -49.1835,  -48.6594,  -48.2842,  -50.4560,  -53.6739,
          -50.8945,  -58.9553,  -63.1824,  -51.5891],
        [-200.6515, -115.1220, -105.9813, -158.4693, -108.6749, -133.8761,
          -87.1591, -132.2457, -108.9312,  -89.5516],
        [-160.1395, -135.2247, -125.7753, -122.3493, -127.3016, -139.5732,
         -116.1907, -114.2045, -118.5107, -113.6905],
        [-128.8113, -111.2909, -106.6367, -106.2563,  -96.5731, -166.3959,
          -97.1750,  -97.0501,  -95.6736,  -98.4278],
        [ -88.7548,  -82.1912,  -72.0582,  -75.6059,  -89.0883,  -87.5120,
          -70.8601,  -69.6693,  -71.4378,  -74.0335],
        [-251.2189, -199.5490, -166.3237, -197.2565, -180.4194, -212.2554,
         -141.1166, -118.9818, -126.0198, -124.8380],
        [ -85.3997,  -88.1155,  -79.3264,  -84.5256, -103.2079, -106.3831,
          -79.4802,  -91.0035,  -86.4504,  -74.1877]], device='cuda:1',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([91, 10, 5]), scale: torch.Size([91, 10, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-118.6503, -130.0451, -123.1968, -112.1340, -104.1076, -136.9388,
         -105.0633, -107.3675, -104.7602, -104.2897],
        [ -78.2386,  -56.9673,  -55.9368,  -54.9462,  -58.3204,  -55.4414,
          -66.7977,  -62.1532,  -59.7903,  -64.1771],
        [ -90.1546,  -90.9294,  -90.0825,  -89.0044,  -93.7194, -138.7027,
          -78.0984,  -85.0356,  -80.3928,  -77.7964],
        [ -84.6746,  -79.2290,  -78.4519,  -80.0556,  -86.8629,  -68.7951,
          -96.3272,  -85.1655,  -73.4144,  -79.7094],
        [-145.0475,  -97.2317, -102.7571,  -99.5099,  -99.5678,  -98.9602,
         -105.4685,  -96.7383, -100.4168, -105.0974],
        [-149.7471, -144.7657, -126.4534, -129.2856, -102.9818, -133.1888,
          -97.8129, -124.9008, -102.3051,  -95.5015],
        [ -47.8158,  -48.5322,  -48.2084,  -47.9134,  -50.9205,  -53.8611,
          -52.3686,  -62.7194,  -57.8886,  -51.3893]], device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([7, 10, 5]), scale: torch.Size([7, 10, 5]))
=============================================================================================
