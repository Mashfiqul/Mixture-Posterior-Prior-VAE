Dataset: Reuters
--------------------------------------------
Model Name: MMVAE_10_Latent_dim_5_Hard_Reuters
==================================================================
MMVAE_10_Latent_dim_5_Hard_Reuters Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 10
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 2000
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 2000
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 4
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-1973.3875, -2823.7080, -2200.0063, -1898.3376],
        [-2329.1665, -2451.1665, -2273.1729, -2278.9573],
        [-2828.3198, -2461.9604, -2462.1997, -2488.8811],
        [-2824.3005, -2792.3782, -2800.3948, -2807.9775],
        [-2789.7078, -2687.9407, -2703.0730, -2720.3330],
        [-2285.9294, -2709.0835, -2348.4275, -2254.3198],
        [-2050.3003, -2682.0537, -2131.1567, -2011.0498],
        [-2471.5015, -2541.1445, -2451.8062, -2476.3445],
        [-2674.0757, -2677.7593, -2703.7744, -2670.0161],
        [-2214.0823, -2799.6221, -2149.9771, -2131.2451],
        [-2687.3677, -2857.8726, -2713.0747, -2699.9673],
        [-2696.5891, -2523.3062, -2580.5488, -2576.5972],
        [-2784.4709, -2683.5249, -2730.1528, -2693.1064],
        [-2534.4233, -2697.4180, -2521.8115, -2528.2563],
        [-1967.3939, -1963.0234, -1869.9973, -1873.0691],
        [-2697.2085, -2819.9595, -2700.8513, -2703.3208],
        [-2813.9431, -2634.1460, -2739.5239, -2711.8630],
        [-2796.1780, -2530.9575, -2522.5327, -2536.1702],
        [-2685.8628, -2661.0662, -2691.4912, -2659.1274],
        [-2189.8584, -2944.8052, -2129.2476, -2124.8752],
        [-2796.5913, -2720.4053, -2767.9844, -2733.0059],
        [-2477.2705, -2578.2878, -2412.7773, -2408.1228],
        [-2618.7175, -2773.6375, -2631.0000, -2627.1965],
        [-2345.8110, -2847.8413, -2338.3743, -2299.2773],
        [-2717.1230, -2501.7144, -2484.5486, -2501.5269],
        [-2673.1892, -2813.2900, -2675.5894, -2672.6548],
        [-2612.0481, -2766.0056, -2623.9658, -2617.5327],
        [-2460.7827, -2830.7363, -2474.4395, -2460.4482],
        [-2805.4370, -2740.8369, -2776.8259, -2755.8755],
        [-2480.6111, -2878.3423, -2521.7542, -2487.7681],
        [-2518.5503, -2785.9692, -2505.9858, -2515.3674],
        [-2633.8455, -2777.9539, -2736.9019, -2663.7690],
        [-2704.3159, -2760.4014, -2711.7666, -2704.8071],
        [-2512.5405, -2675.2471, -2511.1465, -2520.4795],
        [-2588.1411, -2720.1431, -2609.6326, -2605.3467],
        [-2451.4341, -2611.5740, -2448.3040, -2446.8721],
        [-2712.9539, -2727.4014, -2721.2104, -2720.6743],
        [-2505.4258, -2809.6938, -2510.0400, -2509.1982],
        [-2539.8352, -2507.3223, -2494.3330, -2504.0562],
        [-2682.4409, -2728.5498, -2680.3091, -2687.5725],
        [-2649.1787, -2827.2134, -2657.7983, -2652.8135],
        [-2583.6045, -2837.9658, -2581.6069, -2594.1748],
        [-2753.5359, -2753.7256, -2751.8269, -2754.6790],
        [-2655.3887, -2744.5833, -2664.2314, -2664.3906],
        [-2586.1726, -2618.8796, -2590.0452, -2589.1738],
        [-2700.7017, -2848.5181, -2705.5012, -2701.1719],
        [-2618.7637, -2723.7139, -2619.0410, -2624.8665],
        [-2718.9897, -2835.0132, -2743.9385, -2726.8550],
        [-2720.3140, -2829.7207, -2716.5923, -2718.7974],
        [-2585.8330, -2596.7065, -2605.4595, -2582.8857],
        [-2301.2002, -2729.5630, -2283.5154, -2267.2593],
        [-2580.5977, -2649.6169, -2547.3516, -2539.2419],
        [-2791.2529, -2656.5696, -2676.7898, -2679.2251],
        [-2470.3049, -2479.4082, -2463.1528, -2471.9170],
        [-2756.7754, -2784.5708, -2755.9790, -2752.1147],
        [-2797.8477, -2850.9646, -2801.8213, -2796.9741],
        [-2747.0293, -2837.9131, -2754.1609, -2748.7368],
        [-2727.8665, -2813.0598, -2727.1580, -2728.3188],
        [-2388.0303, -2470.2593, -2386.6875, -2398.0718],
        [-2357.0735, -2455.2012, -2310.0640, -2315.3250],
        [-2687.3276, -2275.4492, -2295.7795, -2304.8062],
        [-2757.6929, -2302.7046, -2302.3401, -2352.3276],
        [-2371.7549, -2678.7659, -2355.7461, -2306.8442],
        [-2554.0081, -2854.2429, -2567.7646, -2554.1543],
        [-2595.1304, -2736.9102, -2576.1665, -2577.1235],
        [-2525.5376, -2485.6094, -2491.1868, -2489.2651],
        [-2007.6134, -2845.0068, -2174.9390, -1896.8750],
        [-2740.4019, -2774.9116, -2756.6865, -2743.9990],
        [-2744.2681, -2686.2888, -2723.7266, -2688.1641],
        [-2774.4934, -2744.6431, -2766.5200, -2769.2004],
        [-2738.8394, -2442.7842, -2442.4004, -2475.6924],
        [-1953.9095, -2841.3062, -2105.3091, -1890.3199],
        [-2703.7271, -2804.9810, -2708.7659, -2701.3984],
        [-2613.9321, -2734.9189, -2607.7222, -2608.8447],
        [-2726.3799, -2814.8269, -2729.3462, -2728.7261],
        [-2702.0898, -2765.7444, -2692.9136, -2703.5483],
        [-2416.6577, -2882.3198, -2412.4768, -2416.0659],
        [-2391.2961, -2838.3882, -2398.8333, -2402.1680],
        [-2674.3174, -2685.5483, -2646.7480, -2658.4160],
        [-2588.5972, -2811.7908, -2603.0449, -2557.9038],
        [-2795.5298, -2693.2844, -2775.2551, -2694.7461],
        [-2731.8369, -2633.4805, -2610.8103, -2647.8916],
        [-2262.1797, -1964.4912, -1979.9373, -1996.4844],
        [-2550.4756, -2550.9976, -2555.2097, -2545.7710],
        [-2687.8516, -2647.6401, -2661.5459, -2658.4128],
        [-2776.8174, -2717.1938, -2783.2854, -2732.9170],
        [-2343.8103, -2883.0068, -2360.4624, -2322.2690],
        [-2203.1504, -2998.1992, -2174.1162, -2179.6270],
        [-2678.7979, -2790.2310, -2681.9819, -2676.0415],
        [-2068.4719, -2869.8516, -2244.7529, -1985.2942],
        [-2750.0515, -2816.8438, -2751.7251, -2751.1716],
        [-2600.0635, -2556.9966, -2583.6982, -2556.5410],
        [-2489.2739, -2700.2339, -2547.4580, -2488.8750],
        [-2344.7930, -2884.0486, -2275.0454, -2269.0305],
        [-2523.4958, -2190.3672, -2239.1938, -2221.4053],
        [-2611.9146, -2525.8794, -2512.1550, -2575.3801],
        [-2694.5833, -2797.4565, -2687.9331, -2688.1216],
        [-2752.1494, -2773.1313, -2756.5791, -2751.0337],
        [-2648.9741, -2641.0142, -2636.4150, -2634.1455],
        [-2641.2063, -2769.2900, -2643.0029, -2645.4170]], device='cuda:1',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-2798.9751, -2808.5952, -2799.9727, -2799.9111],
        [-2659.6826, -2742.9851, -2669.4456, -2659.0889],
        [-2366.3896, -2843.8953, -2366.1467, -2337.6099],
        [-2270.1768, -2851.5664, -2163.1943, -2144.6465],
        [-2802.6367, -2783.5859, -2805.9275, -2812.4558],
        [-2714.9712, -2701.2524, -2719.5950, -2706.0713],
        [-2538.4319, -2705.9988, -2545.4585, -2563.5747],
        [-2746.1074, -2562.4561, -2608.8711, -2589.6079],
        [-2523.5767, -2727.4158, -2522.8101, -2525.1025],
        [-2538.6155, -2867.5703, -2582.8481, -2550.0322],
        [-2765.3523, -2579.6221, -2602.7202, -2579.2954],
        [-2770.5366, -2526.7446, -2531.3721, -2536.2759],
        [-2855.7185, -2805.4048, -2847.9321, -2843.2402],
        [-1986.3428, -2867.2480, -1994.2322, -1992.3113],
        [-2702.9971, -2699.1030, -2698.8408, -2697.3472],
        [-2662.3154, -2628.6489, -2664.5029, -2636.7261],
        [-2749.4409, -2753.4766, -2739.0132, -2756.8047],
        [-2772.4370, -2808.3955, -2773.1636, -2771.2656],
        [-2864.0923, -2613.0825, -2615.0601, -2668.2930],
        [-2671.4175, -2672.8584, -2677.4146, -2675.4507],
        [-2453.7388, -2844.0100, -2440.4663, -2461.5752],
        [-2108.0010, -2859.0508, -2226.5566, -2037.9252],
        [-2618.8757, -2618.4175, -2607.9165, -2619.1646],
        [-2558.8789, -2852.2568, -2571.7915, -2570.1831],
        [-2528.7373, -2431.6089, -2440.2324, -2439.6340],
        [-2275.2183, -2829.7737, -2259.9343, -2251.1375],
        [-2824.8962, -2524.8611, -2515.4834, -2525.5591],
        [-2655.6040, -2770.5605, -2651.9375, -2657.3418],
        [-2482.8213, -2722.4399, -2445.9753, -2470.7166],
        [-2658.0059, -2772.6802, -2641.1416, -2653.7183],
        [-2864.4707, -2754.9194, -2892.3218, -2786.7871],
        [-2599.5815, -2842.1636, -2607.8330, -2606.0542],
        [-2392.5107, -2827.8931, -2403.6567, -2396.0610],
        [-2665.9897, -2809.2148, -2673.6606, -2670.8872],
        [-2690.8003, -2696.7588, -2677.8625, -2690.9758],
        [-2777.7605, -2730.2024, -2741.8862, -2734.2075],
        [-2596.4263, -2836.7007, -2604.0596, -2594.7822],
        [-2872.3374, -2735.2207, -2789.9419, -2739.0972],
        [-2841.3850, -2659.2380, -2788.0564, -2689.2830],
        [-2695.6472, -2654.8457, -2660.6060, -2658.1025],
        [-2543.8774, -2778.8054, -2596.8369, -2531.0347],
        [-2573.1147, -2931.3691, -2583.4136, -2529.5005],
        [-2676.1223, -2718.3494, -2677.7688, -2678.2559],
        [-2760.2695, -2733.7793, -2722.7307, -2735.3745],
        [-2631.3806, -2713.2185, -2637.9121, -2637.7017],
        [-2632.9644, -2900.2788, -2643.6445, -2637.3633],
        [-2731.9302, -2793.8359, -2740.0186, -2742.7271],
        [-2482.4060, -2774.3469, -2479.0132, -2490.5728],
        [-2270.2583, -2832.2200, -2283.0911, -2269.3962],
        [-2737.5166, -2707.1904, -2743.9976, -2735.6807],
        [-2509.8799, -2512.1289, -2425.8838, -2460.1467],
        [-2707.2969, -2841.7874, -2693.7456, -2624.2329],
        [-2713.6721, -2638.4351, -2656.0010, -2647.2700],
        [-2716.1011, -2686.6489, -2692.6689, -2707.8267],
        [-2713.6318, -2645.1343, -2689.7048, -2633.5386],
        [-2782.8091, -2778.8057, -2779.8677, -2783.9680],
        [-2773.0747, -2730.6658, -2711.1455, -2803.6895],
        [-2229.1699, -2777.2441, -2126.2312, -2108.0535],
        [-2417.5862, -2741.7043, -2433.0115, -2362.7563],
        [-2787.7979, -2609.9771, -2634.7983, -2645.7488],
        [-2614.5408, -2667.5342, -2621.9385, -2618.7166],
        [-2853.1294, -2762.6284, -2803.4863, -2786.3796],
        [-2617.4197, -2647.4370, -2621.1370, -2604.1260],
        [-2895.9463, -2890.7883, -2854.2944, -2923.1035],
        [-2835.7559, -2802.0144, -2885.0603, -2885.6218],
        [-2429.2629, -2248.2458, -2261.4351, -2257.9624],
        [-2791.6484, -2721.8674, -2709.4204, -2718.5352],
        [-2704.3164, -2814.0215, -2703.4375, -2706.2510],
        [-2826.5803, -2470.8000, -2484.5293, -2493.3945],
        [-2792.2458, -2787.9062, -2807.1528, -2806.9414],
        [-2546.8679, -2591.9966, -2523.8894, -2532.2693],
        [-2671.0071, -2825.6938, -2671.4897, -2669.8320],
        [-2665.4097, -2773.2073, -2677.2820, -2662.6460],
        [-2739.2532, -2525.1421, -2564.5073, -2536.3052],
        [-2763.1963, -2729.1453, -2729.0508, -2729.6377],
        [-2495.7368, -2714.5322, -2465.0874, -2470.3979],
        [-2711.9016, -2759.0576, -2703.4683, -2739.5674],
        [-2694.9573, -2859.4380, -2819.6157, -2725.2678],
        [-2588.9067, -2731.4163, -2590.0840, -2588.0901],
        [-2745.4893, -2740.4602, -2738.9966, -2738.6848],
        [-2350.5659, -2368.7808, -2286.2183, -2303.3481],
        [-2434.9104, -2827.4309, -2441.5522, -2436.8694],
        [-2782.2024, -2758.3882, -2728.4238, -2708.2983],
        [-2813.3467, -2688.6997, -2698.1079, -2728.4033],
        [-2628.0554, -2802.8821, -2647.8269, -2632.8264],
        [-2732.4294, -2748.5974, -2735.5562, -2735.8577],
        [-2704.6382, -2374.6470, -2489.6680, -2390.2715],
        [-2168.4507, -2880.2310, -2060.2615, -2049.6609],
        [-2853.0486, -2799.8320, -2799.5806, -2835.2642],
        [-2188.3408, -2591.2183, -2346.2925, -2179.7017],
        [-2374.8064, -2722.8945, -2374.7639, -2275.7822],
        [-2809.0229, -2848.8994, -2823.2891, -2812.8506],
        [-2599.3313, -2603.5740, -2645.1895, -2593.7012],
        [-2553.6470, -2807.2495, -2532.6250, -2506.6113],
        [-2345.0137, -2850.8008, -2349.2029, -2327.6157],
        [-2785.9893, -2606.6233, -2717.0903, -2618.3992],
        [-2837.5615, -2585.5391, -2633.0283, -2598.6265],
        [-2420.8970, -2845.3008, -2394.8213, -2382.6541],
        [-2670.6572, -2833.9436, -2668.2231, -2665.9097],
        [-2731.9102, -2775.9084, -2740.3176, -2739.6133]], device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
