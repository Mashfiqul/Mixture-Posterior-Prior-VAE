Dataset: Reuters
--------------------------------------------
Model Name: MMVAE_EM_Prior_5_Latent_dim_5_Soft_Reuters
==================================================================
MMVAE_EM_Prior_5_Latent_dim_5_Soft_Reuters Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 5
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 2000
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 2000
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 4
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-3516.7759, -2332.8486, -3024.2056, -3536.6099],
        [-4501.5806, -2496.4053, -3046.2119, -2317.4614],
        [-2453.5068, -4603.5127, -4054.2529, -3498.2300],
        [-2816.0566, -2688.2412, -2843.1775, -2688.6399],
        [-3185.0696, -1897.5970, -3571.3118, -5256.6099],
        [-2772.9263, -2602.9414, -3019.9985, -3265.8818],
        [-2854.8228, -2634.9429, -3061.5405, -2617.7349],
        [-2371.1599, -5318.9736, -3698.1887, -4562.3779],
        [-2558.8120, -2938.4111, -3562.4614, -3557.4658],
        [-2673.2847, -2794.6104, -3132.5669, -3252.1240],
        [-2518.8232, -4422.2905, -4274.6953, -3817.5415],
        [-3093.3955, -2583.8154, -2969.1899, -2586.1172],
        [-2877.9473, -2759.8176, -2871.9031, -2760.0640],
        [-2782.3101, -2762.1724, -3030.1572, -2762.2349],
        [-2377.6851, -2377.7905, -3142.5884, -2677.2559],
        [-3090.0068, -2624.4536, -2815.7312, -2615.5903],
        [-2903.9380, -2661.8521, -2892.5413, -2698.8240],
        [-3213.6753, -2470.7461, -2920.6304, -2468.0972],
        [-2649.5176, -2519.6392, -3051.5464, -2522.4424],
        [-2162.2964, -2924.5679, -2979.5054, -5625.1797],
        [-1968.0021, -4931.7188, -4692.9824, -6088.9194],
        [-2255.0461, -3687.7583, -3195.7510, -4812.7534],
        [-2727.2134, -3530.9438, -3672.2891, -3538.0806],
        [-2597.0801, -2593.1208, -2784.8418, -2590.6025],
        [-3835.1553, -4128.5537, -1929.3508, -3516.7388],
        [-2782.8921, -2724.5591, -2940.3350, -2724.8110],
        [-3194.6270, -3231.2817, -2417.3413, -3242.6191],
        [-2739.8887, -2111.8022, -3512.0901, -4736.5859],
        [-2770.7083, -2741.6006, -2961.1357, -2746.2715],
        [-2854.4419, -2682.1187, -2915.6748, -2677.3931],
        [-2587.7993, -2554.6631, -3009.0781, -2665.8105],
        [-3194.8401, -2619.5684, -3033.9146, -2621.8921],
        [-2926.1899, -2723.4924, -2857.5354, -2708.6182],
        [-2744.2678, -2570.1685, -3138.3936, -3451.8340],
        [-2721.5178, -2664.3342, -2857.6274, -2669.4585],
        [-2825.2593, -2770.8428, -3022.4785, -2768.7957],
        [-2421.0049, -2228.1738, -3495.9595, -2248.6011],
        [-2483.8350, -5212.5996, -4460.5918, -4130.0942],
        [-2816.9463, -2814.5225, -2956.2393, -2815.4990],
        [-2599.9187, -3004.3345, -3232.4521, -3284.3110],
        [-3392.0383, -3454.8892, -2086.9336, -3934.9077],
        [-2438.8438, -3177.6606, -3512.2551, -3525.4600],
        [-2719.8193, -2736.8171, -2956.0933, -3115.7910],
        [-2920.5405, -2733.2837, -3011.7681, -2980.8960],
        [-4273.1875, -2105.8213, -2921.4292, -3392.1104],
        [-2563.4675, -3093.9500, -3265.2295, -4471.9243],
        [-2666.5452, -2518.2988, -3331.9302, -3577.4883],
        [-2671.2751, -2480.7356, -3293.1919, -3114.3740],
        [-2782.4897, -2592.1445, -2939.2156, -2591.6653],
        [-3107.6060, -2731.1160, -2987.2415, -2705.4150],
        [-3056.9746, -2738.5703, -2920.6182, -2737.0176],
        [-2237.6030, -3573.3589, -3306.3203, -3922.9180],
        [-2674.6167, -2420.8398, -2849.5049, -2414.1611],
        [-2721.2629, -2854.2920, -3235.5093, -3187.4192],
        [-2802.7419, -2679.5837, -2785.5605, -2681.9229],
        [-3241.2039, -2612.4014, -2941.0171, -2614.0859],
        [-2571.1338, -2826.7534, -3009.7661, -3301.4451],
        [-2703.8867, -3258.1016, -3276.5046, -2836.5383],
        [-2794.9338, -2717.4285, -2941.5269, -2741.3628],
        [-2963.7991, -2022.2986, -3555.3315, -4798.3394],
        [-2015.0741, -7809.6318, -4001.5505, -4297.5977],
        [-3640.7737, -3109.2480, -3081.5920, -2460.0632],
        [-3326.7542, -3375.9634, -2131.3677, -3300.5283],
        [-1896.7007, -2711.3564, -3646.5215, -5688.4121],
        [-2556.9033, -2856.5127, -3146.6423, -3601.5586],
        [-2691.7036, -2559.4346, -2811.9233, -2559.3982],
        [-2838.8545, -2733.6567, -2957.9109, -2738.8499],
        [-2352.4268, -4587.8945, -4635.0029, -3870.7842],
        [-3266.0310, -2575.9780, -2851.1572, -2565.8760],
        [-2416.6807, -3127.1367, -3122.8306, -2686.7461],
        [-2674.7324, -3066.1077, -3230.7837, -3398.8301],
        [-2929.4941, -2739.4866, -3020.8621, -2724.5652],
        [-2730.8096, -3136.4131, -3220.4609, -3126.9131],
        [-2618.2495, -3070.4819, -3374.3755, -4091.0293],
        [-3781.8555, -3921.7192, -1995.2240, -3474.2983],
        [-3894.7100, -2249.8477, -2953.6523, -3386.1431],
        [-3308.8735, -3205.0957, -3230.7991, -2382.1108],
        [-2245.3350, -2461.3025, -3631.1685, -5449.8320],
        [-3215.4863, -2221.0310, -3439.4512, -4252.6699],
        [-3338.6938, -3204.0178, -3145.2327, -2504.3435],
        [-3497.9907, -3052.3013, -3098.0396, -2501.6899],
        [-2758.9790, -2648.2236, -2909.2505, -2632.0818],
        [-2660.5229, -3227.7776, -3323.1992, -3222.6309],
        [-2711.2988, -2716.1328, -2971.8330, -2908.7798],
        [-2550.8264, -3173.3191, -3303.6343, -3845.8467],
        [-2692.1765, -2693.9277, -3091.5081, -2712.2100],
        [-3750.3049, -3568.4824, -2141.3679, -3474.2012],
        [-2546.3049, -2404.4741, -3007.6685, -2396.9131],
        [-2451.6958, -6106.9688, -4277.4902, -4549.4434],
        [-2702.9583, -2661.7622, -2973.7649, -2672.0981],
        [-2695.3950, -2908.0278, -3438.4805, -3222.9883],
        [-3139.2095, -2443.5361, -2706.5354, -2297.4084],
        [-2929.2922, -2735.8867, -3035.9993, -2750.9868],
        [-2611.0405, -2538.6558, -2940.6006, -2824.4727],
        [-3386.6338, -2217.4646, -3163.7427, -3350.3530],
        [-2951.4133, -2761.1572, -3018.7964, -2876.1650],
        [-2548.9990, -4674.2793, -4364.1016, -4412.9307],
        [-2478.1450, -4077.9092, -3501.9507, -3756.3901],
        [-2575.9839, -2539.3813, -2804.3154, -2539.1812],
        [-3049.5317, -2754.3145, -2851.9434, -2753.8540]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-2889.8687, -2840.3706, -2899.9875, -2837.2278],
        [-2728.3491, -2736.8074, -2932.1265, -2769.3984],
        [-3206.6978, -2377.4888, -3105.3279, -3640.4639],
        [-2140.4214, -3843.4753, -3804.8452, -5155.4546],
        [-2820.9961, -2789.1333, -2886.1062, -2789.5239],
        [-2731.8894, -2714.8945, -2867.5581, -2719.1538],
        [-2540.5754, -2536.1094, -2833.7114, -2644.6333],
        [-3348.3159, -3097.9075, -3187.1260, -2568.9546],
        [-2527.4053, -4084.1763, -4626.8535, -5978.4668],
        [-3133.6318, -2445.0078, -3196.1255, -3347.9312],
        [-3187.1106, -2916.0054, -3071.0103, -2574.8120],
        [-2492.9988, -3023.8025, -3373.5942, -4062.1965],
        [-3065.6592, -2789.3730, -2847.9856, -2778.1501],
        [-2544.6794, -2305.2354, -1997.3289, -5030.5850],
        [-2866.1455, -2715.0696, -2820.7795, -2710.1470],
        [-2729.6851, -2627.1294, -2813.7700, -2620.1655],
        [-2769.1685, -3448.1267, -3418.6521, -3281.5225],
        [-2794.2969, -2775.5542, -2848.3154, -2781.3259],
        [-2636.7939, -2700.6077, -3064.5696, -3044.9094],
        [-2764.0603, -2676.3850, -2814.7476, -2679.7581],
        [-2396.5869, -3573.6030, -3653.4688, -5135.0083],
        [-3766.5771, -3925.9353, -2037.5917, -3567.3394],
        [-2635.4714, -3332.8608, -3263.6367, -4180.7061],
        [-2589.4841, -3197.2568, -3366.1995, -3534.5342],
        [-2432.1902, -5512.3350, -3770.0742, -3657.2705],
        [-3324.3203, -2283.1360, -3080.3286, -3758.7671],
        [-2510.5364, -4155.6904, -3201.7598, -3625.8096],
        [-2929.8762, -2647.7544, -3207.4370, -3312.1655],
        [-2362.2031, -3051.6609, -2993.3306, -4457.8198],
        [-2644.1414, -2914.3792, -3367.3608, -4097.3564],
        [-3063.1628, -2737.9541, -2960.4158, -2730.8486],
        [-2621.7764, -2857.1338, -3056.4727, -3211.7854],
        [-2374.1492, -3475.6777, -3618.1375, -3961.0063],
        [-2821.5010, -2673.6299, -3047.9187, -2803.3474],
        [-2625.4541, -4037.7561, -3832.5693, -3136.0862],
        [-3017.8022, -2713.9985, -2867.9155, -2708.4531],
        [-2600.4009, -2884.1948, -3146.3208, -3464.7393],
        [-3109.1123, -2706.5610, -2863.4783, -2705.4434],
        [-3410.7891, -2694.5645, -2924.1660, -2673.5664],
        [-2793.1409, -2635.3438, -2948.5303, -2624.3154],
        [-3228.6272, -2511.9204, -3316.9961, -3878.3892],
        [-3207.0439, -2541.5498, -3164.2935, -3116.0366],
        [-2915.8984, -2692.8850, -2774.3025, -2698.2307],
        [-2736.3159, -2957.1870, -3025.2158, -3254.7324],
        [-2645.5674, -3465.9385, -3695.0037, -3186.4619],
        [-2623.0215, -3385.6089, -3273.6704, -3987.9717],
        [-2749.3162, -2726.8137, -2984.3813, -2730.4805],
        [-2492.2917, -6476.7002, -5002.9883, -4489.9004],
        [-2272.7920, -3657.8145, -3472.4683, -4304.9141],
        [-2880.4482, -2763.2485, -2983.7690, -2772.6838],
        [-2417.6477, -3477.6089, -3143.2844, -2950.3494],
        [-3127.4995, -3009.7852, -2657.3018, -2972.2732],
        [-3016.4497, -2677.8037, -2835.5793, -2665.4197],
        [-2691.4375, -4201.4277, -3854.7986, -3805.1292],
        [-2914.0830, -2700.9360, -2789.3169, -2693.5376],
        [-3009.4080, -2808.7915, -2786.2944, -2792.6929],
        [-2695.5269, -3007.7593, -3312.4800, -3688.4592],
        [-2124.1123, -3934.7214, -4161.8818, -3738.2612],
        [-4152.0391, -2396.9714, -3118.7910, -3421.9199],
        [-3149.5566, -2519.1255, -3154.2271, -2497.3994],
        [-2654.1250, -2610.5276, -3186.7954, -2625.4902],
        [-3028.7104, -2802.7759, -2933.8889, -2757.8821],
        [-3210.2141, -2648.2068, -2813.4670, -2624.3281],
        [-2810.9258, -2960.2759, -3580.3430, -3927.9136],
        [-2861.3408, -2795.3792, -2994.8142, -2788.3774],
        [-2246.3506, -5568.7900, -3363.8008, -3748.9980],
        [-3044.3972, -2746.2905, -2911.2339, -2742.0618],
        [-2690.6870, -2646.2822, -3016.7842, -2730.8889],
        [-2493.5164, -2869.5063, -3105.3428, -3521.7539],
        [-2881.8223, -2765.1172, -2952.6616, -2758.6440],
        [-2499.4121, -3570.1641, -3152.1108, -4161.4829],
        [-2679.6372, -2666.3154, -2978.3677, -2757.1509],
        [-2667.6641, -2597.4343, -3140.9529, -2788.9861],
        [-3115.8979, -2548.0281, -2908.1606, -2529.1396],
        [-2693.8445, -2943.2021, -3150.1021, -2843.3821],
        [-2446.0166, -4555.6016, -4057.4727, -3596.9087],
        [-2636.7378, -2752.4233, -2921.8335, -3227.0659],
        [-2900.6738, -2920.9417, -3158.5989, -2951.3884],
        [-2581.8167, -3468.8613, -3746.4341, -5021.2666],
        [-3130.0215, -2751.2319, -2772.4287, -2740.1255],
        [-2341.0210, -3122.1797, -3505.9788, -2503.6389],
        [-2475.0210, -3302.0747, -3275.6501, -3648.0659],
        [-2793.8218, -2654.5117, -3016.4399, -2988.4805],
        [-2699.4634, -2700.3394, -3214.1008, -2965.3584],
        [-2828.6975, -2650.4033, -3162.6626, -2636.6912],
        [-3140.2559, -2753.8296, -2909.8372, -2744.2593],
        [-3918.2061, -2541.4016, -3010.5898, -2431.5098],
        [-2035.2953, -4562.9614, -4387.1362, -5636.9727],
        [-3287.9609, -2781.3567, -3283.1702, -3353.5979],
        [-4143.5928, -2182.2549, -3295.2446, -3784.4995],
        [-3564.5322, -2287.5181, -3325.6765, -3560.3472],
        [-2800.6875, -2789.1709, -2957.2058, -2887.7893],
        [-2605.6528, -2707.1482, -2937.8101, -2621.8174],
        [-2551.5444, -2564.6851, -3244.6304, -3649.0518],
        [-3146.8252, -2325.7300, -3114.9299, -3392.1973],
        [-3257.5342, -2598.9702, -2933.7695, -2596.0068],
        [-2951.0625, -2745.6260, -3120.8335, -2590.9395],
        [-3248.0813, -2398.7778, -3090.3499, -3460.7256],
        [-2639.3931, -3265.6333, -3543.5520, -3475.7644],
        [-2798.5303, -2700.0251, -2957.2798, -2985.1382]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
