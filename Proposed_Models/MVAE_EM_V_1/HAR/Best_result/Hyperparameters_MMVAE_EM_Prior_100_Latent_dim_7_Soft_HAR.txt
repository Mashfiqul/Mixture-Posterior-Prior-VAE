Dataset: HAR
--------------------------------------------
Model Name: MMVAE_EM_Prior_100_Latent_dim_7_Soft_HAR
==================================================================
MMVAE_EM_Prior_100_Latent_dim_7_Soft_HAR Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 100
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 561
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 7
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 561
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 6
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-524.8135, -526.3309, -527.7663, -529.6898, -525.6060, -528.2242],
        [-546.8775, -563.8993, -535.6308, -525.5242, -572.0325, -525.7621],
        [-556.9056, -553.8118, -544.1400, -519.7131, -564.6407, -536.1389],
        [-555.6246, -561.1632, -529.8961, -521.6205, -557.4313, -523.5787],
        [-526.7314, -526.3973, -526.9684, -525.9136, -528.7531, -525.2960],
        [-523.2996, -523.2956, -522.8870, -523.0106, -527.0732, -522.9913],
        [-548.1679, -571.5948, -538.1469, -520.4663, -554.9346, -529.4633],
        [-535.0571, -556.6963, -523.0474, -523.6118, -564.8033, -521.6754],
        [-523.4438, -524.2148, -523.7462, -535.6357, -523.7882, -528.6852],
        [-541.6961, -535.1094, -525.6564, -521.2181, -576.4764, -524.0482],
        [-529.1139, -530.6358, -553.2178, -520.0048, -550.9343, -520.7186],
        [-545.0428, -526.4426, -535.4998, -518.9826, -560.7352, -532.0797],
        [-557.2549, -539.4039, -524.8528, -520.9583, -527.6481, -525.9202],
        [-520.6695, -523.3369, -520.2761, -566.1064, -523.0204, -521.5006],
        [-563.0119, -539.9564, -530.7133, -519.7916, -583.8457, -540.3883],
        [-523.8534, -523.1895, -524.5081, -538.6910, -523.4233, -523.2129],
        [-583.3542, -540.1652, -521.6057, -520.2383, -568.0354, -524.0175],
        [-524.3035, -522.6919, -523.0331, -523.4777, -523.6703, -522.8146],
        [-553.5217, -555.6385, -536.2318, -519.2469, -555.8529, -521.8118],
        [-524.9289, -524.7935, -527.9549, -535.4456, -524.7663, -525.5261],
        [-526.9332, -525.8630, -527.0887, -543.3928, -533.1542, -527.4534],
        [-525.3143, -527.3320, -525.0035, -524.1051, -525.6796, -525.2475],
        [-538.1859, -532.9246, -529.1887, -519.4612, -538.2249, -532.2672],
        [-519.8672, -522.2721, -519.5873, -549.2409, -597.8009, -520.9921],
        [-528.1848, -526.1030, -530.0832, -533.5065, -525.8698, -526.3357],
        [-523.9229, -536.7098, -529.3308, -521.8989, -576.6262, -523.3077],
        [-525.3828, -528.0828, -527.5048, -525.7102, -524.9148, -525.0533],
        [-524.9078, -524.9284, -524.1056, -526.9315, -526.4150, -523.5474],
        [-528.0612, -527.9385, -527.8422, -556.7332, -528.7396, -527.9653],
        [-564.7272, -531.7664, -524.9403, -520.4774, -585.1142, -542.3378],
        [-522.4216, -523.0146, -521.8381, -546.0649, -525.1859, -522.5115],
        [-529.8331, -529.6562, -530.2112, -540.2598, -529.9871, -529.9178],
        [-520.7316, -521.0559, -522.0031, -524.3176, -522.3848, -523.3907],
        [-523.7429, -524.1746, -522.3479, -543.5411, -524.1125, -523.5283],
        [-524.8188, -525.2914, -527.4399, -531.5157, -525.1530, -525.6473],
        [-582.8488, -554.6810, -572.3079, -526.8512, -558.1810, -557.4811],
        [-550.4275, -550.9132, -529.4233, -518.8499, -566.1251, -519.8416],
        [-540.0046, -544.9162, -528.9944, -518.9407, -571.4987, -541.5435],
        [-541.2821, -536.5958, -535.6472, -520.8134, -589.3940, -541.3441],
        [-554.1156, -543.8698, -533.1124, -519.7464, -555.2356, -520.0126],
        [-529.9446, -541.2357, -524.0587, -521.5445, -570.8542, -522.0162],
        [-538.1246, -556.6929, -532.4551, -523.7523, -541.4629, -536.0386],
        [-526.3085, -526.2256, -527.7372, -567.0739, -532.9333, -529.2711],
        [-523.9804, -520.5294, -536.7802, -606.2123, -522.9105, -524.9456],
        [-555.5685, -564.4456, -531.6393, -519.8777, -574.9515, -543.9426],
        [-556.0825, -526.9432, -554.0536, -519.5240, -559.4147, -519.6511],
        [-543.0972, -539.8530, -536.2729, -524.8363, -633.3685, -547.8635],
        [-525.8170, -524.5830, -523.9816, -547.2621, -524.5110, -524.5808],
        [-538.6554, -547.5967, -542.5616, -520.2631, -531.3089, -534.3384],
        [-553.3057, -548.4537, -524.3203, -518.5428, -615.2853, -527.3977],
        [-551.7148, -562.6768, -525.6591, -521.3082, -561.6802, -524.9326],
        [-560.7421, -543.9952, -520.6866, -522.3258, -569.2594, -526.7202]],
       device='cuda:0', grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([52, 6, 7]), scale: torch.Size([52, 6, 7]))
=============================================================================================
Decoder Loss (Test) = tensor([[-526.3705, -527.1764, -526.9831, -527.3605, -525.8848, -526.5288],
        [-527.9911, -528.4601, -528.4799, -528.7676, -527.0283, -527.9312],
        [-526.9448, -527.4763, -527.4810, -527.4325, -526.2332, -526.9303],
        [-524.7430, -525.2650, -525.2896, -524.9174, -524.2046, -524.8858],
        [-523.2692, -523.6832, -523.3125, -523.1327, -522.5363, -523.0850],
        [-523.4746, -523.8519, -523.1548, -523.9712, -522.7842, -523.1359],
        [-524.5724, -525.1030, -524.7874, -524.6582, -523.9253, -524.6772],
        [-526.3737, -526.6927, -526.5899, -526.4036, -525.8129, -526.4093],
        [-521.8864, -522.1788, -521.5140, -526.3549, -522.3779, -521.9248],
        [-521.8323, -522.0284, -521.2333, -526.7780, -522.2114, -521.6570],
        [-521.6608, -521.8134, -520.9608, -528.8516, -522.0187, -521.5339],
        [-521.3192, -521.7052, -520.9056, -528.2214, -521.8951, -521.4791],
        [-520.3345, -520.5376, -519.8281, -526.7614, -520.7681, -520.2957],
        [-521.4382, -521.7241, -520.8795, -528.9729, -522.0979, -521.4633],
        [-520.6240, -520.9426, -520.1892, -527.4513, -521.2228, -520.7588],
        [-520.9153, -521.1013, -520.3448, -527.0940, -521.4858, -520.8287],
        [-522.6333, -522.8532, -521.8403, -527.4765, -522.8638, -522.3391],
        [-522.4055, -522.5507, -521.5801, -526.5479, -522.5656, -522.0518],
        [-522.1367, -522.1478, -521.4058, -526.9290, -522.3600, -521.9366],
        [-522.9034, -523.0815, -522.1410, -526.0823, -523.1002, -522.6006],
        [-522.1582, -522.3174, -521.5035, -525.5792, -522.4240, -521.9487],
        [-523.9780, -523.8874, -523.2262, -526.4726, -523.9899, -523.6263],
        [-523.3829, -523.3389, -522.6251, -525.5920, -523.4744, -522.9869],
        [-522.0768, -522.2096, -521.3560, -525.9590, -522.3650, -521.7720],
        [-522.2053, -522.2275, -521.5515, -525.6830, -522.4393, -521.9434],
        [-522.6755, -523.1570, -522.2594, -527.9950, -523.4271, -522.7504],
        [-521.7077, -521.9990, -521.1956, -527.2922, -522.3109, -521.6789],
        [-521.6476, -521.8535, -521.1394, -527.7660, -522.1270, -521.5696],
        [-524.5704, -525.2574, -525.0688, -525.3386, -523.6342, -524.6154],
        [-525.7878, -526.4006, -526.3697, -527.0524, -525.1755, -526.0049],
        [-524.0546, -524.5732, -524.5787, -524.6551, -523.3517, -524.1657],
        [-525.7177, -525.9247, -525.6044, -525.5009, -524.9387, -525.5154],
        [-523.9824, -524.1313, -524.0588, -523.8889, -523.4199, -523.8298],
        [-523.0269, -523.5460, -523.6178, -523.2845, -522.6123, -523.1333],
        [-522.2506, -522.8309, -522.5549, -522.2935, -521.6418, -522.1063],
        [-523.1140, -523.5384, -523.2206, -522.7379, -522.2407, -523.0510],
        [-525.7719, -526.3687, -526.0087, -525.9839, -524.9067, -525.7773],
        [-524.7278, -524.6464, -523.9220, -527.3052, -524.8695, -524.3689],
        [-522.9094, -522.9797, -522.1612, -525.5924, -523.0219, -522.6244],
        [-521.3854, -521.5876, -520.9447, -525.3447, -521.7448, -521.3347],
        [-520.8014, -520.8336, -520.1862, -525.3205, -521.0710, -520.6782],
        [-520.1759, -520.3618, -519.7210, -525.7208, -520.6428, -520.2172],
        [-522.2258, -522.4175, -521.5952, -526.5788, -522.5960, -522.0574],
        [-522.1158, -522.0150, -521.4371, -524.0870, -522.2116, -521.8074],
        [-522.4130, -522.5981, -521.8962, -525.6777, -522.7649, -522.2696],
        [-522.3042, -522.6063, -521.7994, -528.1255, -522.8885, -522.2260],
        [-521.6290, -522.0333, -521.2382, -526.6631, -522.2654, -521.6381]],
       device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([47, 6, 7]), scale: torch.Size([47, 6, 7]))
=============================================================================================
