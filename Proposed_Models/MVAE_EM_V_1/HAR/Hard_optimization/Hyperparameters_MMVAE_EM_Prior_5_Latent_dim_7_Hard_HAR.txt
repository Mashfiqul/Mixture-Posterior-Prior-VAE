Dataset: HAR
--------------------------------------------
Model Name: MMVAE_EM_Prior_5_Latent_dim_7_Hard_HAR
==================================================================
MMVAE_EM_Prior_5_Latent_dim_7_Hard_HAR Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 5
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 561
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 7
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 561
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 6
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-527.5487, -530.6638, -523.4025, -527.0042, -531.2743, -524.9325],
        [-534.8348, -533.2147, -527.5924, -532.5138, -527.5653, -524.8318],
        [-528.6698, -527.4875, -523.5878, -522.8192, -524.9108, -525.2629],
        [-532.5336, -528.0303, -523.2244, -526.7385, -526.6750, -526.0098],
        [-524.1836, -523.2657, -521.6256, -521.1922, -521.3645, -524.5651],
        [-528.7191, -526.9719, -524.2174, -527.3918, -527.0294, -527.0374],
        [-521.8099, -521.2057, -520.9252, -523.6340, -523.7851, -533.3641],
        [-522.3956, -520.8109, -520.7866, -522.0004, -522.7972, -520.3651],
        [-525.2295, -523.4586, -523.0422, -523.1724, -523.5269, -525.6548],
        [-527.6376, -527.7453, -521.7708, -531.5343, -540.8901, -525.8905],
        [-524.4016, -524.4412, -523.2919, -523.8572, -522.9617, -522.7686],
        [-528.2610, -523.7862, -526.4601, -523.6577, -525.8105, -523.7052],
        [-528.2075, -528.3471, -523.6902, -526.1737, -524.9363, -524.6753],
        [-518.5737, -518.9105, -518.4447, -521.9091, -519.6806, -519.8085],
        [-523.3500, -524.0545, -523.6758, -523.7418, -523.3467, -524.6763],
        [-533.8470, -526.5131, -523.1299, -527.8275, -527.1793, -527.5157],
        [-523.7477, -524.0275, -522.9765, -523.3549, -522.8413, -523.2301],
        [-528.5216, -528.4767, -523.7264, -526.6140, -524.4007, -523.7631],
        [-527.5177, -529.9124, -520.7222, -523.7230, -521.1242, -521.6624],
        [-535.1302, -532.6273, -531.4090, -536.4243, -538.5714, -533.6564],
        [-529.1528, -524.9861, -523.3352, -524.4941, -524.7028, -526.1400],
        [-531.8823, -521.0718, -520.5142, -521.1958, -522.5544, -527.7390],
        [-522.7159, -533.3269, -519.7379, -522.2614, -519.2026, -521.3042],
        [-529.2001, -523.5882, -522.0990, -525.7699, -522.2532, -522.4283],
        [-519.9446, -520.9965, -518.9346, -520.6083, -525.1259, -523.0266],
        [-524.7931, -523.1426, -522.6937, -525.7277, -532.7177, -522.9995],
        [-522.7368, -521.5962, -522.0148, -524.3313, -526.4047, -524.8658],
        [-533.6007, -528.7203, -522.3929, -524.9453, -533.6860, -527.0552],
        [-528.4286, -523.6769, -526.8395, -522.7203, -524.1319, -523.6996],
        [-522.3782, -521.5674, -520.8170, -521.5897, -532.4542, -520.8515],
        [-543.9705, -544.9319, -519.4370, -521.7367, -520.9486, -522.9899],
        [-529.4165, -526.6330, -524.5806, -524.5571, -539.7496, -525.5580],
        [-522.3279, -527.5316, -522.8298, -523.2896, -527.4993, -524.8280],
        [-527.3527, -532.8640, -525.6086, -529.1513, -528.4968, -526.8139],
        [-523.2921, -524.2266, -522.9750, -525.5052, -531.5938, -524.7754],
        [-526.7720, -526.2937, -525.7488, -523.8161, -528.2236, -524.2134],
        [-527.8748, -524.5043, -524.7283, -526.7588, -527.3169, -525.8972],
        [-553.0126, -523.5021, -521.9674, -522.7463, -524.0374, -524.3877],
        [-523.5536, -525.2679, -522.9672, -523.3649, -535.9800, -526.2384],
        [-528.9242, -536.1536, -523.5991, -535.4075, -530.3322, -524.3624],
        [-524.5739, -521.9688, -521.9655, -522.5758, -523.8043, -522.4113],
        [-533.6256, -530.6454, -524.0043, -531.5013, -523.7590, -524.2660],
        [-525.0028, -525.4598, -524.7491, -525.6989, -543.2524, -532.4124],
        [-522.1456, -526.1196, -520.4755, -521.8370, -520.8203, -521.5159],
        [-530.3109, -526.0490, -522.2742, -522.2557, -534.3207, -524.3448],
        [-527.2365, -530.2440, -521.8761, -528.3188, -524.0405, -524.4763],
        [-539.1346, -529.7546, -524.3131, -527.5213, -575.8050, -525.8361],
        [-524.0672, -528.0578, -526.9299, -525.2854, -525.7590, -523.7028],
        [-524.9573, -526.3895, -525.0543, -524.2617, -540.9102, -526.4362],
        [-526.1628, -533.1348, -525.8120, -534.4918, -533.2922, -526.2526],
        [-528.0128, -525.7590, -525.4929, -525.3383, -534.9434, -530.8187],
        [-529.2966, -528.3236, -523.3485, -529.1215, -523.5421, -524.5450]],
       device='cuda:0', grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([52, 6, 7]), scale: torch.Size([52, 6, 7]))
=============================================================================================
Decoder Loss (Test) = tensor([[-525.6866, -525.7961, -525.5914, -525.6190, -525.2361, -525.2008],
        [-527.5378, -527.4384, -527.5543, -527.4048, -527.3854, -527.4545],
        [-526.3744, -526.3034, -526.3557, -526.3775, -526.3688, -526.3124],
        [-524.0143, -523.9988, -523.9266, -523.9567, -523.9038, -523.7239],
        [-522.4884, -522.5128, -522.3987, -522.4305, -522.4835, -522.1953],
        [-522.8879, -523.0133, -522.8835, -522.8386, -522.8982, -522.8669],
        [-523.6904, -523.7189, -523.5393, -523.6410, -523.5536, -523.2136],
        [-525.7439, -525.7540, -525.7067, -525.6116, -525.5960, -525.4673],
        [-521.5957, -521.7819, -521.6147, -521.6816, -521.6147, -521.9030],
        [-521.5168, -521.7177, -521.5768, -521.8330, -521.6345, -522.0253],
        [-521.2334, -521.4261, -521.4021, -521.4268, -521.5137, -521.9297],
        [-520.9724, -521.1177, -521.0948, -521.1432, -521.1389, -521.4139],
        [-519.9229, -520.0737, -520.0760, -520.2944, -520.1738, -520.6005],
        [-521.0622, -521.2087, -521.1547, -521.1450, -521.1845, -521.5831],
        [-520.3082, -520.4636, -520.3921, -520.4058, -520.4067, -520.6667],
        [-520.5214, -520.7095, -520.6281, -520.7659, -520.6559, -520.9806],
        [-522.1028, -522.3151, -522.1758, -522.1166, -522.2482, -522.5312],
        [-521.8578, -522.1113, -521.9272, -521.9641, -521.9860, -522.3743],
        [-521.6326, -521.7523, -521.7264, -521.8199, -521.7792, -522.1693],
        [-522.4825, -522.6661, -522.5193, -522.6930, -522.5220, -522.8320],
        [-521.6830, -521.8391, -521.7172, -521.8287, -521.7239, -522.0320],
        [-523.4254, -523.5250, -523.4760, -523.5330, -523.4801, -523.6689],
        [-522.9299, -522.9689, -522.9637, -523.1344, -522.9651, -523.1080],
        [-521.5913, -521.7417, -521.6553, -521.7272, -521.7397, -522.0844],
        [-521.7092, -521.7910, -521.7952, -521.8248, -521.8380, -522.1442],
        [-522.4084, -522.7245, -522.3561, -522.7717, -522.3573, -522.6287],
        [-521.3672, -521.6202, -521.4103, -521.5592, -521.4399, -521.7159],
        [-521.3071, -521.5519, -521.3550, -521.3453, -521.3966, -521.6355],
        [-523.6666, -523.6913, -523.4476, -523.4783, -523.3911, -523.2695],
        [-525.1346, -525.3187, -524.9118, -524.9958, -524.6495, -524.6865],
        [-523.5666, -523.5916, -523.4351, -523.4308, -523.2748, -523.1184],
        [-525.0605, -525.1077, -525.0524, -524.9999, -525.0986, -524.9733],
        [-523.4420, -523.4329, -523.4257, -523.3435, -523.5203, -523.5646],
        [-522.4016, -522.3273, -522.3410, -522.2620, -522.3352, -522.3334],
        [-521.5032, -521.5139, -521.3746, -521.4608, -521.5001, -521.3445],
        [-522.2083, -522.2219, -522.0698, -522.2305, -522.3122, -521.8710],
        [-524.8264, -524.8484, -524.6458, -524.7672, -524.5386, -524.2704],
        [-524.1151, -524.1445, -524.2087, -524.1992, -524.2368, -524.4180],
        [-522.3315, -522.4295, -522.3726, -522.4404, -522.3633, -522.6125],
        [-520.9961, -521.1369, -521.0015, -521.1062, -521.0074, -521.3790],
        [-520.3463, -520.4685, -520.4241, -520.5214, -520.4651, -520.8574],
        [-519.7708, -519.8700, -519.8739, -519.9020, -519.8897, -520.1173],
        [-521.6992, -521.8647, -521.7734, -521.7796, -521.8477, -522.1263],
        [-521.5079, -521.5278, -521.6060, -521.6275, -521.6826, -521.9376],
        [-522.0737, -522.2388, -522.0532, -522.2851, -522.0752, -522.2979],
        [-521.9098, -522.2080, -521.9329, -522.0299, -521.9561, -522.1952],
        [-521.3973, -521.7542, -521.3323, -521.5560, -521.2903, -521.4788]],
       device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([47, 6, 7]), scale: torch.Size([47, 6, 7]))
=============================================================================================
