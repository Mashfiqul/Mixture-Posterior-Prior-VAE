Dataset: Reuters
--------------------------------------------
Model Name: MMVAE_random_seed_5_Latent_dim_5_Hard_Optimization_Beta_0.4_Reuters_dataset
==================================================================
MMVAE_random_seed_5_Latent_dim_5_Hard_Optimization_Beta_0.4_Reuters_dataset Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 5
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2500
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 50
--------------------------------------------
Input Dimension = 2000
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 2000
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
--------------------------------------------
beta_1 = 0.4
==================================================================

GMM Part:
==================================================================
Number of Components= 4
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2501
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-2229.3027, -2196.8855, -2196.7100, -2196.7781],
        [-2542.7837, -2103.4568, -2053.8975, -2061.8245],
        [-2731.1318, -2601.2998, -2599.3525, -2585.0493],
        [-2654.4968, -2625.3711, -2624.2698, -2622.6675],
        [-2614.3279, -2487.7639, -2481.7837, -2483.0598],
        [-2623.6406, -2599.9854, -2600.1711, -2601.1794],
        [-2691.2622, -2672.5117, -2672.2793, -2671.3650],
        [-2659.4580, -2650.9561, -2651.6162, -2653.1196],
        [-2543.8169, -2631.4048, -2633.9976, -2635.2292],
        [-2497.5234, -2482.3718, -2482.0195, -2481.2505],
        [-1987.9517, -1998.2854, -1991.7034, -1987.4282],
        [-2122.7207, -2180.6895, -2134.0669, -2127.8220],
        [-2632.6218, -2582.8953, -2580.6265, -2580.9297],
        [-2106.6096, -1987.4271, -1984.0647, -1983.8965],
        [-2979.7368, -2324.4521, -2322.0237, -2319.9468],
        [-2703.0977, -2692.5364, -2690.2397, -2688.4570],
        [-3034.3335, -2580.9954, -2577.8442, -2568.8323],
        [-2521.4653, -2499.7314, -2499.8828, -2499.8726],
        [-2621.3616, -2471.2471, -2459.0417, -2470.3062],
        [-2659.2456, -2515.2744, -2514.5532, -2514.1084],
        [-2336.8484, -2246.8469, -2245.1367, -2245.0491],
        [-2307.7861, -2368.8213, -2308.0425, -2314.2100],
        [-2689.2246, -2679.1875, -2679.8750, -2680.6631],
        [-2655.0320, -2594.7510, -2596.3672, -2594.1904],
        [-2621.1580, -2628.1421, -2627.3904, -2626.8901],
        [-2723.4917, -2720.6641, -2721.6824, -2720.9880],
        [-2613.5601, -2619.7600, -2618.0054, -2614.9751],
        [-2658.5518, -2649.5254, -2650.5684, -2649.7144],
        [-2542.8848, -2426.1167, -2376.7104, -2372.3228],
        [-2577.7034, -2513.6885, -2511.8662, -2511.2651],
        [-2403.0977, -2407.8025, -2402.5432, -2402.2700],
        [-2729.0474, -2643.3574, -2644.6240, -2639.9722],
        [-2567.1748, -2392.3257, -2389.9609, -2390.7295],
        [-2587.8311, -2528.3071, -2526.9668, -2526.8877],
        [-2197.6348, -2188.4233, -2187.0264, -2187.4351],
        [-2482.2947, -2428.1973, -2428.2874, -2426.4082],
        [-2618.4155, -2592.4148, -2591.9551, -2591.7639],
        [-2469.4600, -2540.8027, -2478.0493, -2485.8677],
        [-2325.4805, -2431.4265, -2435.1479, -2431.6665],
        [-2573.1057, -2574.3188, -2573.8057, -2574.2556],
        [-2401.9128, -2435.4233, -2405.3357, -2402.5464],
        [-2660.2188, -2670.3621, -2670.4890, -2671.3340],
        [-2600.8110, -2628.4575, -2627.8311, -2627.2578],
        [-2362.7891, -2363.2336, -2362.5713, -2364.7144],
        [-2591.9338, -2472.7664, -2472.2627, -2475.4370],
        [-2638.6499, -2621.8767, -2621.8857, -2621.9961],
        [-2539.5774, -2479.1577, -2475.2935, -2476.2188],
        [-2586.7307, -2412.8186, -2405.4312, -2407.6655],
        [-2081.7332, -2108.9761, -2094.1111, -2089.7974],
        [-2502.7612, -2449.0859, -2448.7075, -2446.4448],
        [-2680.9390, -2571.5190, -2569.5312, -2570.1499],
        [-2504.7080, -2509.6426, -2504.5547, -2508.7786],
        [-2629.1211, -2628.8779, -2628.1362, -2628.5889],
        [-2051.3052, -2039.6240, -2009.1255, -2005.3365],
        [-2191.6650, -2214.7563, -2191.3101, -2190.7673],
        [-2721.4326, -2645.9521, -2642.7134, -2635.9756],
        [-2318.4622, -2390.7163, -2319.2173, -2319.1294],
        [-2646.4026, -2552.9458, -2550.0513, -2549.3550],
        [-2680.4478, -2694.0129, -2693.5730, -2694.3147],
        [-2576.4277, -2563.5808, -2563.1411, -2565.7676],
        [-2585.2798, -2569.6968, -2569.3003, -2569.9431],
        [-2804.0010, -2143.7915, -2134.4072, -2135.4045],
        [-2630.0254, -2660.7935, -2660.9092, -2661.8318],
        [-2702.3809, -2697.2236, -2697.8459, -2696.4749],
        [-2625.3284, -2603.9897, -2604.2080, -2603.9646],
        [-2762.7974, -2772.0015, -2772.2520, -2772.3223],
        [-2649.9556, -2645.9321, -2648.5806, -2646.1182],
        [-2737.4375, -2743.2075, -2743.4351, -2743.6104],
        [-2620.4050, -2562.5938, -2562.7366, -2562.0161],
        [-2490.0298, -2536.7688, -2533.7134, -2535.1201],
        [-2373.4282, -2344.4346, -2341.3430, -2328.3647],
        [-2368.3979, -2302.8403, -2292.6550, -2291.9136],
        [-2075.5847, -2102.1858, -2075.6433, -2075.4375],
        [-2520.8398, -2464.1172, -2461.0190, -2460.0022],
        [-2737.2175, -2717.9810, -2718.2947, -2718.6628],
        [-2726.2778, -2696.4819, -2696.4399, -2696.5422],
        [-2681.4878, -2618.3374, -2617.3970, -2609.7004],
        [-2258.2915, -2157.9404, -2154.0325, -2153.6289],
        [-2583.8457, -2407.8735, -2386.3835, -2376.9727],
        [-2547.1814, -2433.0474, -2431.3491, -2432.2148],
        [-2105.1162, -2121.8315, -2104.8916, -2106.0457],
        [-2620.7207, -2602.0046, -2601.8398, -2601.7107],
        [-2362.2021, -2246.8403, -2243.0139, -2243.2480],
        [-2006.3425, -2042.0723, -2028.5359, -2019.8873],
        [-2604.8279, -2565.4446, -2563.8176, -2564.1311],
        [-2727.4741, -2428.7805, -2425.7109, -2426.7354],
        [-2505.6909, -2539.8091, -2539.6392, -2540.6467],
        [-3025.9153, -2504.1863, -2508.5281, -2499.3240],
        [-2425.5083, -2438.5015, -2438.5161, -2437.2529],
        [-2693.2859, -2734.5596, -2734.1321, -2737.2351],
        [-2657.6963, -2672.6831, -2671.3179, -2672.1211],
        [-2686.3098, -2678.9766, -2680.1333, -2679.7690],
        [-2724.5166, -2727.8999, -2728.5000, -2728.2476],
        [-2618.2251, -2638.0352, -2637.7490, -2637.8896],
        [-2640.9395, -2642.5845, -2641.9590, -2641.4414],
        [-2638.3213, -2611.8818, -2612.6079, -2612.3013],
        [-2572.2305, -2605.6270, -2604.9106, -2605.2605],
        [-2490.3643, -2480.6323, -2480.5776, -2479.6943],
        [-2749.8066, -2590.1741, -2588.2417, -2588.6787],
        [-2643.3191, -2599.2959, -2600.0637, -2599.2573]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-2815.4224, -2810.0361, -2809.6812, -2812.7490],
        [-2645.3467, -2659.2466, -2660.7168, -2661.7520],
        [-2348.2166, -2332.8359, -2320.2715, -2319.8706],
        [-2271.7642, -2132.2219, -2127.3069, -2114.0630],
        [-2791.0010, -2805.1912, -2805.1333, -2806.4697],
        [-2819.8862, -2736.5608, -2739.5557, -2738.8584],
        [-2604.4001, -2604.6973, -2612.3779, -2612.7915],
        [-2855.0491, -2588.6206, -2616.0562, -2570.0151],
        [-2586.8740, -2488.1360, -2493.8635, -2496.5234],
        [-2585.7368, -2630.6260, -2628.9414, -2665.5391],
        [-2620.1309, -2579.0542, -2585.8452, -2593.5049],
        [-2657.4116, -2553.0015, -2572.5159, -2588.5483],
        [-2818.3271, -2822.8760, -2818.8218, -2829.5474],
        [-1966.3815, -1997.0653, -1966.4329, -1967.9318],
        [-2710.7297, -2714.7114, -2715.2061, -2709.9780],
        [-2811.7144, -2653.2527, -2676.6548, -2628.8479],
        [-2756.6360, -2768.0146, -2769.3120, -2768.5254],
        [-2793.6826, -2799.7227, -2800.3545, -2799.2188],
        [-2659.7178, -2606.4663, -2603.2073, -2609.6626],
        [-2774.4587, -2712.7397, -2712.9639, -2712.9187],
        [-2440.5518, -2423.3604, -2449.5237, -2444.1753],
        [-2044.9224, -2047.0876, -2044.1454, -2045.2733],
        [-2611.0137, -2588.0698, -2587.1404, -2592.2075],
        [-2560.2527, -2652.4204, -2596.9465, -2601.8071],
        [-2618.6094, -2445.9648, -2468.8469, -2468.6724],
        [-2229.6870, -2222.3081, -2234.1831, -2230.8918],
        [-2607.5908, -2543.4219, -2576.3918, -2572.7224],
        [-2669.6602, -2633.7666, -2633.8403, -2633.2373],
        [-2646.6235, -2451.6421, -2445.6826, -2446.3875],
        [-2651.4023, -2559.9519, -2557.1436, -2562.7183],
        [-2761.6526, -2759.2141, -2760.7549, -2744.6592],
        [-2598.0833, -2662.8132, -2630.3699, -2626.8386],
        [-2385.9336, -2450.5808, -2384.7007, -2385.6206],
        [-2685.4312, -2653.3274, -2656.5842, -2654.9189],
        [-2731.4600, -2582.4229, -2575.9033, -2579.8472],
        [-2780.9675, -2690.6956, -2684.0000, -2675.8572],
        [-2580.9731, -2591.2969, -2583.7693, -2587.4038],
        [-2771.1543, -2725.9373, -2733.9146, -2726.8794],
        [-2674.9773, -2671.0542, -2667.6914, -2669.0684],
        [-2552.0906, -2559.3289, -2548.7605, -2555.8003],
        [-2618.6826, -2542.4868, -2543.0850, -2547.0759],
        [-2543.7856, -2511.2095, -2509.7632, -2508.9941],
        [-2694.2534, -2690.8193, -2696.0767, -2693.4644],
        [-2744.7856, -2761.4297, -2758.3921, -2767.9375],
        [-2549.2812, -2590.1187, -2588.5894, -2589.4797],
        [-2690.3428, -2695.0654, -2687.4077, -2683.9734],
        [-2738.9561, -2750.0439, -2750.5986, -2751.7639],
        [-2454.4673, -2451.0962, -2461.3374, -2460.9561],
        [-2287.8125, -2325.3647, -2290.5298, -2292.0457],
        [-2767.3164, -2719.6982, -2720.3740, -2724.0596],
        [-2478.5681, -2427.4968, -2443.3833, -2445.7090],
        [-2661.5176, -2652.1396, -2656.9968, -2657.8623],
        [-2620.3213, -2633.3911, -2624.3369, -2628.2517],
        [-2749.0767, -2692.1877, -2702.5100, -2703.1865],
        [-2762.5869, -2746.1436, -2738.2441, -2784.1069],
        [-2802.8909, -2815.9683, -2815.7617, -2814.3313],
        [-2670.9517, -2568.9592, -2565.7837, -2567.8167],
        [-2322.8569, -2120.7539, -2095.8423, -2102.4268],
        [-2712.9023, -2410.3564, -2392.5042, -2386.7861],
        [-2709.4639, -2485.5156, -2489.1985, -2480.1873],
        [-2587.3354, -2576.5632, -2568.3457, -2567.7527],
        [-2908.0911, -2846.9612, -2848.2568, -2854.8057],
        [-2698.7129, -2655.2075, -2656.5276, -2657.9426],
        [-2819.8813, -2913.3625, -2918.8159, -2916.0625],
        [-2744.7983, -2789.4741, -2793.9790, -2791.8530],
        [-2368.7456, -2271.3762, -2296.7290, -2252.6819],
        [-2761.9978, -2714.4976, -2743.2283, -2742.0176],
        [-2695.0918, -2689.5317, -2695.8391, -2695.0933],
        [-2488.6641, -2510.3020, -2497.1372, -2493.9775],
        [-2813.5464, -2834.3374, -2842.6606, -2846.0098],
        [-2632.4382, -2502.6194, -2498.1729, -2506.8101],
        [-2681.7065, -2695.7966, -2695.2773, -2695.3984],
        [-2641.2207, -2655.7783, -2660.7236, -2657.1982],
        [-2688.6226, -2558.5117, -2547.9124, -2557.3247],
        [-2743.8455, -2790.4417, -2785.8247, -2791.4756],
        [-2432.2026, -2467.4604, -2423.5156, -2422.6453],
        [-2711.7886, -2679.8132, -2677.8804, -2678.7024],
        [-2593.0054, -2563.5601, -2584.6187, -2581.6868],
        [-2603.7422, -2457.9971, -2461.8379, -2465.3928],
        [-2759.1694, -2712.8250, -2716.8369, -2717.3682],
        [-2413.2456, -2325.5559, -2316.9150, -2318.0122],
        [-2453.1819, -2476.0457, -2453.9365, -2452.3599],
        [-2930.0972, -2838.2104, -2847.0786, -2834.7725],
        [-2699.3760, -2674.1274, -2671.9895, -2671.7539],
        [-2628.4417, -2627.1907, -2630.8496, -2633.5564],
        [-2710.1235, -2716.7793, -2719.3132, -2718.6987],
        [-2808.9644, -2341.5776, -2360.5327, -2311.6455],
        [-2014.1389, -2003.2700, -1987.8888, -1979.4647],
        [-2852.4067, -2876.3967, -2884.1660, -2879.5542],
        [-2162.4138, -2167.6953, -2163.1316, -2163.0073],
        [-2356.1812, -2252.2739, -2245.0151, -2244.3672],
        [-2792.2405, -2795.4209, -2800.4604, -2796.6929],
        [-2852.8337, -2611.9600, -2623.4851, -2593.4746],
        [-2825.3354, -2285.3159, -2288.2188, -2289.1499],
        [-2420.7014, -2346.8354, -2344.3359, -2344.7451],
        [-2650.7615, -2611.7075, -2606.1868, -2613.9480],
        [-2692.8145, -2632.8213, -2639.9500, -2659.1145],
        [-2457.8677, -2375.8965, -2375.3311, -2376.7173],
        [-2721.7903, -2702.3618, -2715.0525, -2718.0156],
        [-2756.5547, -2731.1401, -2740.1182, -2740.2207]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
