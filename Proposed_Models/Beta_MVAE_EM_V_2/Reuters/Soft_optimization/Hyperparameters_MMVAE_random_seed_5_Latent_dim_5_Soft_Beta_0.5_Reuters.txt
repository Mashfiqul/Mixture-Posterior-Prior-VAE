Dataset: Reuters
--------------------------------------------
Model Name: MMVAE_random_seed_5_Latent_dim_5_Soft_Optimization_Beta_0.5_Reuters_dataset
==================================================================
MMVAE_random_seed_5_Latent_dim_5_Soft_Optimization_Beta_0.5_Reuters_dataset Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 5
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2500
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 100
--------------------------------------------
Input Dimension = 2000
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 2000
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
--------------------------------------------
beta_1 = 0.5
==================================================================

GMM Part:
==================================================================
Number of Components= 4
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2501
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-2214.2366, -2214.4077, -2214.4307, -2214.2734],
        [-1988.5105, -1989.9620, -1993.2085, -1989.3193],
        [-2565.7236, -2565.2505, -2569.4368, -2565.6399],
        [-2629.8093, -2631.2178, -2629.5415, -2629.0662],
        [-2359.7434, -2358.3403, -2364.8215, -2363.0315],
        [-2583.2031, -2583.5562, -2583.8677, -2584.0930],
        [-2668.1958, -2667.3369, -2668.3638, -2668.0371],
        [-2553.5039, -2554.2500, -2551.1040, -2551.0469],
        [-2495.1426, -2494.6597, -2496.4316, -2494.5779],
        [-2641.3901, -2638.2422, -2638.9509, -2638.0613],
        [-1990.6918, -1989.9326, -1990.8683, -1990.9705],
        [-2115.5430, -2114.9783, -2115.5142, -2114.4492],
        [-2585.1501, -2584.4006, -2584.9126, -2584.8928],
        [-1989.2550, -1992.6924, -1989.4863, -1989.9008],
        [-2331.7754, -2330.2686, -2330.1870, -2328.9121],
        [-2706.8799, -2706.2097, -2706.4902, -2707.9175],
        [-2637.4077, -2639.0205, -2637.3972, -2637.2708],
        [-2470.8389, -2470.6221, -2471.3721, -2471.1851],
        [-2354.6411, -2353.0298, -2358.9907, -2353.7109],
        [-2493.7314, -2493.4438, -2494.7524, -2493.0242],
        [-2226.3560, -2229.7578, -2228.7769, -2227.8003],
        [-2302.3042, -2302.2837, -2302.7417, -2303.1953],
        [-2653.7490, -2652.7622, -2652.3118, -2653.8088],
        [-2518.6064, -2517.9324, -2518.9624, -2517.3225],
        [-2604.7456, -2605.5952, -2605.6816, -2605.2378],
        [-2654.1680, -2654.2441, -2654.7686, -2655.4060],
        [-2570.0981, -2570.0225, -2572.2800, -2570.9395],
        [-2587.0518, -2587.5430, -2586.3718, -2585.9775],
        [-2335.8308, -2336.6145, -2337.0303, -2336.9653],
        [-2468.8730, -2468.1646, -2469.7998, -2468.0730],
        [-2400.3267, -2398.2332, -2398.2061, -2398.5913],
        [-2615.9785, -2616.3669, -2617.7490, -2617.9102],
        [-2337.7937, -2337.8315, -2338.1091, -2337.3848],
        [-2518.4543, -2518.4033, -2518.3408, -2517.8538],
        [-2176.2732, -2176.9487, -2176.4307, -2176.6260],
        [-2413.2080, -2413.1162, -2414.1226, -2413.5608],
        [-2565.3413, -2562.0825, -2562.1382, -2563.9146],
        [-2394.5518, -2393.9470, -2394.1846, -2394.0376],
        [-2340.3684, -2339.9995, -2340.8984, -2340.2476],
        [-2608.3523, -2607.0952, -2607.9497, -2607.3608],
        [-2405.0884, -2404.7656, -2404.9980, -2404.8232],
        [-2658.2837, -2656.9336, -2656.8965, -2657.3000],
        [-2576.2764, -2575.8706, -2577.1655, -2576.4189],
        [-2350.2639, -2350.6604, -2350.4331, -2353.1060],
        [-2477.8716, -2477.9790, -2478.2305, -2478.4353],
        [-2522.5601, -2522.6606, -2522.3813, -2522.2432],
        [-2434.3435, -2434.8862, -2433.6025, -2432.6099],
        [-2282.1335, -2281.8311, -2282.8982, -2281.3831],
        [-2082.8516, -2082.8674, -2085.4727, -2082.9709],
        [-2387.1658, -2386.6084, -2387.7583, -2386.9331],
        [-2533.2583, -2533.6338, -2534.0713, -2533.1357],
        [-2499.7910, -2499.8262, -2499.6567, -2502.2759],
        [-2545.8882, -2545.5117, -2545.2798, -2546.4414],
        [-1993.2739, -1997.3892, -1999.0437, -1995.7777],
        [-2180.7229, -2180.5022, -2180.0876, -2180.0669],
        [-2619.0593, -2621.7212, -2620.4458, -2619.6362],
        [-2318.6802, -2319.4043, -2319.0593, -2318.9019],
        [-2552.3464, -2552.0454, -2552.1697, -2552.5652],
        [-2629.7954, -2629.2263, -2631.1824, -2632.6321],
        [-2571.6226, -2571.7864, -2571.0867, -2574.6685],
        [-2426.8396, -2426.7429, -2426.8767, -2426.6113],
        [-2100.4014, -2100.0696, -2101.4563, -2099.8433],
        [-2630.4932, -2631.9316, -2630.9551, -2630.5000],
        [-2689.9434, -2691.4385, -2691.8335, -2691.8196],
        [-2595.8662, -2594.4219, -2594.0278, -2593.9502],
        [-2670.0088, -2670.0225, -2670.7434, -2670.6702],
        [-2627.0698, -2627.9324, -2628.1152, -2627.0254],
        [-2720.5198, -2720.7283, -2722.6897, -2721.1589],
        [-2541.4448, -2540.2346, -2541.0420, -2540.2231],
        [-2416.4553, -2416.0381, -2417.7275, -2415.9001],
        [-2326.9321, -2328.9624, -2332.4661, -2327.7134],
        [-2264.9443, -2263.5981, -2263.7075, -2263.7720],
        [-2058.8838, -2058.8384, -2060.2964, -2058.5649],
        [-2473.9800, -2473.3325, -2474.6248, -2474.1855],
        [-2645.6665, -2645.6633, -2646.1218, -2645.8289],
        [-2670.7700, -2670.3838, -2670.4290, -2670.0488],
        [-2631.4253, -2631.7031, -2631.5918, -2631.3940],
        [-2152.3484, -2152.4644, -2152.7419, -2154.0444],
        [-2375.9019, -2377.0156, -2376.3501, -2375.5640],
        [-2365.3145, -2364.2095, -2364.6792, -2365.0732],
        [-2096.8193, -2096.1904, -2097.0239, -2096.8306],
        [-2571.9624, -2572.2173, -2572.4731, -2572.1475],
        [-2232.0229, -2233.4238, -2236.6262, -2234.6387],
        [-2010.1056, -2010.8064, -2011.2960, -2012.0740],
        [-2417.2007, -2417.5171, -2421.2449, -2417.4790],
        [-2439.2478, -2438.0693, -2437.9546, -2437.5403],
        [-2476.0239, -2475.8149, -2477.7251, -2475.5681],
        [-2583.3701, -2583.9746, -2583.8960, -2583.8320],
        [-2293.1274, -2291.8711, -2292.9102, -2292.3184],
        [-2675.4285, -2676.5293, -2675.6448, -2675.3462],
        [-2627.0759, -2626.3552, -2626.8745, -2626.6243],
        [-2507.7908, -2507.3396, -2508.1982, -2508.0493],
        [-2676.2063, -2676.3997, -2676.0518, -2677.0393],
        [-2607.0188, -2607.7495, -2607.2822, -2606.8589],
        [-2642.3398, -2642.8076, -2642.2417, -2643.5227],
        [-2576.0935, -2574.6235, -2574.3042, -2575.4814],
        [-2533.5420, -2532.7485, -2533.4424, -2533.5503],
        [-2457.4204, -2457.7266, -2463.2454, -2457.7720],
        [-2596.0857, -2595.0847, -2596.8398, -2595.0166],
        [-2586.7539, -2586.4653, -2586.8606, -2586.7280]], device='cuda:1',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-2857.3550, -2856.9624, -2854.9563, -2857.8428],
        [-2644.4194, -2644.2544, -2644.8787, -2643.8540],
        [-2316.3994, -2312.3428, -2320.2859, -2317.4751],
        [-2119.5137, -2120.5820, -2119.4636, -2120.2974],
        [-2832.8279, -2832.5095, -2836.2688, -2831.9702],
        [-2745.7485, -2745.5337, -2745.0154, -2745.7744],
        [-2649.6416, -2650.1758, -2649.5215, -2650.5667],
        [-2518.8928, -2518.4629, -2518.4006, -2518.9373],
        [-2498.6453, -2495.9912, -2490.0977, -2499.8230],
        [-2466.8525, -2469.0850, -2465.6072, -2466.9607],
        [-2598.1328, -2600.4150, -2597.3601, -2596.7085],
        [-2457.0454, -2452.3652, -2464.3032, -2461.4189],
        [-2803.1201, -2804.3884, -2808.6875, -2803.4375],
        [-1954.4552, -1955.1543, -1954.4254, -1954.4832],
        [-2727.5789, -2727.7900, -2731.2437, -2727.9160],
        [-2671.4702, -2671.1199, -2676.6614, -2666.3823],
        [-2769.9956, -2770.4011, -2768.8718, -2770.6689],
        [-2881.4985, -2878.0454, -2874.9028, -2877.3149],
        [-2658.2625, -2662.1235, -2662.6826, -2660.6221],
        [-2708.7056, -2710.6836, -2711.6047, -2711.7646],
        [-2389.8752, -2389.4199, -2388.8931, -2389.1262],
        [-2045.3186, -2045.6960, -2045.5186, -2045.5166],
        [-2644.4373, -2643.6626, -2641.1294, -2648.2354],
        [-2538.4634, -2536.9370, -2538.7332, -2537.7666],
        [-2424.5215, -2425.5942, -2414.7200, -2416.5093],
        [-2155.3242, -2157.3445, -2157.5789, -2154.6128],
        [-2536.7949, -2541.0994, -2551.7271, -2538.5005],
        [-2709.6113, -2711.1704, -2709.5815, -2711.6086],
        [-2291.5232, -2290.5051, -2292.5081, -2291.6738],
        [-2646.9702, -2651.9070, -2652.4028, -2655.8357],
        [-2798.3740, -2802.6560, -2801.8970, -2799.7119],
        [-2622.3870, -2623.1050, -2622.2300, -2621.6316],
        [-2398.6250, -2397.8020, -2400.0146, -2400.6797],
        [-2627.9231, -2626.9114, -2627.2207, -2627.9517],
        [-2481.7290, -2483.3755, -2485.2078, -2486.7805],
        [-2742.2122, -2742.4355, -2747.1414, -2743.0151],
        [-2548.4556, -2547.9702, -2547.2324, -2548.0808],
        [-2651.8462, -2652.2000, -2661.1472, -2650.4116],
        [-2705.9990, -2703.5708, -2698.7285, -2705.7920],
        [-2593.9336, -2594.4053, -2595.5183, -2593.9651],
        [-2805.2781, -2815.5952, -2786.2583, -2818.2983],
        [-2482.0342, -2482.1184, -2484.8323, -2481.8188],
        [-2662.6057, -2663.6387, -2663.7026, -2663.7827],
        [-2836.3350, -2835.4604, -2835.1934, -2838.1064],
        [-2523.1426, -2522.7263, -2524.5864, -2524.3950],
        [-2753.4536, -2750.6841, -2749.8911, -2752.1362],
        [-2633.6123, -2633.4065, -2634.7622, -2633.1636],
        [-2417.2510, -2416.7134, -2415.0601, -2415.7971],
        [-2275.9746, -2274.9678, -2274.6943, -2275.0034],
        [-2861.8394, -2867.1567, -2863.7944, -2865.0645],
        [-2457.7056, -2459.5046, -2452.3901, -2453.1162],
        [-2624.0947, -2622.8833, -2620.7053, -2623.5017],
        [-2670.5273, -2674.1318, -2660.3977, -2669.1855],
        [-2776.7112, -2776.1111, -2779.3530, -2782.3357],
        [-2787.6643, -2783.5547, -2782.5586, -2786.4790],
        [-2813.8721, -2815.0015, -2811.0386, -2814.9331],
        [-2756.2266, -2758.2227, -2767.2837, -2764.6157],
        [-2095.1592, -2101.7622, -2094.8818, -2095.4065],
        [-2394.4502, -2384.7178, -2409.3643, -2392.6394],
        [-2485.7500, -2483.4978, -2479.9565, -2484.9460],
        [-2636.7695, -2635.4309, -2635.0986, -2635.5190],
        [-2799.3196, -2802.5522, -2799.3586, -2800.4507],
        [-2657.5781, -2660.9695, -2664.8677, -2657.4006],
        [-2778.0044, -2775.8901, -2788.1562, -2791.0481],
        [-2677.3184, -2674.1714, -2674.6611, -2675.9971],
        [-2265.5239, -2276.6250, -2260.6438, -2264.9001],
        [-2716.6392, -2714.2625, -2710.8179, -2714.0957],
        [-2703.9326, -2700.6650, -2703.0522, -2702.4211],
        [-2458.3140, -2467.5427, -2463.9832, -2460.5791],
        [-2831.1860, -2831.4182, -2832.4578, -2831.6187],
        [-2558.5554, -2558.8813, -2560.7593, -2574.2432],
        [-2690.7805, -2690.5928, -2690.5107, -2690.5674],
        [-2665.5723, -2662.8804, -2664.8145, -2666.0459],
        [-2552.9751, -2552.4797, -2553.0156, -2550.7144],
        [-2816.3464, -2818.9148, -2821.7473, -2824.5422],
        [-2437.1973, -2437.2214, -2436.0452, -2436.0107],
        [-2696.3047, -2695.8953, -2692.8232, -2697.0938],
        [-2778.0366, -2805.1318, -2810.4575, -2793.1868],
        [-2362.0601, -2358.8789, -2356.8669, -2357.0405],
        [-2792.8813, -2793.0923, -2793.1289, -2792.1741],
        [-2144.1536, -2144.4731, -2146.8652, -2145.5593],
        [-2441.0474, -2439.8960, -2440.4673, -2440.4033],
        [-2689.6567, -2694.3389, -2689.9165, -2689.3833],
        [-2625.6831, -2625.7920, -2625.4702, -2625.7190],
        [-2650.5088, -2648.1357, -2649.9666, -2649.5842],
        [-2763.2671, -2762.1667, -2760.5703, -2763.3291],
        [-2256.6191, -2291.8164, -2354.9709, -2260.7988],
        [-1980.7677, -1980.4852, -1980.7869, -1980.9344],
        [-2951.8787, -2956.1431, -2953.3015, -2951.7017],
        [-2183.1577, -2180.1973, -2178.2593, -2181.7119],
        [-2278.8333, -2267.5029, -2280.4531, -2278.3137],
        [-2835.7249, -2834.6899, -2834.1606, -2834.2446],
        [-2640.3582, -2637.3125, -2648.4109, -2640.2798],
        [-2307.1416, -2311.9497, -2307.0786, -2304.9470],
        [-2345.2397, -2343.0129, -2338.6057, -2344.4048],
        [-2638.7041, -2637.5654, -2637.1577, -2639.0681],
        [-2604.5120, -2608.4949, -2597.9260, -2604.3730],
        [-2467.3511, -2467.7070, -2479.6685, -2464.1418],
        [-2788.9658, -2788.4990, -2787.1089, -2789.7395],
        [-2827.6450, -2823.9836, -2827.3147, -2825.5549]], device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
