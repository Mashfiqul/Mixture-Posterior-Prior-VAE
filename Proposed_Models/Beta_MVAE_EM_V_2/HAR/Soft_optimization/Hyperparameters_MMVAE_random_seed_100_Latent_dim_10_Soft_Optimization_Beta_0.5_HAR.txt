Dataset: HAR
--------------------------------------------
Model Name: MMVAE_random_seed_100_Latent_dim_10_Soft_Optimization_Beta_0.5_HAR
==================================================================
MMVAE_random_seed_100_Latent_dim_10_Soft_Optimization_Beta_0.5_HAR Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 100
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 30
--------------------------------------------
Input Dimension = 561
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 10
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 561
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
--------------------------------------------
beta_1 = 0.5
==================================================================

GMM Part:
==================================================================
Number of Components= 6
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-538.7366, -535.8375, -532.7721, -519.5541, -538.6088, -539.9183],
        [-522.2947, -522.9053, -522.9120, -539.2416, -522.2087, -522.3969],
        [-566.3604, -562.7960, -562.9229, -519.1404, -573.7194, -571.5154],
        [-524.5881, -521.8369, -521.7205, -538.3041, -524.6133, -523.9554],
        [-570.9587, -563.7430, -563.6882, -518.0367, -577.7010, -576.0780],
        [-519.5359, -542.5564, -553.5526, -559.1187, -520.3517, -529.4374],
        [-562.9758, -559.8948, -560.6460, -519.2751, -568.9763, -566.4136],
        [-523.8397, -525.1487, -525.4132, -535.5538, -524.4647, -523.8040],
        [-568.3784, -563.1078, -563.7968, -519.2003, -575.5864, -572.0817],
        [-520.8815, -524.1022, -524.2522, -539.8236, -521.6188, -521.0801],
        [-521.8734, -542.6297, -589.8948, -548.3314, -530.0924, -528.4398],
        [-518.5745, -521.0078, -536.4305, -535.4099, -519.6809, -520.2286],
        [-521.4651, -522.5162, -522.5555, -535.3485, -520.7716, -521.6240],
        [-520.2079, -523.4897, -544.7983, -539.7847, -520.6172, -520.4080],
        [-518.7834, -523.4399, -522.9360, -530.9434, -518.9490, -520.0617],
        [-561.3981, -555.6525, -555.0677, -518.3829, -565.7961, -563.1565],
        [-548.5999, -548.0852, -548.0715, -522.7466, -549.3525, -549.8099],
        [-559.8602, -556.2172, -554.5048, -518.2533, -565.5754, -562.3707],
        [-556.9443, -553.1953, -552.8160, -519.2696, -561.6193, -559.0983],
        [-520.0051, -522.2205, -520.9747, -537.6129, -520.3690, -520.3845],
        [-524.6874, -523.6871, -520.6941, -536.2681, -524.5164, -523.4081],
        [-558.2460, -553.1318, -553.3717, -519.9012, -561.0374, -560.1460],
        [-567.6700, -560.1513, -559.1122, -519.2561, -573.2963, -570.6677],
        [-524.6779, -524.3475, -524.9509, -536.4109, -525.5345, -524.2222],
        [-554.7230, -553.6493, -552.9801, -520.4711, -561.3550, -558.2664],
        [-532.1201, -536.5521, -522.4113, -543.7033, -535.0244, -529.7935],
        [-565.1098, -559.5184, -558.8406, -518.7434, -570.1041, -566.4171],
        [-521.7059, -521.9276, -522.4951, -532.0465, -522.7311, -522.4324],
        [-556.6448, -549.9417, -551.1222, -518.9153, -559.4551, -555.8322],
        [-539.8061, -536.2816, -533.3583, -521.2792, -539.1750, -540.6226],
        [-550.5645, -550.1853, -549.1512, -522.1625, -555.7712, -555.2904],
        [-525.0074, -524.6212, -541.8602, -542.0543, -524.6296, -524.5000],
        [-547.5796, -543.4557, -544.1003, -524.6841, -547.3096, -547.1106],
        [-521.2188, -521.0785, -523.4659, -541.0288, -521.8546, -520.5591],
        [-521.9023, -522.9243, -523.6613, -539.0095, -523.1484, -522.0417],
        [-524.1373, -522.5394, -518.4691, -538.1410, -522.9152, -522.0560],
        [-522.9313, -522.4224, -527.8925, -534.3308, -524.2761, -523.1151],
        [-550.9565, -550.3574, -553.1783, -521.5525, -554.4470, -553.8237],
        [-562.5778, -556.4866, -555.0605, -518.8456, -565.7626, -565.0972],
        [-546.0699, -542.9244, -544.3916, -520.8108, -547.5101, -546.8976],
        [-521.9467, -522.6107, -523.0614, -528.6356, -522.1147, -521.5972],
        [-559.7146, -555.1577, -554.8029, -518.8428, -561.2929, -561.2202],
        [-519.8790, -523.7838, -529.5891, -537.8745, -518.6856, -520.0905],
        [-560.2922, -556.1128, -554.2440, -518.6304, -563.4991, -562.1448],
        [-544.3977, -537.8589, -536.4400, -519.3049, -547.0305, -545.8292],
        [-568.4874, -561.6393, -561.1504, -519.4087, -573.9500, -571.5393],
        [-567.3418, -563.4977, -563.1436, -521.1693, -573.8625, -570.3744],
        [-577.1229, -567.3270, -569.7350, -519.6166, -579.7243, -578.7371],
        [-527.4462, -524.9403, -520.9548, -541.6328, -526.2416, -525.2040],
        [-562.7383, -558.3318, -556.8925, -519.3642, -567.8627, -566.1788],
        [-522.1302, -536.4449, -552.5269, -534.8837, -523.8528, -528.9852],
        [-559.5382, -553.5657, -553.4752, -522.4292, -563.6281, -561.7418]],
       device='cuda:1', grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([52, 6, 10]), scale: torch.Size([52, 6, 10]))
=============================================================================================
Decoder Loss (Test) = tensor([[-526.4125, -525.2380, -527.3221, -535.1550, -527.6605, -527.1725],
        [-526.3396, -526.3737, -527.1158, -534.3936, -529.4335, -526.8699],
        [-525.0728, -524.8109, -525.7209, -532.9044, -527.7037, -525.4473],
        [-525.1693, -523.7888, -525.7748, -533.0924, -526.0173, -525.4837],
        [-521.9213, -521.6851, -522.8989, -530.8171, -522.7396, -522.2223],
        [-523.0604, -522.2018, -523.2817, -532.6608, -523.7651, -523.5208],
        [-524.9481, -522.9134, -524.5927, -535.4111, -526.0051, -525.7074],
        [-527.9399, -525.0380, -528.2212, -534.0582, -528.7986, -528.1100],
        [-522.0985, -522.2788, -522.5926, -537.0468, -521.5418, -521.8578],
        [-520.8991, -522.0250, -522.7302, -538.3275, -520.3673, -520.6012],
        [-521.2235, -521.3575, -521.6983, -540.6246, -520.7212, -520.9606],
        [-522.2476, -521.1729, -521.4250, -539.6152, -521.3845, -521.5845],
        [-520.1282, -520.3021, -520.8384, -539.2493, -519.5443, -519.8944],
        [-520.2239, -520.7616, -521.2419, -539.8694, -520.5964, -520.4524],
        [-520.8820, -520.6279, -520.8632, -538.7038, -520.4108, -520.5804],
        [-519.9543, -520.7822, -521.7942, -538.9994, -520.2645, -520.1278],
        [-520.7480, -521.3193, -522.0052, -536.7899, -521.4535, -521.1285],
        [-520.6441, -521.5217, -522.3016, -536.6478, -521.2087, -520.9430],
        [-521.8702, -522.0245, -522.4158, -538.8784, -521.2511, -521.6427],
        [-522.0944, -522.4646, -524.0515, -537.1405, -521.6698, -521.8909],
        [-521.7513, -522.0604, -522.7762, -536.7726, -521.2687, -521.5576],
        [-523.2531, -523.4118, -524.1732, -536.5611, -522.8194, -523.1189],
        [-522.5662, -523.0687, -524.3717, -536.2933, -522.2332, -522.4685],
        [-520.6401, -521.0834, -522.2769, -536.6931, -520.9822, -520.8269],
        [-521.5323, -522.0092, -522.8310, -536.3171, -521.3333, -521.5986],
        [-521.9349, -522.8533, -523.4579, -539.5681, -522.0981, -522.0366],
        [-520.4078, -521.4153, -522.2797, -538.3009, -520.9219, -520.6850],
        [-520.3984, -521.1149, -521.4416, -537.8027, -521.0193, -520.7114],
        [-523.2391, -521.9706, -524.8870, -533.1143, -525.4012, -523.4441],
        [-525.9341, -524.1063, -527.4106, -535.1122, -527.6488, -526.5029],
        [-523.6976, -522.6425, -525.0968, -531.4876, -524.7819, -524.0365],
        [-523.7052, -523.7515, -525.2355, -533.9916, -524.3898, -524.0782],
        [-523.2485, -523.2858, -523.5272, -532.0873, -523.6482, -523.4594],
        [-522.9137, -522.4872, -523.4160, -530.7572, -523.6681, -523.2621],
        [-520.6849, -520.9318, -521.5776, -530.7273, -521.2740, -520.8517],
        [-522.6412, -521.7775, -522.9794, -530.8872, -523.4580, -522.8068],
        [-526.1333, -523.0893, -526.6246, -533.6559, -527.5871, -526.3823],
        [-523.8484, -524.3450, -525.4072, -538.1515, -523.5208, -523.6630],
        [-522.3845, -522.8368, -523.8082, -536.0715, -522.0596, -522.2545],
        [-521.6942, -521.6096, -521.6790, -536.1081, -521.1110, -521.3369],
        [-520.5005, -520.7239, -521.2245, -535.9162, -519.8426, -520.4146],
        [-520.6384, -520.1507, -520.8884, -537.2106, -520.0649, -520.2619],
        [-521.2959, -521.6619, -522.4564, -537.5648, -521.8163, -521.5082],
        [-521.6277, -522.3041, -522.6811, -535.1547, -521.3527, -521.6697],
        [-521.7419, -522.4791, -523.0746, -536.0620, -522.0332, -521.8961],
        [-520.8023, -521.6262, -522.2030, -538.3857, -521.3917, -521.1620],
        [-520.3647, -521.1876, -521.3884, -537.2833, -520.8306, -520.6619]],
       device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([47, 6, 10]), scale: torch.Size([47, 6, 10]))
=============================================================================================
