Dataset: HAR
--------------------------------------------
Model Name: MMVAE_random_seed_20_Latent_dim_5_Hard_Optimization_Beta_0.1_HAR
==================================================================
MMVAE_random_seed_20_Latent_dim_5_Hard_Optimization_Beta_0.1_HAR Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 20
--------------------------------------------
Device = cuda:2
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 30
--------------------------------------------
Input Dimension = 561
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 561
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
--------------------------------------------
beta_1 = 0.1
==================================================================

GMM Part:
==================================================================
Number of Components= 6
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-530.3950, -519.0248, -522.1801, -524.1055, -519.7234, -520.3826],
        [-522.2125, -518.4656, -523.2718, -519.9349, -518.3613, -520.7349],
        [-532.2478, -519.7778, -522.0856, -523.3748, -519.6442, -520.0382],
        [-532.9425, -520.7299, -524.4376, -524.7628, -520.9863, -521.6577],
        [-535.3040, -518.2169, -522.8301, -522.8454, -518.4384, -520.2017],
        [-535.3566, -517.5553, -521.0668, -521.2219, -519.0063, -520.1096],
        [-520.8226, -525.1482, -519.3503, -520.5411, -527.4807, -520.2057],
        [-534.7897, -517.5378, -521.7797, -522.0192, -518.6575, -518.9904],
        [-525.2522, -519.7286, -520.4203, -522.5076, -520.3065, -522.6338],
        [-520.9684, -523.3065, -519.2542, -520.8020, -526.4445, -518.9351],
        [-521.5878, -535.4727, -519.7532, -521.3274, -525.0714, -519.7621],
        [-523.6821, -529.2471, -522.8199, -523.6596, -526.1579, -523.2311],
        [-527.8154, -518.5363, -524.7257, -526.0834, -521.3526, -519.5868],
        [-520.9463, -525.8154, -521.0338, -520.9011, -522.1572, -520.7944],
        [-520.9005, -527.5632, -519.2014, -520.6708, -527.8301, -519.1307],
        [-532.1486, -518.4835, -519.8239, -524.4180, -519.4360, -520.6466],
        [-536.8940, -518.8553, -522.3677, -523.2443, -520.6062, -520.9711],
        [-532.5847, -517.5840, -521.2763, -521.4680, -517.3570, -517.8652],
        [-526.6323, -520.1395, -523.9171, -525.5854, -520.3633, -524.2243],
        [-519.3051, -521.2878, -518.1301, -519.2612, -521.2314, -518.8284],
        [-531.5278, -518.4475, -525.9261, -528.9948, -519.8496, -521.9517],
        [-537.6278, -519.1101, -522.8087, -523.6683, -519.3401, -520.5569],
        [-533.8586, -517.5903, -522.7581, -522.9708, -517.7838, -518.1915],
        [-521.7868, -523.9785, -519.9464, -521.4152, -525.9724, -521.2849],
        [-529.2257, -518.0027, -522.5290, -524.7448, -518.2935, -519.8616],
        [-521.4392, -525.6647, -520.1730, -521.5029, -525.1387, -520.5090],
        [-531.8951, -519.3956, -523.9120, -531.8492, -525.3538, -523.0429],
        [-520.2501, -518.4718, -521.1108, -520.0045, -518.3433, -518.4824],
        [-519.7620, -528.7555, -519.0266, -519.5704, -530.5778, -519.1401],
        [-520.0471, -533.3793, -519.9293, -520.0375, -531.4871, -520.2195],
        [-523.4515, -518.8381, -522.5831, -519.9237, -518.5524, -520.3596],
        [-524.1245, -524.6021, -518.9747, -523.7500, -520.8087, -518.9866],
        [-521.8827, -530.4706, -518.9385, -521.9680, -532.7939, -519.3565],
        [-520.3455, -520.0764, -521.0472, -520.2654, -520.3895, -520.0797],
        [-530.7747, -519.0302, -521.8884, -524.5258, -521.1840, -519.3649],
        [-526.8704, -518.2900, -523.8681, -521.0338, -518.3337, -518.8630],
        [-525.3482, -522.0679, -524.7413, -517.4100, -521.1487, -526.8121],
        [-521.7258, -528.5781, -518.6641, -521.5167, -529.4271, -520.1717],
        [-522.6829, -520.4931, -521.4952, -520.7720, -520.5657, -520.8950],
        [-532.3659, -518.5839, -522.4252, -523.6185, -519.9622, -519.4890],
        [-520.4293, -526.0164, -520.1191, -520.3528, -524.0845, -520.6749],
        [-531.8838, -521.0466, -523.2779, -524.5106, -520.9501, -521.3274],
        [-519.8159, -521.1570, -518.8496, -519.6127, -521.0515, -518.7350],
        [-520.7356, -528.4617, -519.5433, -520.7793, -528.1005, -519.8400],
        [-529.5721, -519.5688, -522.4418, -526.9338, -521.2805, -519.6529],
        [-521.1225, -518.9544, -522.7105, -520.2871, -518.9058, -519.6770],
        [-523.6543, -520.6890, -526.1808, -521.6498, -520.4961, -521.4578],
        [-538.2402, -517.8583, -522.9022, -523.3552, -519.3721, -519.6860],
        [-536.4531, -518.8099, -521.3168, -524.8102, -521.8027, -522.2473],
        [-530.6132, -520.3931, -525.1613, -527.0583, -523.3062, -521.9528],
        [-518.9482, -522.4323, -518.9811, -518.6536, -521.6418, -519.3457],
        [-521.5185, -523.0294, -519.4419, -521.3942, -524.4012, -519.4679]],
       device='cuda:2', grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([52, 6, 5]), scale: torch.Size([52, 6, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-526.6466, -529.9358, -524.5641, -526.3917, -528.7433, -524.9149],
        [-529.2552, -530.9424, -527.8901, -529.2377, -530.4454, -528.7262],
        [-526.3128, -530.5115, -524.9534, -526.8105, -530.0707, -525.8946],
        [-525.7526, -527.0431, -523.9344, -525.5371, -525.1797, -524.2197],
        [-521.8022, -526.6880, -521.3375, -521.6894, -525.5299, -521.3654],
        [-523.2645, -525.5697, -522.6207, -523.0354, -525.4357, -522.9110],
        [-523.4312, -525.9973, -523.2545, -523.3965, -524.9628, -523.3291],
        [-526.4856, -527.5315, -525.1517, -525.8843, -525.7601, -525.2892],
        [-522.4789, -523.7887, -521.1195, -522.0714, -523.9265, -521.2421],
        [-520.4726, -522.3567, -520.1464, -520.3914, -522.8119, -520.2932],
        [-521.2507, -523.3735, -520.1974, -521.1566, -524.6178, -520.9946],
        [-520.6965, -522.3586, -520.8058, -520.7048, -523.4112, -521.8262],
        [-519.9796, -521.0031, -519.3003, -519.6580, -522.5303, -519.4300],
        [-521.3534, -523.5419, -520.4005, -521.2134, -525.6086, -520.3928],
        [-521.3401, -522.1194, -520.1493, -520.9290, -522.8112, -520.6112],
        [-521.1377, -522.3447, -520.3134, -520.8418, -523.8054, -520.3702],
        [-520.9948, -523.7379, -520.4576, -521.0763, -526.8895, -520.9929],
        [-521.6675, -524.1411, -520.7180, -521.5892, -525.7302, -521.0328],
        [-521.7850, -522.7274, -520.6664, -521.4391, -523.8674, -520.6765],
        [-522.1296, -522.4836, -521.2120, -521.6526, -522.3801, -521.1428],
        [-522.1703, -523.6589, -520.4395, -521.7245, -523.7894, -520.6696],
        [-524.2131, -524.7970, -521.8419, -523.2805, -525.6478, -521.7202],
        [-522.6223, -524.8699, -521.8401, -522.3480, -524.9435, -521.8109],
        [-522.6092, -525.4450, -519.9216, -522.4154, -526.3376, -520.4214],
        [-522.0593, -522.8146, -520.7055, -521.6182, -524.3726, -520.8839],
        [-523.3434, -524.3163, -521.7580, -522.9298, -525.6102, -522.2623],
        [-522.1125, -524.8763, -520.0324, -521.8539, -526.2468, -520.3653],
        [-522.3510, -524.0953, -520.3303, -522.0977, -525.6807, -520.5935],
        [-521.5022, -528.6259, -521.3819, -521.6542, -528.3356, -521.4954],
        [-524.9366, -528.8484, -523.0966, -524.6771, -528.5907, -523.4943],
        [-524.2123, -527.2023, -521.8038, -524.0853, -525.7689, -522.6249],
        [-524.9626, -527.5292, -524.3565, -525.0977, -528.4872, -525.4525],
        [-524.0022, -526.2756, -524.4794, -523.8371, -526.2368, -525.1111],
        [-523.7188, -525.6083, -522.7877, -523.3309, -524.8727, -522.8262],
        [-521.6815, -525.3842, -520.3621, -521.6806, -526.9392, -521.0078],
        [-523.6251, -525.3676, -522.0909, -523.5871, -523.3384, -522.1651],
        [-525.0027, -526.3460, -522.2311, -524.8272, -523.8564, -522.5445],
        [-525.1100, -526.2422, -522.4949, -524.4802, -525.7499, -522.3467],
        [-522.7938, -523.8760, -521.7056, -522.3608, -523.4999, -521.5957],
        [-521.3370, -523.5504, -520.3231, -520.8774, -523.3107, -520.5515],
        [-519.7466, -521.2249, -519.1777, -519.2830, -521.7550, -519.5378],
        [-520.0126, -521.5132, -519.2570, -519.8094, -522.0806, -519.7435],
        [-522.8462, -523.6992, -521.9428, -522.7482, -525.7874, -521.8924],
        [-522.1918, -523.0140, -520.6121, -521.9741, -523.9391, -520.7891],
        [-522.7608, -524.0971, -521.0190, -522.6372, -525.4766, -521.4092],
        [-522.8094, -524.6601, -520.6589, -522.6680, -526.7396, -520.8620],
        [-521.6785, -523.0386, -520.1671, -521.6517, -525.0431, -520.2373]],
       device='cuda:2')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([47, 6, 5]), scale: torch.Size([47, 6, 5]))
=============================================================================================
