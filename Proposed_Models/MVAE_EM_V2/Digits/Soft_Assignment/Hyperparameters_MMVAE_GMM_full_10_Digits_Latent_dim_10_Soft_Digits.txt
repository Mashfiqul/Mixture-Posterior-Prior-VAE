Dataset: Digits
--------------------------------------------
Model Name: MMVAE_GMM_full_10_Digits_Latent_dim_10_Soft_Optimization
==================================================================
MMVAE_GMM_full_10_Digits_Latent_dim_10_Soft_Optimization Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 10
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 1000
--------------------------------------------
Learning Rate = 1e-06
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 64
--------------------------------------------
Encoder Hidden Dimension 1 = 256
--------------------------------------------
Encoder Hidden Dimension 2 = 256
--------------------------------------------
Latent Dimension = 10
--------------------------------------------
Decoder Hidden Layer 1 = 256
--------------------------------------------
Decoder Hidden Layer 2 = 256
--------------------------------------------
Output Dimension = 64
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 50
--------------------------------------------
Batch Size (Test) = 50
--------------------------------------------
gamma = 0.5
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 1001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-42.5418, -42.9836, -42.5359, -42.5207, -43.2944, -42.7794, -42.5706,
         -42.7182, -43.2723, -43.1450],
        [-43.0885, -42.7659, -43.3382, -43.2590, -43.5239, -42.4805, -42.9532,
         -43.2884, -43.3560, -43.1717],
        [-43.0002, -42.6577, -42.1922, -42.5005, -42.4937, -43.4445, -42.9204,
         -42.7006, -42.6721, -42.8721],
        [-42.3271, -43.0055, -43.1704, -42.7722, -42.8464, -42.8700, -43.0577,
         -42.8166, -43.0040, -43.1332],
        [-43.3204, -42.9620, -43.1490, -43.1123, -43.3617, -43.2148, -42.2986,
         -42.9122, -42.0513, -42.8999],
        [-43.1647, -43.1381, -43.8366, -43.5415, -43.7386, -43.6622, -43.4869,
         -43.6096, -43.8322, -43.6915],
        [-43.4042, -43.7780, -43.7373, -43.3520, -43.2170, -43.2592, -43.0749,
         -42.9214, -43.4813, -43.1515],
        [-43.0935, -42.8147, -42.7218, -43.1070, -42.9364, -42.8853, -43.2846,
         -43.4190, -42.9716, -43.0477],
        [-43.5691, -43.6022, -43.7621, -43.7471, -43.6171, -43.5963, -43.2762,
         -43.5270, -43.9376, -43.4397],
        [-43.2742, -42.9739, -42.8549, -42.8194, -42.9029, -42.7159, -43.0788,
         -42.8263, -43.0383, -42.7690],
        [-43.3702, -42.9282, -43.2607, -43.2369, -43.1244, -43.6536, -43.3023,
         -42.5165, -43.2734, -43.2798],
        [-43.4064, -42.8441, -43.1223, -42.7108, -42.5818, -42.5983, -43.0397,
         -42.8989, -42.7303, -43.2012],
        [-42.4098, -43.0916, -42.9539, -42.9586, -42.2938, -43.4293, -42.4881,
         -43.2961, -42.9476, -42.9367],
        [-42.9996, -42.9931, -43.6800, -43.5841, -43.4698, -42.9895, -43.2195,
         -43.4664, -43.6707, -42.6350],
        [-43.3551, -42.9202, -43.6652, -43.4192, -43.4318, -43.5371, -43.5156,
         -43.0477, -43.3366, -43.6040],
        [-43.0388, -43.2609, -43.1169, -42.2513, -43.1667, -43.0613, -42.9730,
         -42.9759, -42.6620, -43.2403],
        [-43.2762, -43.3998, -43.3553, -43.0056, -43.2560, -43.6462, -42.8388,
         -43.5140, -43.2045, -42.6187],
        [-42.5439, -42.7512, -42.4135, -42.7035, -42.7073, -43.0261, -42.3152,
         -42.5974, -42.7019, -42.6749],
        [-42.9075, -43.1853, -43.2657, -43.2898, -43.2147, -42.4943, -43.1062,
         -43.3609, -43.1205, -43.2848],
        [-42.3193, -42.9373, -42.6304, -43.3243, -43.0262, -42.3112, -43.3328,
         -42.7628, -42.6957, -41.4286],
        [-44.0082, -43.7869, -43.8674, -43.4276, -43.9889, -43.9767, -44.1706,
         -43.7952, -43.8482, -43.8444],
        [-42.9003, -43.3264, -42.7166, -43.1984, -42.0627, -42.9150, -43.2589,
         -43.3198, -43.4621, -43.4307],
        [-42.9338, -42.7052, -42.5847, -43.0378, -42.9641, -42.3462, -42.3224,
         -42.7010, -42.6027, -42.8904],
        [-42.7186, -42.5161, -43.1627, -43.2360, -43.0207, -43.0783, -42.5742,
         -42.7955, -42.3448, -42.8266],
        [-42.8102, -42.4791, -42.5822, -42.5325, -42.1817, -43.2216, -42.6304,
         -42.9233, -42.8140, -43.3002],
        [-43.0153, -42.0086, -42.9646, -42.9123, -42.5016, -42.6278, -42.8483,
         -43.1522, -42.9510, -42.9714],
        [-43.3714, -42.9531, -43.5290, -43.6219, -43.4969, -43.3210, -43.3595,
         -43.0598, -42.9731, -42.3045]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([27, 10, 10]), scale: torch.Size([27, 10, 10]))
=============================================================================================
Decoder Loss (Test) = tensor([[-43.9429, -43.9462, -43.9227, -43.9467, -43.9645, -43.9554, -43.9768,
         -43.9422, -43.9305, -43.9380],
        [-43.9033, -43.9167, -43.9054, -43.9154, -43.9303, -43.9286, -43.9273,
         -43.9011, -43.8858, -43.8647],
        [-43.9491, -43.9317, -43.9318, -43.9502, -43.9508, -43.9341, -43.9575,
         -43.9427, -43.9277, -43.9527],
        [-43.8948, -43.8820, -43.8571, -43.8830, -43.8938, -43.9041, -43.8832,
         -43.9048, -43.8786, -43.8805],
        [-43.7745, -43.7824, -43.7263, -43.7642, -43.7585, -43.7667, -43.7773,
         -43.7690, -43.7473, -43.7489],
        [-43.9715, -43.9304, -43.9220, -43.9386, -43.9593, -43.9737, -43.9606,
         -43.9771, -43.9377, -43.9596],
        [-44.0338, -44.0085, -44.0046, -44.0233, -44.0417, -44.0323, -44.0185,
         -44.0283, -44.0142, -44.0370],
        [-43.8363, -43.8478, -43.8190, -43.8546, -43.8417, -43.8252, -43.8717,
         -43.8606, -43.8573, -43.8384],
        [-43.9136, -43.8820, -43.8728, -43.9008, -43.9095, -43.8970, -43.9107,
         -43.8927, -43.8868, -43.8925],
        [-43.8724, -43.8625, -43.8719, -43.8892, -43.8831, -43.8553, -43.8822,
         -43.8927, -43.8817, -43.8639],
        [-43.7804, -43.7720, -43.7631, -43.8011, -43.8061, -43.7789, -43.8085,
         -43.7852, -43.7847, -43.7812],
        [-43.9010, -43.9052, -43.8828, -43.9111, -43.9198, -43.9172, -43.9342,
         -43.8966, -43.8756, -43.8800],
        [-43.9848, -43.9468, -43.9168, -43.9535, -43.9409, -43.9712, -43.9517,
         -43.9310, -43.9186, -43.9629],
        [-44.0380, -43.9828, -43.9823, -44.0060, -44.0058, -44.0213, -44.0010,
         -43.9987, -43.9818, -44.0251],
        [-43.9070, -43.9020, -43.8725, -43.8993, -43.9180, -43.9156, -43.9270,
         -43.9080, -43.8744, -43.8817],
        [-43.8192, -43.7970, -43.8155, -43.8309, -43.8186, -43.8068, -43.8214,
         -43.8177, -43.8417, -43.8234],
        [-43.6800, -43.7005, -43.6678, -43.6869, -43.6643, -43.6673, -43.7015,
         -43.6867, -43.6756, -43.6519],
        [-43.9094, -43.8733, -43.8479, -43.8870, -43.8862, -43.9091, -43.9017,
         -43.8903, -43.8758, -43.8968],
        [-43.8356, -43.8326, -43.8230, -43.8422, -43.8209, -43.8167, -43.8476,
         -43.8626, -43.8527, -43.8484],
        [-43.9055, -43.9259, -43.8751, -43.9041, -43.8933, -43.9019, -43.9031,
         -43.9109, -43.8902, -43.8802]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([20, 10, 10]), scale: torch.Size([20, 10, 10]))
=============================================================================================
