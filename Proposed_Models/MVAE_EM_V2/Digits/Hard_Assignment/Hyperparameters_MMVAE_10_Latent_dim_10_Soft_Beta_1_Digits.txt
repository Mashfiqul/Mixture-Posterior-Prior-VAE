Dataset: Digits
--------------------------------------------
Model Name: MMVAE_random_seed_10_Latent_dim_10_Soft_Optimization_Beta_1_Digits
==================================================================
MMVAE_random_seed_10_Latent_dim_10_Soft_Optimization_Beta_1_Digits Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 10
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 1000
--------------------------------------------
Learning Rate = 1e-06
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 64
--------------------------------------------
Encoder Hidden Dimension 1 = 256
--------------------------------------------
Encoder Hidden Dimension 2 = 256
--------------------------------------------
Latent Dimension = 10
--------------------------------------------
Decoder Hidden Layer 1 = 256
--------------------------------------------
Decoder Hidden Layer 2 = 256
--------------------------------------------
Output Dimension = 64
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 50
--------------------------------------------
Batch Size (Test) = 50
--------------------------------------------
gamma = 0.5
--------------------------------------------
beta_1 = 1
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 1001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-42.5697, -43.0013, -42.5538, -42.5382, -43.3138, -42.7972, -42.5993,
         -42.7361, -43.2909, -43.1706],
        [-43.1067, -42.7915, -43.3549, -43.2811, -43.5358, -42.5099, -42.9667,
         -43.3094, -43.3734, -43.1974],
        [-43.0241, -42.6912, -42.2411, -42.5447, -42.5216, -43.4727, -42.9436,
         -42.7185, -42.7052, -42.8955],
        [-42.3543, -43.0292, -43.1888, -42.7990, -42.8684, -42.8926, -43.0777,
         -42.8416, -43.0267, -43.1568],
        [-43.3439, -42.9776, -43.1783, -43.1270, -43.3779, -43.2428, -42.3273,
         -42.9381, -42.0770, -42.9221],
        [-43.1842, -43.1513, -43.8535, -43.5593, -43.7541, -43.6767, -43.5043,
         -43.6313, -43.8482, -43.7060],
        [-43.4360, -43.7927, -43.7468, -43.3757, -43.2385, -43.2786, -43.0982,
         -42.9442, -43.5022, -43.1729],
        [-43.1263, -42.8470, -42.7576, -43.1340, -42.9674, -42.9078, -43.3005,
         -43.4385, -42.9982, -43.0776],
        [-43.5836, -43.6120, -43.7753, -43.7634, -43.6275, -43.6241, -43.2923,
         -43.5429, -43.9534, -43.4618],
        [-43.2991, -42.9970, -42.8864, -42.8462, -42.9327, -42.7453, -43.1070,
         -42.8512, -43.0653, -42.8063],
        [-43.4025, -42.9549, -43.2745, -43.2529, -43.1462, -43.6695, -43.3252,
         -42.5525, -43.2960, -43.2988],
        [-43.4265, -42.8685, -43.1384, -42.7444, -42.6146, -42.6280, -43.0743,
         -42.9189, -42.7610, -43.2224],
        [-42.4400, -43.1163, -42.9809, -42.9904, -42.3378, -43.4453, -42.5180,
         -43.3136, -42.9771, -42.9702],
        [-43.0189, -43.0139, -43.6942, -43.6002, -43.4801, -43.0099, -43.2315,
         -43.4909, -43.6865, -42.6623],
        [-43.3692, -42.9375, -43.6733, -43.4348, -43.4500, -43.5577, -43.5297,
         -43.0765, -43.3545, -43.6162],
        [-43.0655, -43.2827, -43.1495, -42.2884, -43.1872, -43.0835, -43.0024,
         -43.0021, -42.6937, -43.2652],
        [-43.3026, -43.4180, -43.3812, -43.0387, -43.2783, -43.6633, -42.8641,
         -43.5355, -43.2341, -42.6597],
        [-42.5720, -42.7733, -42.4381, -42.7290, -42.7255, -43.0524, -42.3418,
         -42.6303, -42.7329, -42.7024],
        [-42.9277, -43.2057, -43.2853, -43.3015, -43.2305, -42.5142, -43.1207,
         -43.3751, -43.1486, -43.2976],
        [-42.3538, -42.9619, -42.6559, -43.3475, -43.0427, -42.3462, -43.3432,
         -42.7840, -42.7247, -41.4838],
        [-44.0314, -43.7977, -43.8800, -43.4444, -44.0012, -43.9905, -44.1825,
         -43.8074, -43.8623, -43.8622],
        [-42.9205, -43.3432, -42.7373, -43.2169, -42.0838, -42.9450, -43.2755,
         -43.3358, -43.4746, -43.4408],
        [-42.9577, -42.7306, -42.6078, -43.0547, -42.9894, -42.3742, -42.3413,
         -42.7250, -42.6398, -42.9169],
        [-42.7424, -42.5420, -43.1812, -43.2509, -43.0355, -43.0978, -42.5966,
         -42.8220, -42.3796, -42.8537],
        [-42.8374, -42.5154, -42.6180, -42.5641, -42.2275, -43.2420, -42.6662,
         -42.9441, -42.8389, -43.3233],
        [-43.0405, -42.0520, -42.9885, -42.9353, -42.5385, -42.6582, -42.8713,
         -43.1760, -42.9772, -42.9965],
        [-43.3943, -42.9882, -43.5394, -43.6431, -43.5191, -43.3459, -43.3888,
         -43.0913, -43.0003, -42.3450]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([27, 10, 10]), scale: torch.Size([27, 10, 10]))
=============================================================================================
Decoder Loss (Test) = tensor([[-43.9485, -43.9496, -43.9276, -43.9522, -43.9682, -43.9606, -43.9823,
         -43.9465, -43.9341, -43.9443],
        [-43.9076, -43.9192, -43.9075, -43.9203, -43.9337, -43.9334, -43.9310,
         -43.9055, -43.8899, -43.8705],
        [-43.9558, -43.9357, -43.9378, -43.9563, -43.9566, -43.9397, -43.9630,
         -43.9475, -43.9320, -43.9591],
        [-43.9001, -43.8869, -43.8646, -43.8884, -43.8985, -43.9119, -43.8875,
         -43.9116, -43.8846, -43.8875],
        [-43.7814, -43.7886, -43.7346, -43.7714, -43.7653, -43.7751, -43.7835,
         -43.7774, -43.7545, -43.7581],
        [-43.9751, -43.9339, -43.9292, -43.9431, -43.9630, -43.9822, -43.9635,
         -43.9835, -43.9421, -43.9653],
        [-44.0394, -44.0127, -44.0124, -44.0289, -44.0470, -44.0407, -44.0242,
         -44.0346, -44.0197, -44.0438],
        [-43.8441, -43.8537, -43.8278, -43.8631, -43.8504, -43.8335, -43.8796,
         -43.8692, -43.8643, -43.8469],
        [-43.9215, -43.8867, -43.8795, -43.9081, -43.9156, -43.9045, -43.9169,
         -43.8985, -43.8926, -43.9013],
        [-43.8789, -43.8656, -43.8779, -43.8963, -43.8903, -43.8627, -43.8898,
         -43.8975, -43.8863, -43.8713],
        [-43.7900, -43.7779, -43.7722, -43.8101, -43.8147, -43.7883, -43.8170,
         -43.7943, -43.7928, -43.7923],
        [-43.9050, -43.9087, -43.8858, -43.9152, -43.9232, -43.9234, -43.9374,
         -43.9011, -43.8794, -43.8857],
        [-43.9912, -43.9529, -43.9260, -43.9592, -43.9466, -43.9808, -43.9561,
         -43.9409, -43.9253, -43.9715],
        [-44.0439, -43.9881, -43.9908, -44.0121, -44.0124, -44.0305, -44.0065,
         -44.0070, -43.9886, -44.0330],
        [-43.9116, -43.9070, -43.8763, -43.9039, -43.9223, -43.9242, -43.9295,
         -43.9149, -43.8794, -43.8887],
        [-43.8282, -43.8019, -43.8239, -43.8404, -43.8272, -43.8143, -43.8307,
         -43.8250, -43.8476, -43.8325],
        [-43.6865, -43.7077, -43.6751, -43.6947, -43.6731, -43.6759, -43.7066,
         -43.6949, -43.6810, -43.6598],
        [-43.9175, -43.8795, -43.8569, -43.8939, -43.8925, -43.9179, -43.9061,
         -43.9004, -43.8837, -43.9051],
        [-43.8427, -43.8393, -43.8324, -43.8512, -43.8306, -43.8271, -43.8555,
         -43.8715, -43.8597, -43.8568],
        [-43.9110, -43.9322, -43.8817, -43.9102, -43.8994, -43.9094, -43.9089,
         -43.9176, -43.8957, -43.8874]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([20, 10, 10]), scale: torch.Size([20, 10, 10]))
=============================================================================================
