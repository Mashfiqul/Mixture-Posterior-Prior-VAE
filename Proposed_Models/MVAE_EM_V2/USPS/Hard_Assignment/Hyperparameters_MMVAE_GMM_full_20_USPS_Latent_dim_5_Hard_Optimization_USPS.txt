Dataset: USPS
--------------------------------------------
Model Name: MMVAE_GMM_full_20_USPS_Latent_dim_5_Hard_Optimization
==================================================================
MMVAE_GMM_full_20_USPS_Latent_dim_5_Hard_Optimization Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 20
--------------------------------------------
Device = cuda:2
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 30
--------------------------------------------
Input Dimension = 256
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 256
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-109.4847, -108.9409, -103.7287, -116.0015, -102.9501, -110.4968,
         -108.4706, -108.7267, -112.7811, -108.4181],
        [ -45.0746,  -42.4387,  -45.4689,  -40.2921,  -39.9073,  -44.8724,
          -42.8640,  -43.1426,  -44.2619,  -40.2554],
        [-113.8074, -106.2797, -118.7709, -102.1692, -112.4885, -117.0797,
         -137.4679, -102.5470, -108.2848, -101.7828],
        [ -93.0549,  -75.1395,  -98.9212,  -84.4310,  -94.6125,  -78.8309,
          -79.9775,  -83.0693,  -78.5695,  -76.2705],
        [ -89.6605,  -75.1655,  -82.5628,  -77.2415,  -72.0795,  -77.2021,
          -78.8124,  -73.6955,  -74.2489,  -73.7460],
        [-112.1943, -107.9852, -110.6801, -108.8432, -121.4820, -111.8110,
         -115.1759, -110.9274, -105.9612, -106.2175],
        [-112.3132, -114.6341, -110.1266, -111.1335, -107.1308, -117.9364,
         -119.8860, -110.8214, -116.0097, -112.6759],
        [ -65.3653,  -66.7061,  -69.4938,  -65.2177,  -66.4031,  -66.8138,
          -65.3842,  -66.9921,  -66.9665,  -65.1485],
        [ -84.0922,  -82.5159,  -82.6245,  -83.6586,  -85.2106,  -85.8145,
          -92.0989,  -86.8212,  -84.7549,  -82.4865],
        [ -84.0757,  -83.3956,  -81.3594,  -81.3565,  -85.1578,  -82.5257,
          -85.4103,  -82.4921,  -86.8049,  -87.4319],
        [ -99.2035, -101.6159,  -99.9440, -100.6043, -101.2193, -102.2120,
          -99.7363, -101.6984, -101.3997, -100.9807],
        [-117.5591, -102.8550, -114.2545, -102.8243, -113.8075, -116.5526,
         -131.6000, -103.0378, -104.3100, -102.1715],
        [-117.0268, -115.1859, -110.3800, -113.7619, -112.2222, -117.2756,
         -119.1313, -116.1753, -114.8298, -114.6434],
        [ -85.4740,  -86.8358,  -89.8215,  -88.0590, -100.6954,  -91.3762,
          -86.9942,  -85.5872,  -86.3530,  -85.9386],
        [-136.5016, -142.8305, -137.6573, -133.7246, -204.8401, -142.6649,
         -157.8399, -135.8580, -138.3263, -139.9867],
        [-103.8206,  -90.1059, -107.1801,  -89.4221,  -99.7276,  -96.9430,
         -111.8644,  -90.9146,  -91.7228,  -89.0711],
        [ -93.9805,  -75.6906,  -86.7862,  -69.4770,  -77.1966,  -68.8616,
          -70.3542,  -78.4001, -112.3287,  -68.4520],
        [ -75.7262,  -75.3618,  -79.6900,  -75.7467,  -81.1906,  -80.2673,
          -76.2827,  -76.0639,  -76.5051,  -75.4541],
        [ -83.0568,  -80.6771,  -82.8022,  -80.5471,  -89.2656,  -81.9023,
          -86.2148,  -80.6569,  -80.5178,  -80.1209],
        [ -94.7680,  -89.7180,  -94.5533,  -94.2948, -104.5013,  -95.0397,
          -93.2421,  -89.7260,  -90.5515,  -89.5491],
        [-113.3974, -117.9010, -115.2597, -107.6096, -115.3548, -146.2578,
         -111.1617, -107.4541, -107.2857, -107.4295],
        [ -94.1328,  -98.6149,  -95.0634,  -96.7569,  -95.5319, -107.0697,
          -96.0997,  -92.6913,  -95.0201,  -93.8763],
        [-116.1662, -112.4964, -112.4922, -109.8612, -114.5021, -124.1296,
         -124.2792, -109.9291, -111.5876, -110.0752],
        [-106.5127, -103.6665, -101.3752, -106.6138,  -96.4806, -107.6599,
         -107.1854, -102.7290, -103.1694, -103.0497],
        [ -90.6815,  -78.4627,  -90.5972,  -82.3863,  -96.5396,  -90.9452,
          -93.4147,  -78.3741,  -79.3657,  -77.7397],
        [ -67.9180,  -62.0848,  -70.5687,  -60.8084,  -65.7455,  -62.3672,
          -63.2499,  -61.6141,  -69.3510,  -61.6427],
        [-111.1146, -103.2002, -113.4515, -102.6390, -111.0067, -111.7059,
         -114.1458, -101.0748, -105.3068, -102.0709],
        [-120.4124, -101.9785, -107.9674, -101.9755,  -94.1662, -112.4933,
         -117.7222, -101.1650, -104.8410, -100.8820],
        [-115.1450, -103.8950, -125.4589, -101.5426, -107.7179, -131.1229,
         -136.1287, -101.7031,  -98.8855,  -99.4770],
        [ -86.5461,  -84.8765,  -89.0783,  -85.3889,  -91.6839,  -85.9499,
          -84.9168,  -85.4328,  -84.5513,  -86.0133],
        [ -64.4796,  -67.4092,  -63.1540,  -63.0315,  -63.3116,  -63.9584,
          -64.0106,  -63.0752,  -65.3698,  -63.3930],
        [ -47.3763,  -49.4214,  -47.5385,  -49.3650,  -46.7599,  -54.3724,
          -47.4786,  -48.9945,  -48.6993,  -49.0459],
        [ -85.8564,  -87.2416,  -84.9682,  -87.3806,  -86.8190,  -87.6988,
          -85.8902,  -87.3019,  -87.7325,  -87.2330],
        [ -79.0496,  -76.5573,  -83.9605,  -76.0943,  -78.0107,  -74.1392,
          -76.7948,  -74.9521,  -81.0836,  -75.4016],
        [ -50.8581,  -50.2745,  -52.3310,  -50.4699,  -48.6201,  -55.6086,
          -50.2706,  -49.6955,  -50.9985,  -49.5286],
        [-105.8517,  -84.7565,  -95.7215,  -93.2304,  -82.1052,  -94.4815,
          -94.4143,  -84.6896,  -96.6453,  -86.1051],
        [ -72.5320,  -68.6717,  -77.0868,  -64.1713,  -66.3838,  -65.3075,
          -67.8505,  -66.3073,  -90.7141,  -64.2444],
        [ -83.0431,  -83.3231,  -81.7837,  -87.1828,  -81.5620,  -86.0323,
          -82.6167,  -82.2794,  -82.5295,  -83.1849],
        [-113.1921,  -97.6860, -114.1569, -102.7182, -115.8480, -106.2922,
         -148.5138,  -98.4116,  -98.9176, -100.7168],
        [ -94.4421,  -93.1678,  -99.2607,  -94.1728, -106.4775,  -93.5111,
          -93.1955,  -93.1954,  -93.4189,  -92.9699],
        [ -79.7697,  -78.9910,  -78.7630,  -80.5163,  -80.3593,  -86.0441,
          -78.2540,  -78.0722,  -83.5213,  -77.9295],
        [-100.6224,  -91.9457, -104.9067,  -93.9588, -102.6624,  -99.1161,
         -102.4440,  -88.8351,  -90.3341,  -89.2353],
        [ -99.0210,  -78.3904, -137.7390,  -92.6398,  -92.8344,  -74.1213,
          -73.9699,  -86.4533,  -89.3132,  -80.6394],
        [-114.7859,  -86.8573, -131.5330,  -94.2694, -119.6478,  -93.7030,
          -96.7760,  -95.7815,  -96.6371,  -87.9114],
        [ -95.8105,  -79.3193,  -80.1576,  -82.1218,  -76.2843,  -87.4109,
          -80.8507,  -79.8683,  -82.1985,  -80.4416],
        [ -71.7958,  -73.5722,  -75.8350,  -74.8848,  -86.6136,  -74.1708,
          -73.3778,  -82.0431,  -91.4732,  -73.1588],
        [ -94.7478, -100.6550,  -95.0324,  -97.5785, -100.1156,  -95.2301,
         -105.6509, -100.4930, -107.5474,  -99.2259],
        [-106.5562, -109.7948, -105.9855, -102.6719, -106.7965, -117.8373,
         -104.3935, -104.8571, -103.1202, -102.3462],
        [ -42.9120,  -42.5464,  -46.1956,  -40.9626,  -40.1092,  -44.5821,
          -40.5020,  -41.6193,  -42.9702,  -41.9379],
        [ -82.2813,  -77.3797,  -81.5357,  -77.2846,  -83.0865,  -77.7664,
          -76.9334,  -78.4902,  -83.7381,  -77.1801],
        [ -97.1518, -101.2181,  -95.4817,  -98.0132, -104.7499,  -97.5178,
          -98.3140,  -98.5552, -109.9073,  -99.4936],
        [ -73.1373,  -73.9142,  -76.8759,  -73.2633,  -78.7732,  -73.6392,
          -73.2549,  -73.7373,  -74.1520,  -74.2382],
        [ -48.7633,  -48.5461,  -49.4978,  -49.6591,  -47.1487,  -56.2441,
          -50.3303,  -48.2484,  -50.4609,  -49.6827],
        [-103.2317,  -93.3045,  -95.5011,  -95.7847, -111.5931,  -93.8119,
          -93.6751,  -99.1117, -104.0424,  -94.4709],
        [ -95.5844,  -94.4556,  -97.3089,  -97.5008,  -98.1311,  -95.9792,
          -97.2134,  -95.9970,  -95.4368,  -95.1918],
        [ -76.5402,  -74.3010,  -79.0875,  -74.7845,  -77.3953,  -79.3428,
          -78.4390,  -74.6865,  -76.1534,  -74.7381],
        [ -90.0171,  -78.8306,  -83.8651,  -80.4692,  -79.3432,  -82.5444,
          -81.0175,  -78.5522,  -80.0046,  -77.8148],
        [ -77.3551,  -70.3590,  -83.6406,  -69.1199,  -76.1168,  -67.9161,
          -68.2737,  -70.2376,  -89.1401,  -68.2656],
        [ -73.9752,  -61.9982,  -81.4365,  -66.6789,  -59.7670,  -73.8921,
          -76.2582,  -63.9660,  -64.1733,  -63.4417],
        [-115.4373, -117.0189, -120.5529, -104.8941, -117.1185, -135.7266,
         -137.3189, -100.0581, -102.0987, -104.2625],
        [ -96.4734, -102.3275,  -97.5374, -104.8993, -100.3320,  -98.3293,
         -123.1572, -105.6228, -110.6199, -103.8913],
        [ -74.7596,  -76.7639,  -80.7498,  -75.1027,  -77.1437,  -74.4043,
          -75.0566,  -74.9694,  -77.6022,  -74.9433],
        [ -44.5489,  -43.9382,  -52.6356,  -45.5325,  -43.4551,  -46.2448,
          -44.0375,  -45.2103,  -45.4391,  -45.3523],
        [ -89.2894,  -86.8794, -106.4128,  -86.2110,  -91.9605,  -86.1940,
          -87.4696,  -92.2903,  -87.4440,  -87.5159],
        [ -94.9016,  -92.7834,  -91.1006,  -93.8006,  -94.5555,  -93.5495,
          -91.7779,  -94.6883,  -93.8925,  -93.1748],
        [ -91.4965,  -90.5514,  -89.8566,  -91.3461,  -97.4937,  -92.6903,
          -90.2028,  -90.6521,  -91.4683,  -90.2893],
        [-110.0860, -108.3912, -115.0642, -103.0876, -113.1918, -134.7525,
         -113.4080, -105.8959, -103.1771, -104.6951],
        [ -69.3166,  -67.5461,  -72.1847,  -67.5227,  -69.8578,  -68.8381,
          -69.0922,  -68.0184,  -71.6060,  -67.6182],
        [ -90.5496,  -89.9516,  -89.9763,  -92.2845, -113.1207,  -90.9139,
          -89.6432,  -89.9178,  -91.2552,  -89.6373],
        [-112.7879, -114.2856, -112.3264, -116.8424, -119.8579, -116.7891,
         -117.6282, -111.9270, -115.7159, -113.0663],
        [ -78.9503,  -75.2481,  -91.6309,  -75.9360,  -79.9945,  -77.1069,
          -78.7858,  -78.0712,  -83.2033,  -75.7442],
        [ -45.5277,  -45.1546,  -46.1313,  -45.0346,  -43.3521,  -52.6794,
          -44.0933,  -45.5044,  -44.7435,  -44.7886],
        [-112.0946, -103.2941, -113.5717, -103.3493, -109.3060, -106.5116,
         -116.2306, -103.6288, -104.6870, -104.4500],
        [-101.5043,  -87.5750, -109.3515,  -89.4267,  -96.7292,  -99.4515,
         -108.1568,  -88.8961,  -88.9146,  -88.9316],
        [-116.6191, -113.9058, -111.5579, -115.3969, -119.8799, -115.3682,
         -144.8181, -114.7816, -114.3284, -114.5495],
        [ -77.9347,  -75.5631,  -76.3291,  -78.8794,  -76.5753,  -82.2715,
          -74.9796,  -77.4713,  -77.3473,  -77.0591],
        [ -48.1207,  -48.0914,  -50.5256,  -48.1234,  -47.1567,  -51.3318,
          -47.0855,  -48.8581,  -48.0143,  -48.6287],
        [-132.4586, -122.5361, -134.9189, -121.0067, -128.4004, -120.1720,
         -147.2949, -117.8026, -118.6786, -116.9385],
        [ -46.6110,  -44.4227,  -50.9317,  -45.1795,  -44.4752,  -49.7328,
          -44.8257,  -44.9833,  -47.6003,  -44.4660],
        [ -81.2657,  -69.5177, -100.2323,  -78.4566,  -78.5908,  -68.9398,
          -66.9594,  -74.0303,  -81.2218,  -69.2885],
        [ -81.7909,  -81.0928,  -89.0614,  -80.5176,  -84.1442,  -82.4642,
          -83.8332,  -85.6855,  -84.1902,  -80.6491],
        [ -80.5387,  -86.0636,  -82.9810,  -83.7128,  -81.3847,  -88.1033,
          -84.9410,  -84.2394,  -92.6269,  -85.2876],
        [ -69.5624,  -65.1138,  -77.4001,  -66.8201,  -66.0127,  -71.1295,
          -72.2149,  -66.4971,  -65.1090,  -65.8191],
        [ -65.2865,  -60.8039,  -67.7035,  -59.7552,  -61.2018,  -58.3832,
          -61.2421,  -60.6073,  -63.8302,  -60.0114],
        [ -70.7009,  -72.2781,  -72.8161,  -74.2308,  -74.9838,  -70.9517,
          -72.5289,  -74.4164,  -78.1510,  -72.3580],
        [-105.8631, -102.0795, -105.5292, -101.2020, -135.6304, -105.6906,
         -111.2700, -102.3759, -104.9582,  -99.7098],
        [ -72.0613,  -70.2152,  -73.9402,  -70.4247,  -71.3094,  -70.0164,
          -69.6126,  -70.5395,  -71.5540,  -69.7313],
        [-125.5994, -127.4988, -123.8735, -123.3980, -148.5187, -134.1219,
         -130.4649, -124.4242, -130.6288, -126.6255],
        [-101.0089,  -92.2206, -114.3905,  -93.4427, -101.3243,  -79.4298,
          -80.5225, -102.5573, -119.1284,  -86.3879],
        [ -92.7001,  -84.9835,  -84.3168,  -91.2378,  -81.6688,  -92.7203,
          -88.1355,  -86.4438,  -89.2174,  -84.9928],
        [ -88.5400,  -79.2178,  -81.7741,  -86.2231,  -81.9970,  -84.9381,
          -79.0069,  -79.3056,  -81.9393,  -80.0432]], device='cuda:2',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([91, 10, 5]), scale: torch.Size([91, 10, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-115.3791, -110.6653, -118.0777, -105.4721, -122.1732, -112.6076,
         -120.4832, -103.9314, -105.6117, -104.8826],
        [ -58.3010,  -55.7973,  -60.4391,  -56.2914,  -59.8655,  -54.6437,
          -55.7722,  -55.4517,  -58.9000,  -55.6401],
        [ -79.3898,  -79.8373,  -79.1592,  -80.0161,  -80.1674,  -80.5852,
          -79.2383,  -80.0011,  -79.7368,  -79.8604],
        [ -69.4562,  -69.5503,  -78.2117,  -69.4084,  -74.1625,  -69.8564,
          -68.9028,  -69.6711,  -72.1286,  -69.3772],
        [-105.0150,  -97.1499, -121.9023,  -97.8047, -123.7799,  -93.7223,
          -94.2059, -107.7913, -108.0360,  -98.0967],
        [-109.5634, -104.5286, -112.2235,  -97.7166, -112.1538, -120.0455,
         -135.6408,  -96.3690,  -95.3226,  -97.4393],
        [ -51.4164,  -49.8599,  -52.9943,  -50.5635,  -47.8628,  -58.3860,
          -49.4421,  -50.0208,  -50.1990,  -50.1828]], device='cuda:2')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([7, 10, 5]), scale: torch.Size([7, 10, 5]))
=============================================================================================
