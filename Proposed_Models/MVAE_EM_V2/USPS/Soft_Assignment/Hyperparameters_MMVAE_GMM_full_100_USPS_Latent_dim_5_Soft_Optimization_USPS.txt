Dataset: USPS
--------------------------------------------
Model Name: MMVAE_GMM_full_100_USPS_Latent_dim_5_Soft_Optimization
==================================================================
MMVAE_GMM_full_100_USPS_Latent_dim_5_Soft_Optimization Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 100
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2500
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 30
--------------------------------------------
Input Dimension = 256
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 256
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 10
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2501
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[ -56.7549,  -59.6559,  -63.1392, -352.5699,  -45.0024,  -63.8167,
          -46.2354,  -45.6208,  -58.7052,  -46.5278],
        [ -84.9843,  -80.5198,  -98.8941, -194.7810, -191.4244,  -79.8918,
         -103.9616, -176.2163,  -88.0082, -130.7833],
        [ -61.1749,  -59.9027,  -56.8319, -410.4189,  -40.7414,  -66.6251,
          -45.9252,  -40.7828,  -63.6121,  -47.9813],
        [-116.8292, -116.4364, -138.0352, -258.5546, -159.3774, -121.0852,
          -69.9707,  -73.5436, -149.5973,  -73.1822],
        [ -97.0763,  -97.7980,  -96.9914, -152.6974, -316.8339,  -98.9890,
         -182.0496, -264.2759,  -96.9545, -215.3737],
        [ -55.2808,  -55.6532,  -65.2994, -354.8516,  -47.3147,  -58.4642,
          -49.8142,  -52.9133,  -58.7498,  -49.0516],
        [ -69.6959,  -68.4537,  -73.3427, -259.8489,  -86.3749,  -68.0598,
          -86.1974,  -82.0855,  -84.1308, -107.6220],
        [-134.3350, -119.1346, -200.7033, -256.4205, -369.2597, -116.8539,
         -199.7552, -253.3932, -234.4683, -218.9099],
        [-124.6477, -133.9218, -157.8069, -219.4747, -121.5765, -161.4259,
          -67.1724,  -66.3807, -192.9936,  -65.9395],
        [-136.1672, -109.2163, -202.4153, -244.6787, -401.2537, -106.1342,
         -179.0539, -254.3995, -238.0080, -222.8385],
        [-214.3444, -232.4623, -208.4859, -220.6455, -325.0251, -299.7281,
         -104.2851, -152.3914, -221.7333, -103.5918],
        [-143.7241, -145.8735, -165.3968, -252.7518, -258.6332, -156.0638,
          -85.8183,  -83.0232, -198.8662,  -87.6324],
        [-131.4559, -168.5363, -115.3441, -204.3570, -257.9453, -125.2406,
          -62.2086,  -62.5374, -111.9252,  -64.6189],
        [ -97.1282,  -95.0598, -114.3225, -206.4751, -231.5464,  -96.0540,
         -216.0891, -334.7389, -163.3760, -281.6625],
        [-114.0137, -113.1797, -173.5527, -263.2695, -403.3992, -111.9532,
         -186.7825, -281.3882, -183.9065, -226.6233],
        [-125.8130, -133.4267, -221.6558, -248.2939, -170.7059, -153.5518,
          -91.4230,  -93.0821, -202.9228,  -92.6101],
        [ -98.8412, -101.7630, -100.1270, -281.9123, -397.0447, -103.4025,
         -296.8211, -347.2348, -106.2649, -271.4765],
        [-128.7409, -121.1140, -123.0424, -154.9532, -331.0020, -115.8173,
         -193.7989, -189.4933, -146.6815, -183.2717],
        [-149.8054, -151.1735, -163.1407, -225.7718, -368.9965, -136.1322,
          -82.3901,  -79.8620, -177.9393,  -81.0754],
        [-113.2573,  -98.7915, -154.7190, -179.6531, -259.8055,  -88.9295,
         -131.8851, -105.7362, -198.4242, -152.9623],
        [-109.6080, -107.7350, -108.2974, -257.5289, -351.3129, -107.5948,
         -309.3182, -293.7604, -121.4031, -287.7148],
        [ -93.4553,  -93.5834,  -94.3499, -194.9922, -237.1418,  -99.7739,
         -193.5212, -223.0132,  -94.1045, -212.7766],
        [ -76.7073,  -75.9464,  -76.4689, -204.0286, -127.2647,  -77.9033,
         -126.8414, -205.3634,  -77.2612, -180.4376],
        [-121.2603, -125.3945, -170.3728, -260.6092, -181.3667, -141.4347,
          -76.4248,  -75.4450, -181.6008,  -72.1318],
        [ -54.2035,  -53.4319,  -57.3484, -296.8368,  -70.8327,  -54.7494,
          -77.3425,  -77.4022,  -61.6328,  -82.1539],
        [-109.4678, -100.6089, -117.9577, -249.3583, -101.4843, -130.3716,
          -71.1177,  -71.4878, -112.4734,  -70.0837],
        [-113.8202, -108.5203, -112.8621, -240.4453, -187.9064,  -99.2307,
          -66.0367,  -64.8006, -120.9720,  -63.4595],
        [ -61.5243,  -63.6032,  -58.0758, -395.0937,  -41.2629,  -69.9839,
          -45.1292,  -41.0037,  -60.9065,  -45.1685],
        [ -74.8493,  -79.1004,  -83.9026, -275.5732, -137.2780,  -74.8051,
         -106.2986, -110.3722,  -97.9051, -128.1665],
        [-101.6662, -106.6120, -106.1069, -250.1808, -309.6892, -106.8051,
         -249.3552, -308.0047, -101.3280, -257.2072],
        [-102.5674, -112.3729, -143.4839, -220.7986, -142.3965, -125.3633,
          -71.1841,  -71.7269, -186.8660,  -71.0665],
        [-243.1428, -248.4843, -229.2125, -204.5811, -375.9662, -302.6454,
         -114.3890, -126.6165, -252.2488, -112.5889],
        [ -74.9851,  -75.4097,  -83.7327, -271.4752, -129.7833,  -74.7460,
          -99.1003, -113.3460,  -93.3105, -118.1389],
        [-107.0497, -105.8877, -150.0421, -251.2108, -203.8016, -125.2885,
          -90.2263,  -94.2141, -144.7266,  -91.2608],
        [ -83.1473,  -75.6475, -139.5416, -260.6457, -181.1517,  -77.7137,
         -102.7994, -158.6797, -147.4222, -140.3484],
        [-100.9591, -101.8271, -104.4081, -400.7137, -319.9519, -101.8046,
         -279.8782, -205.1041, -154.0768, -277.3049],
        [ -58.2852,  -59.7973,  -65.7579, -336.9426,  -50.2619,  -65.4090,
          -50.2088,  -52.5462,  -64.1193,  -50.2969],
        [-122.1133, -128.5046, -168.4405, -251.9065, -165.0368, -147.0963,
          -74.3928,  -74.8923, -197.1146,  -75.0540],
        [-111.9775, -115.7975, -112.0158, -342.8044, -368.8619, -112.9294,
         -305.4940, -330.3647, -126.1986, -292.9858],
        [-103.4091, -105.2550, -102.4994, -238.0375, -310.3135, -105.8120,
         -206.4782, -285.0610, -101.5409, -213.6708],
        [ -95.0844,  -93.3469,  -94.4725, -158.3413, -244.1102,  -96.7686,
         -163.6771, -131.9785, -110.4812, -168.9745],
        [ -87.5841,  -88.2371, -131.0421, -171.5732, -327.9834,  -92.7788,
         -154.3768, -312.8419,  -90.4544, -209.8496],
        [ -90.0718,  -85.9113,  -97.7249, -233.8420, -168.6711, -110.1193,
          -84.4783, -113.4034, -126.1947,  -84.0299],
        [-201.3906, -199.1590, -199.1486, -220.7829, -428.3069, -171.2229,
          -91.3732,  -86.5488, -238.2752,  -85.1202],
        [-100.0383, -101.0027, -100.4696, -318.3924, -348.0056, -101.6391,
         -290.8122, -286.0080, -105.7020, -277.8720],
        [-112.3385, -115.3613, -165.2962, -215.6215, -157.7058, -137.3800,
          -77.2455,  -76.2180, -197.2496,  -74.8108],
        [ -72.0504,  -73.0754,  -81.6039, -237.8802, -139.9640,  -72.7556,
          -82.0604,  -97.5457, -108.9221,  -98.3742],
        [ -88.2176,  -88.3568,  -96.7559, -271.9475, -157.6415,  -95.9145,
          -58.6223,  -61.0775,  -99.9455,  -58.5667],
        [ -91.4995,  -91.6649,  -91.0807, -146.0360, -243.9666,  -93.7009,
         -171.9919, -233.9621,  -92.2739, -179.7447],
        [-118.4385, -121.5734, -116.4163, -232.2675, -344.3027, -121.0775,
         -252.6954, -320.1522, -116.3590, -270.2574],
        [ -87.2683, -105.6002, -109.9867, -234.7616, -153.1184, -108.5205,
          -58.1737,  -57.0067,  -87.6393,  -60.9589],
        [ -96.7003,  -94.0526, -119.5473, -189.3923, -225.1108,  -93.4057,
         -148.4615, -258.9511, -114.2494, -196.5980],
        [-101.6167,  -99.7290,  -99.1168, -283.1500, -367.5793, -101.3577,
         -219.9767, -302.8683, -115.9576, -235.4067],
        [-111.5194, -114.8403, -182.4554, -267.9214, -142.8880, -123.2869,
          -80.7238,  -79.7990, -194.6720,  -76.3347],
        [ -91.3197,  -93.4648, -153.8078, -225.4804, -198.6984,  -92.9755,
          -97.6914, -110.5156, -174.2035, -101.4485],
        [-115.8033, -114.1820, -145.6234, -217.4522, -411.0140, -114.2138,
         -152.2187, -207.8871, -153.5212, -167.4755],
        [-161.7027, -158.8177, -238.4720, -260.7928, -229.5543, -160.8414,
          -87.3529, -114.9115, -266.3054,  -84.5663],
        [ -60.7529,  -62.5332,  -59.2195, -404.8766,  -42.8479,  -68.2014,
          -45.5331,  -44.6262,  -60.0771,  -47.5431],
        [ -79.9408,  -79.8791, -130.3671, -238.2563, -144.4828,  -82.6092,
          -76.8948,  -76.2772, -113.9916,  -76.8644],
        [-207.6419, -206.8009, -260.8224, -285.1298, -274.5553, -239.8608,
          -85.7915,  -95.5968, -281.2412,  -86.0100],
        [ -95.8208,  -96.6319,  -99.3878, -206.1201, -326.8475,  -99.7877,
         -211.7328, -307.1800,  -96.5605, -214.5373],
        [-130.7051, -126.9940, -174.2013, -234.7478, -368.6342, -128.1232,
         -232.1515, -291.6488, -209.1095, -263.8859],
        [ -99.5460,  -98.9008, -159.8913, -227.7657, -192.4524,  -98.7528,
         -115.1476, -218.4420, -158.7522, -115.5998],
        [ -87.8897,  -88.9124,  -87.9502, -198.7216, -210.3135,  -89.9510,
         -148.9344, -285.2839,  -88.3185, -210.5640],
        [-164.0027, -154.7916, -215.7695, -255.0084, -302.8417, -151.4044,
         -119.5589, -123.1158, -247.3811, -119.1514],
        [ -99.5664,  -98.6683,  -99.4123, -195.4126, -319.4144, -102.4571,
         -208.3663, -229.0318, -105.3214, -229.9266],
        [-155.8006, -161.8254, -170.2800, -247.3139, -226.4295, -163.0452,
          -81.8856,  -89.7930, -179.2476,  -82.3114],
        [-117.7220, -123.3170, -131.6703, -239.1171, -275.1017, -125.6119,
          -74.5632,  -71.9251, -145.4373,  -74.1292],
        [ -59.7002,  -63.2225,  -59.3004, -376.8198,  -39.4355,  -64.1748,
          -44.7465,  -40.7509,  -57.6158,  -44.4729],
        [ -57.5844,  -59.3294,  -61.9113, -383.5818,  -45.6332,  -64.7743,
          -44.6964,  -45.0466,  -58.3681,  -45.0408],
        [-104.9918, -105.8983, -105.1489, -272.3525, -392.7144, -112.5593,
         -263.8598, -318.2527, -116.6971, -253.0650],
        [-120.1877, -115.2993, -115.9289, -228.2746, -357.5153, -113.6647,
         -231.2251, -358.7564, -119.3543, -258.5209],
        [-137.2257, -136.3473, -179.7811, -267.7967, -209.3038, -146.1815,
          -76.5196,  -85.7801, -191.0581,  -76.2499],
        [-322.7690, -368.2446, -433.1615, -281.8349, -414.4997, -432.4099,
          -92.9687, -101.2045, -439.6696,  -94.0479],
        [-108.4195, -124.7748, -142.6406, -176.8153, -426.7134, -110.0026,
         -225.5466, -250.5576, -149.3892, -210.0394],
        [ -84.0963,  -85.3387, -114.8832, -279.7411, -224.1055,  -84.7775,
         -135.6686, -154.4325, -131.9113, -174.0457],
        [-117.3614, -118.5846, -177.7341, -217.0845, -389.5605, -116.2576,
         -174.1997, -427.3671, -167.5349, -224.9618],
        [-115.0803,  -76.0662, -289.5179, -295.0438, -253.4279,  -80.3773,
         -170.1453, -117.6652, -282.0496, -261.0322],
        [-101.0217, -101.7012, -156.1658, -184.3438, -296.1261, -100.4602,
         -142.2607, -328.4384, -141.3219, -174.0037],
        [-132.2272, -129.5219, -143.0120, -223.3576, -231.3487, -136.0165,
         -103.1873, -104.0040, -140.5653, -102.8162],
        [ -59.2141,  -60.2194,  -58.3865, -349.7025,  -38.6723,  -68.9248,
          -41.9166,  -38.9305,  -54.2529,  -42.4100],
        [ -94.9924,  -92.8793, -113.1485, -230.0707, -248.2744,  -95.4666,
         -123.3962, -177.8011, -122.9571, -156.8001],
        [-103.1296, -103.8767,  -98.7505, -248.6623, -288.7474, -103.8441,
         -194.4336, -348.2889, -101.1267, -216.5692],
        [ -70.0195,  -69.2231,  -71.7177, -253.5222, -119.5954,  -72.9136,
         -121.0962, -203.6508,  -73.8170, -163.8565],
        [ -84.2521,  -93.7546, -125.0043, -191.8363, -307.1316,  -91.9504,
          -68.8444,  -67.8005, -102.0891,  -71.7770],
        [-173.9407, -210.4407, -249.9936, -244.7008, -275.9991, -246.0120,
          -99.1442,  -98.2213, -292.0261,  -99.8940],
        [ -96.5285, -102.0350, -123.2570, -257.4700, -136.5909, -118.0430,
          -72.3159,  -70.6048, -126.5662,  -70.8386],
        [-102.3178, -100.2367, -123.5792, -163.3498, -357.7344, -103.6669,
         -173.6490, -328.5264, -115.2615, -228.3275],
        [ -57.4595,  -59.2968,  -61.0728, -394.9999,  -42.3077,  -66.9578,
          -43.5155,  -41.9017,  -59.8954,  -44.3252],
        [-166.9852, -185.2894, -174.7082, -187.8009, -555.5676, -150.0191,
          -89.7293,  -80.4682, -173.5987,  -86.8553],
        [-105.0409, -107.7046, -109.9479, -217.8402, -364.4539, -107.3071,
         -242.5177, -268.3188, -108.7223, -252.0631]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([91, 10, 5]), scale: torch.Size([91, 10, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-103.4167, -105.7621, -103.7868, -255.8148, -274.4646, -104.3639,
         -262.8654, -212.5563, -119.3682, -251.0573],
        [ -99.4413,  -95.6171,  -89.5701, -287.4048, -123.1738,  -90.4427,
          -55.4062,  -56.1559,  -95.1993,  -56.3964],
        [ -80.2473,  -80.6029,  -99.2286, -192.4192, -248.7386,  -80.7572,
         -148.9716, -276.0439,  -82.4641, -212.0536],
        [-118.4171, -124.5275, -159.1405, -271.0862, -124.5638, -146.0359,
          -68.6983,  -68.7458, -185.6709,  -69.3172],
        [-272.2381, -294.3271, -235.4648, -242.4884, -288.7747, -287.5134,
         -100.1844, -158.0374, -272.1647, -104.9543],
        [ -95.3563,  -97.2977,  -95.4708, -350.7882, -327.4219,  -99.1060,
         -278.6974, -303.9067, -100.8170, -277.9993],
        [ -58.5577,  -59.9952,  -67.2450, -345.0797,  -48.0801,  -63.2345,
          -50.5878,  -52.8080,  -68.1632,  -51.7476]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([7, 10, 5]), scale: torch.Size([7, 10, 5]))
=============================================================================================
