Dataset: HAR
--------------------------------------------
Model Name: MMVAE_GMM_full_5_HAR_Latent_dim_10_Soft_Optimization
==================================================================
MMVAE_GMM_full_5_HAR_Latent_dim_10_Soft_Optimization Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 5
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 561
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 10
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 561
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 6
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-583.7729, -522.2407, -624.2117, -586.3994, -521.8485, -525.0664],
        [-615.2300, -518.5223, -606.5897, -561.5385, -518.7525, -518.5989],
        [-631.1998, -519.2898, -654.3517, -597.7293, -526.0615, -522.0688],
        [-605.1459, -519.4774, -644.4128, -525.3669, -519.5958, -519.3766],
        [-623.0437, -518.5013, -649.4882, -613.9062, -518.5316, -518.5244],
        [-520.7567, -531.0054, -521.4033, -521.8372, -522.2447, -548.3197],
        [-524.3474, -538.7751, -524.9309, -527.1495, -541.6368, -588.3977],
        [-591.9761, -522.5797, -636.2179, -579.5060, -522.8012, -523.2010],
        [-526.9091, -533.3462, -532.2975, -526.2130, -544.6865, -600.3737],
        [-523.1509, -524.1777, -522.7869, -522.7800, -536.8649, -582.7145],
        [-523.6327, -550.8022, -539.8336, -540.7161, -541.9443, -606.2106],
        [-624.9138, -520.2612, -690.0718, -622.9960, -526.4290, -519.7921],
        [-621.4766, -525.4539, -639.3444, -583.5134, -525.0247, -525.2394],
        [-521.6413, -523.9620, -523.5459, -521.5658, -524.6455, -589.3542],
        [-593.7637, -521.0043, -619.2338, -602.4195, -521.1406, -520.8721],
        [-605.6459, -519.6724, -649.2551, -597.6981, -520.3273, -519.9991],
        [-575.9851, -521.1382, -628.4357, -590.7353, -525.9823, -520.0109],
        [-613.1294, -528.1548, -628.1725, -586.5833, -528.1807, -528.1315],
        [-526.5328, -528.9423, -525.7218, -525.7006, -541.2806, -609.8659],
        [-520.3251, -526.6180, -520.2113, -520.1929, -522.2247, -573.5765],
        [-583.5999, -526.3811, -640.4404, -600.6437, -545.5083, -527.8120],
        [-526.2412, -563.8915, -552.6917, -535.6225, -570.8077, -586.4606],
        [-609.1861, -520.5684, -644.7014, -607.2397, -521.6242, -522.9752],
        [-521.8783, -531.7584, -521.7463, -522.1168, -522.5032, -555.5243],
        [-577.6086, -520.8298, -601.2200, -570.1284, -521.3894, -520.5006],
        [-615.6035, -526.9744, -665.6638, -620.7001, -534.1727, -525.9147],
        [-527.7923, -534.6931, -528.0244, -528.5354, -549.1969, -597.6258],
        [-598.5720, -519.7039, -596.1670, -567.2162, -520.0527, -520.8419],
        [-592.3300, -519.7299, -643.1365, -596.9596, -519.9576, -520.3822],
        [-527.5662, -534.9740, -527.6129, -527.4790, -535.4007, -574.4465],
        [-524.5169, -526.2625, -524.1808, -525.7074, -527.4244, -566.5984],
        [-585.1812, -527.3873, -654.4461, -573.2928, -532.9445, -527.2402],
        [-603.5905, -519.3110, -647.6418, -589.3778, -520.7218, -520.8373],
        [-522.9355, -523.8575, -522.1549, -521.4044, -527.6210, -580.6144],
        [-562.5498, -524.9951, -581.6526, -550.4559, -527.2980, -526.8728],
        [-617.5379, -522.6527, -657.8264, -608.2832, -524.2083, -523.3729],
        [-624.0916, -524.3544, -691.7599, -615.3134, -536.2255, -526.0536],
        [-523.1824, -529.4615, -524.4396, -524.4027, -525.8223, -591.4458],
        [-600.1414, -523.4047, -606.2641, -577.2949, -526.3113, -524.1888],
        [-616.7646, -519.7991, -650.8418, -570.1376, -518.7504, -518.3110],
        [-521.5405, -530.4917, -521.6946, -521.6443, -524.6254, -583.9806],
        [-525.1623, -531.9704, -525.2247, -526.1310, -538.2366, -588.2275],
        [-526.3060, -526.2458, -525.3151, -525.5027, -541.5718, -592.9066],
        [-596.8270, -520.2303, -634.0074, -590.1596, -519.6829, -519.6123],
        [-523.5005, -528.0549, -523.1963, -523.2225, -529.7403, -595.2017],
        [-620.1476, -519.4857, -654.7414, -563.6008, -519.5226, -521.8057],
        [-523.7914, -526.3457, -523.9338, -524.0708, -534.1830, -593.7142],
        [-598.7662, -519.6042, -632.1776, -573.2484, -520.0171, -519.6176],
        [-620.8958, -519.6632, -660.2770, -573.7981, -519.4839, -519.8918],
        [-609.0648, -517.9494, -649.5356, -584.3969, -518.1648, -518.2734],
        [-554.8411, -521.7504, -592.8910, -560.6473, -534.6349, -530.0398],
        [-610.6561, -518.9881, -641.9697, -596.4644, -520.3937, -519.7200]],
       device='cuda:1', grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([52, 6, 10]), scale: torch.Size([52, 6, 10]))
=============================================================================================
Decoder Loss (Test) = tensor([[-526.4502, -528.6987, -526.2991, -526.2216, -532.2804, -598.6597],
        [-527.4954, -530.9868, -527.4380, -527.3887, -533.6373, -597.3818],
        [-526.5123, -529.3375, -526.3102, -526.2606, -532.4863, -592.3859],
        [-524.4949, -527.0596, -524.0903, -524.0641, -529.2066, -596.3558],
        [-522.9788, -525.9717, -522.5261, -522.5306, -526.2711, -588.8539],
        [-523.2179, -527.1539, -522.7435, -522.7994, -524.2851, -584.2479],
        [-524.3116, -526.4487, -523.8114, -523.8060, -528.4832, -595.6292],
        [-525.8666, -528.6649, -525.7264, -525.7126, -531.5753, -595.9888],
        [-521.9355, -531.5486, -521.6245, -521.6005, -522.0153, -573.2505],
        [-522.1541, -531.1434, -521.7687, -521.7894, -521.9492, -573.5546],
        [-521.9170, -533.6529, -521.4711, -521.5002, -522.4073, -567.6138],
        [-521.4405, -533.4238, -521.1608, -521.1363, -522.1899, -566.6007],
        [-520.8082, -530.6836, -520.2148, -520.2280, -520.8812, -565.9984],
        [-521.6390, -533.1838, -521.2697, -521.2334, -522.0664, -564.2722],
        [-520.6738, -532.4902, -520.4681, -520.4089, -521.2999, -565.5377],
        [-521.2219, -531.3868, -520.7684, -520.7913, -521.1411, -568.3974],
        [-522.7762, -531.6293, -522.1863, -522.2316, -522.2556, -569.3238],
        [-522.6572, -530.9637, -522.0111, -522.0812, -522.0457, -574.0121],
        [-522.4564, -532.1691, -521.8649, -521.9082, -522.3416, -567.6692],
        [-523.1257, -530.1752, -522.5808, -522.6606, -522.7300, -575.0463],
        [-522.3010, -530.5609, -521.8389, -521.9000, -521.9972, -575.5964],
        [-524.5031, -532.4808, -523.6215, -523.7559, -523.7842, -578.6479],
        [-524.0294, -530.7768, -523.1217, -523.2533, -523.2536, -576.2560],
        [-522.5186, -530.2643, -521.8813, -521.9388, -521.9575, -576.5317],
        [-522.6030, -530.8080, -521.9359, -521.9852, -522.0802, -576.4736],
        [-522.6431, -532.4639, -522.5234, -522.4772, -522.7131, -572.2264],
        [-521.8466, -531.6301, -521.5805, -521.5687, -521.7301, -569.1974],
        [-521.6096, -532.4238, -521.4563, -521.4121, -521.8162, -565.9395],
        [-524.5383, -527.0244, -524.2751, -524.2465, -528.2203, -591.1998],
        [-526.0336, -528.3948, -525.9755, -525.8861, -530.7773, -595.0135],
        [-523.8634, -526.5864, -523.7684, -523.7202, -528.5385, -589.0952],
        [-525.2524, -528.9704, -525.0051, -524.9874, -528.0276, -587.4005],
        [-523.6506, -527.7010, -523.4911, -523.4850, -527.8868, -590.1156],
        [-522.9603, -525.3262, -522.5469, -522.5106, -529.2677, -594.6814],
        [-522.1047, -524.4820, -521.5236, -521.5129, -526.5876, -590.4047],
        [-522.6857, -525.2267, -522.1078, -522.1173, -526.1180, -591.5960],
        [-525.3096, -528.2365, -525.1694, -525.1385, -528.7397, -596.0618],
        [-525.0323, -532.6783, -524.2776, -524.4086, -524.5344, -576.5475],
        [-522.9577, -530.4316, -522.4301, -522.4924, -522.6267, -576.1356],
        [-521.3059, -530.7464, -521.1375, -521.1136, -521.2435, -571.8289],
        [-521.0624, -530.8319, -520.5580, -520.5886, -520.7805, -566.9047],
        [-520.3694, -530.8087, -519.9177, -519.9222, -520.8135, -565.0956],
        [-522.3547, -531.0728, -521.8477, -521.8822, -522.1117, -572.6086],
        [-522.5507, -529.8660, -521.7612, -521.8593, -521.9661, -576.2202],
        [-522.5805, -530.3609, -522.1660, -522.1945, -522.1820, -571.8060],
        [-521.9708, -532.0891, -522.1365, -522.0324, -522.3282, -565.1340],
        [-521.2792, -531.2474, -521.4797, -521.3538, -521.7537, -567.1716]],
       device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([47, 6, 10]), scale: torch.Size([47, 6, 10]))
=============================================================================================
