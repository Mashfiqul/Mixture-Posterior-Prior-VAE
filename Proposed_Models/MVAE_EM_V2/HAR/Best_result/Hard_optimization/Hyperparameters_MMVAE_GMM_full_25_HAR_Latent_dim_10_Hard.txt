Dataset: HAR
--------------------------------------------
Model Name: MMVAE_GMM_full_25_HAR_Latent_dim_10_Hard_Optimization
==================================================================
MMVAE_GMM_full_25_HAR_Latent_dim_10_Hard_Optimization Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 25
--------------------------------------------
Device = cuda:1
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 561
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 10
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 561
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 6
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-522.0419, -519.5375, -573.9006, -556.1923, -532.1441, -541.4089],
        [-523.8268, -525.1615, -524.0203, -524.8854, -523.7383, -524.5660],
        [-523.6278, -524.3312, -541.6119, -542.1542, -524.0739, -527.6265],
        [-531.2130, -526.6841, -524.2489, -525.3801, -529.2730, -554.2961],
        [-526.4443, -523.3608, -538.9723, -553.9132, -521.6572, -521.1420],
        [-524.6569, -524.2585, -525.9680, -522.7347, -523.0032, -524.1876],
        [-529.9669, -525.1273, -523.3593, -523.0847, -524.0077, -570.5546],
        [-524.2611, -523.7317, -524.0221, -523.3763, -523.4818, -523.7617],
        [-521.8792, -519.7440, -538.2307, -544.6517, -522.6766, -531.8711],
        [-524.0884, -518.9344, -548.5011, -543.9042, -527.2590, -540.1171],
        [-521.1325, -518.2706, -564.6097, -560.1592, -531.2892, -539.1378],
        [-527.8760, -529.4057, -525.5515, -526.2562, -526.0782, -527.7046],
        [-523.5371, -523.2969, -528.8987, -521.7341, -521.7313, -525.0101],
        [-524.2184, -526.2518, -524.1459, -526.4518, -525.6382, -524.2368],
        [-527.6298, -522.2435, -522.7817, -522.4944, -521.1310, -523.3849],
        [-519.5466, -520.7512, -538.1847, -531.6908, -519.5500, -524.4600],
        [-523.7084, -526.8463, -558.4653, -545.2505, -524.7775, -530.5255],
        [-532.2108, -533.8674, -522.2575, -525.9216, -522.5878, -565.6677],
        [-526.8701, -527.7307, -532.4390, -526.8024, -524.5438, -524.0775],
        [-524.1967, -523.4655, -551.7971, -542.1093, -526.3912, -535.5266],
        [-539.6434, -538.5829, -522.1155, -524.9200, -523.2034, -664.5923],
        [-524.1290, -523.5703, -522.9421, -523.0481, -522.8809, -523.7869],
        [-524.7877, -523.6061, -522.0267, -523.0558, -522.6953, -524.3865],
        [-521.6509, -521.4100, -557.4684, -541.1550, -523.3391, -525.7209],
        [-520.5231, -518.9328, -539.9249, -544.6736, -521.3840, -527.0079],
        [-523.1285, -526.7039, -524.4525, -525.4346, -524.3638, -523.1906],
        [-526.0742, -527.4490, -527.4909, -525.6793, -531.0048, -525.9602],
        [-526.4371, -526.5111, -534.2990, -544.1348, -527.4979, -537.2723],
        [-526.1949, -526.9281, -524.8920, -526.3754, -526.7884, -633.4796],
        [-524.3578, -524.3239, -523.5433, -524.1643, -522.7344, -523.7906],
        [-523.0658, -521.8879, -556.1396, -552.4070, -525.6315, -526.1972],
        [-522.9982, -525.9496, -534.4327, -528.9701, -522.5421, -524.9525],
        [-526.6736, -528.2500, -528.6759, -527.8795, -524.6002, -524.6096],
        [-526.1477, -530.6414, -521.7160, -522.0459, -521.8185, -522.4324],
        [-524.8558, -525.7940, -526.0021, -526.5206, -524.6559, -526.7646],
        [-524.4946, -524.8834, -526.5061, -523.6118, -524.0435, -524.0789],
        [-521.4623, -522.5904, -538.6307, -543.6972, -521.2420, -529.6985],
        [-519.4796, -519.3702, -546.3976, -549.2291, -520.9478, -531.5217],
        [-526.9856, -534.2715, -522.5540, -525.6179, -527.5710, -520.7802],
        [-524.3569, -519.9023, -519.5413, -519.1042, -524.6342, -537.4736],
        [-520.5871, -519.1044, -556.7814, -565.2538, -522.9457, -531.4023],
        [-523.0386, -524.1493, -534.0004, -558.8209, -525.4492, -527.5075],
        [-531.6080, -530.5425, -528.6622, -531.5683, -530.1736, -558.4984],
        [-524.7853, -526.1710, -525.4483, -525.3171, -527.7542, -525.9445],
        [-520.5750, -520.3516, -531.8306, -542.2350, -520.8445, -528.0770],
        [-520.5041, -519.0605, -522.4955, -523.2935, -534.9851, -536.4350],
        [-534.0550, -534.8651, -522.3548, -524.3357, -523.4739, -692.7996],
        [-522.9145, -519.9006, -553.2267, -548.4018, -526.0388, -535.2422],
        [-520.8846, -519.2228, -553.6026, -558.7361, -523.8962, -533.6285],
        [-530.1537, -532.5350, -527.3330, -529.5027, -529.9459, -530.3822],
        [-520.9536, -520.1108, -556.3121, -557.0258, -525.6277, -539.2231],
        [-523.6434, -526.2184, -521.9723, -522.2533, -529.0537, -523.4608]],
       device='cuda:1', grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([52, 6, 10]), scale: torch.Size([52, 6, 10]))
=============================================================================================
Decoder Loss (Test) = tensor([[-526.2358, -526.9142, -526.0347, -526.0005, -525.4336, -526.2766],
        [-527.7349, -527.1129, -527.3684, -527.8650, -527.1376, -527.9515],
        [-526.7786, -526.4850, -526.2423, -526.5575, -526.2147, -527.3501],
        [-524.4238, -524.7937, -524.0428, -524.2944, -523.8151, -524.0302],
        [-522.6613, -523.9957, -522.4691, -522.7546, -522.2816, -522.2178],
        [-522.8891, -524.7628, -522.7849, -523.2067, -522.8302, -522.4847],
        [-523.9835, -524.8838, -523.6993, -524.0874, -523.5347, -523.4019],
        [-525.7187, -526.2216, -525.7588, -526.0597, -525.5183, -525.4885],
        [-523.1194, -525.5811, -534.0174, -526.1708, -521.9192, -521.6993],
        [-522.9598, -525.8634, -534.7615, -527.9099, -522.1232, -521.5861],
        [-523.2726, -526.2965, -540.8712, -535.3943, -521.8984, -521.5240],
        [-522.9119, -525.3063, -540.0646, -534.6957, -521.3997, -521.1553],
        [-521.6608, -524.4505, -538.6879, -531.2411, -520.5961, -520.1414],
        [-522.9693, -525.7023, -541.7399, -535.9071, -521.6442, -521.3035],
        [-522.3035, -524.9418, -540.2002, -534.9117, -520.8027, -520.5648],
        [-522.3002, -524.4128, -540.3208, -534.0840, -521.1973, -520.6901],
        [-523.3998, -525.9694, -538.7944, -529.9310, -522.5775, -522.0483],
        [-523.0449, -525.6089, -535.0339, -527.4376, -522.3991, -521.7818],
        [-523.3087, -525.7801, -533.1039, -526.8925, -522.1166, -521.7722],
        [-523.3572, -525.7961, -529.6552, -525.0479, -522.7721, -522.2920],
        [-522.8001, -524.9501, -530.1644, -525.0962, -522.1234, -521.6912],
        [-524.6639, -526.5859, -529.8890, -525.5276, -523.7456, -523.4149],
        [-523.7426, -525.2773, -529.0919, -524.9374, -523.1194, -522.7866],
        [-522.6124, -524.3665, -532.2240, -526.2164, -522.1706, -521.5414],
        [-522.9119, -524.9476, -530.4562, -525.2955, -522.1532, -521.7997],
        [-524.0170, -526.0287, -541.0807, -531.8807, -522.7118, -522.4362],
        [-522.8961, -524.8275, -540.0460, -531.4437, -521.8034, -521.4357],
        [-523.0040, -525.1517, -541.0506, -534.4449, -521.7421, -521.4255],
        [-524.1370, -525.0204, -523.7688, -524.1217, -523.5623, -523.4734],
        [-525.4997, -526.1771, -525.2076, -525.4263, -524.8174, -524.8503],
        [-523.7090, -524.1228, -523.4717, -523.7469, -523.2086, -523.1370],
        [-525.2052, -525.9772, -524.8167, -525.2804, -525.0822, -524.8878],
        [-523.6692, -524.8387, -523.3376, -523.6724, -523.5299, -523.8355],
        [-522.8629, -523.6830, -522.4266, -522.5828, -522.2781, -523.4763],
        [-521.9105, -523.1376, -521.4559, -521.6991, -521.2332, -521.5753],
        [-522.4375, -523.0730, -522.1240, -522.4288, -521.9096, -521.9034],
        [-525.3326, -526.0051, -525.1835, -525.4051, -524.7560, -524.5251],
        [-525.3850, -527.1007, -529.6932, -525.9017, -524.4249, -524.1829],
        [-523.3367, -525.2950, -528.9960, -524.2708, -522.5842, -522.3190],
        [-522.2601, -524.3778, -532.2888, -524.7188, -521.4062, -521.0990],
        [-521.9378, -524.8176, -532.9836, -525.8727, -520.9655, -520.4716],
        [-521.4865, -523.8864, -537.1575, -529.2566, -520.1572, -519.8920],
        [-522.9833, -524.8517, -536.8533, -528.2946, -522.0776, -521.7327],
        [-522.5562, -524.3378, -527.5299, -523.1692, -521.7480, -521.6588],
        [-523.0721, -524.8463, -532.4934, -525.2347, -522.2750, -522.0201],
        [-523.4963, -525.6509, -539.9843, -531.2993, -522.3572, -522.0239],
        [-523.0583, -525.1870, -536.3350, -527.1128, -521.6742, -521.4255]],
       device='cuda:1')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([47, 6, 10]), scale: torch.Size([47, 6, 10]))
=============================================================================================
