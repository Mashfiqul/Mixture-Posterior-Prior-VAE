Dataset: Reuters
--------------------------------------------
Model Name: MMVAE_GMM_full_5_Reuters_Latent_dim_5_Hard_Optimization
==================================================================
MMVAE_GMM_full_5_Reuters_Latent_dim_5_Hard_Optimization Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 5
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 2000
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 5
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 2000
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 4
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-2317.0801, -2318.0605, -2316.2966, -2316.3188],
        [-2511.5171, -2398.8398, -2584.5667, -2331.9014],
        [-2468.8557, -2468.2300, -2466.7939, -2467.9163],
        [-2705.8699, -2708.8923, -2703.4541, -2703.9700],
        [-1908.5640, -1921.9592, -1927.2510, -1907.0718],
        [-2569.8779, -2568.3462, -2571.2188, -2568.7300],
        [-2673.2847, -2672.2507, -2684.3855, -2669.1836],
        [-2328.4248, -2337.3342, -2392.4375, -2342.3416],
        [-2611.6650, -2613.2429, -2617.9321, -2614.0283],
        [-2682.6250, -2680.9678, -2684.3555, -2680.8218],
        [-2503.6738, -2507.1514, -2515.4595, -2502.0278],
        [-2591.9414, -2594.3350, -2593.4253, -2591.8196],
        [-2772.9028, -2773.3337, -2783.0337, -2774.9717],
        [-2718.4724, -2714.6675, -2715.6191, -2715.1548],
        [-2355.3713, -2367.5686, -2371.8870, -2371.8713],
        [-2621.3474, -2619.6562, -2657.9326, -2618.4001],
        [-2691.2593, -2691.9824, -2691.0977, -2689.6594],
        [-2533.6106, -2511.3931, -2534.3247, -2508.3262],
        [-2515.4285, -2515.8770, -2514.1851, -2517.9949],
        [-2147.9194, -2139.8091, -2138.9768, -2140.8750],
        [-2011.3690, -2007.6577, -1990.8636, -2003.6592],
        [-2237.2395, -2234.8613, -2220.9253, -2227.3333],
        [-2745.0476, -2744.9248, -2746.6873, -2744.4856],
        [-2570.8328, -2575.0806, -2585.3696, -2571.1848],
        [-1955.6140, -1950.9509, -1931.5210, -1944.4979],
        [-2712.9832, -2713.8770, -2714.9543, -2713.2656],
        [-2444.4573, -2430.9829, -2431.4451, -2431.3828],
        [-2106.8970, -2123.6099, -2127.5791, -2104.7539],
        [-2741.8369, -2739.6538, -2742.7891, -2743.2627],
        [-2653.1526, -2652.0056, -2666.6411, -2654.1519],
        [-2591.2937, -2596.8413, -2590.6458, -2592.4182],
        [-2573.0125, -2573.7556, -2573.1489, -2573.3838],
        [-2735.8262, -2716.1589, -2750.8320, -2706.5303],
        [-2475.6321, -2475.3813, -2474.8760, -2474.3965],
        [-2638.9165, -2638.1121, -2638.4951, -2638.0947],
        [-2732.4089, -2729.4954, -2730.2930, -2728.7656],
        [-2253.9736, -2256.1499, -2254.8721, -2254.1189],
        [-2437.3640, -2441.9849, -2476.1592, -2440.2832],
        [-2808.4688, -2809.1714, -2809.2507, -2809.4851],
        [-2588.8396, -2586.0464, -2586.0088, -2585.8784],
        [-2122.8918, -2129.0327, -2117.8435, -2121.0857],
        [-2410.6543, -2404.8093, -2407.1992, -2407.4963],
        [-2718.6194, -2718.9722, -2717.9634, -2718.2495],
        [-2754.1133, -2753.6309, -2753.4028, -2753.4497],
        [-2177.0315, -2138.0820, -2213.4170, -2113.4526],
        [-2427.5088, -2454.3804, -2449.1636, -2455.1670],
        [-2467.5771, -2466.5947, -2481.1423, -2468.1431],
        [-2500.0552, -2496.5894, -2494.6570, -2496.4995],
        [-2603.5647, -2597.5552, -2603.2002, -2597.5518],
        [-2708.8171, -2704.1543, -2727.7563, -2704.5337],
        [-2736.9182, -2735.5730, -2749.2395, -2736.2371],
        [-2257.6313, -2253.9187, -2247.2046, -2254.0759],
        [-2448.3191, -2427.0376, -2533.1997, -2419.7549],
        [-2723.5942, -2725.2773, -2722.4424, -2723.8352],
        [-2716.6499, -2713.9644, -2725.8379, -2717.9783],
        [-2620.6956, -2615.4463, -2626.2507, -2617.1462],
        [-2697.3762, -2696.4321, -2705.0930, -2697.2349],
        [-2697.0398, -2695.2373, -2694.2095, -2695.8311],
        [-2722.3652, -2722.8491, -2722.0171, -2721.2275],
        [-2025.9877, -2045.2972, -2041.6497, -2021.2626],
        [-2260.7661, -2264.9912, -2266.8838, -2301.9180],
        [-2504.6584, -2478.4089, -2528.6582, -2476.5190],
        [-2168.7695, -2161.0771, -2143.7271, -2159.3921],
        [-1889.6001, -1961.1990, -1955.2998, -1939.3115],
        [-2535.7417, -2532.9053, -2536.0508, -2532.5659],
        [-2539.1851, -2535.3413, -2574.4263, -2531.1089],
        [-2718.0059, -2713.9673, -2722.2378, -2717.7246],
        [-2306.9961, -2315.0518, -2351.9204, -2312.1819],
        [-2658.6831, -2653.8813, -2688.9746, -2654.4983],
        [-2406.7231, -2409.0913, -2409.0059, -2414.7400],
        [-2659.4983, -2656.3000, -2675.9243, -2658.9766],
        [-2706.9331, -2690.6260, -2767.0818, -2683.9829],
        [-2740.7148, -2739.9099, -2750.4333, -2739.9397],
        [-2367.8899, -2381.9165, -2434.4890, -2379.2869],
        [-2015.1777, -2020.3673, -1999.2844, -2022.4480],
        [-2284.6318, -2266.6567, -2326.5117, -2235.8643],
        [-2476.3743, -2455.6055, -2496.2983, -2451.4353],
        [-2273.3870, -2276.3291, -2277.2231, -2273.5977],
        [-2212.6545, -2226.4780, -2231.3611, -2212.6919],
        [-2509.0366, -2480.4058, -2547.4353, -2481.5576],
        [-2497.0056, -2478.7795, -2538.1816, -2477.6729],
        [-2641.5488, -2626.8970, -2694.3943, -2619.5188],
        [-2643.3721, -2639.7847, -2652.7397, -2641.8474],
        [-2729.9619, -2729.6191, -2727.1765, -2732.3223],
        [-2529.0635, -2530.4556, -2557.7114, -2553.4275],
        [-2664.0369, -2664.1560, -2663.6973, -2668.9929],
        [-2160.0664, -2159.9231, -2153.4185, -2164.4380],
        [-2395.6538, -2394.9849, -2489.5625, -2393.1655],
        [-2449.3718, -2460.5776, -2472.0068, -2482.4653],
        [-2664.9009, -2663.9761, -2665.2339, -2664.2925],
        [-2713.1655, -2712.9229, -2714.1982, -2710.6743],
        [-2430.1765, -2370.8096, -2533.2871, -2302.1904],
        [-2734.7559, -2735.3184, -2734.7588, -2734.5894],
        [-2597.2827, -2598.2869, -2596.4810, -2597.0398],
        [-2230.9734, -2233.8140, -2239.4941, -2233.8838],
        [-2740.9946, -2745.8560, -2739.9966, -2740.1704],
        [-2558.3218, -2558.0164, -2558.6150, -2558.0649],
        [-2522.7817, -2522.4956, -2546.5396, -2542.6824],
        [-2583.3589, -2581.7280, -2621.1523, -2587.1367],
        [-2686.1184, -2685.3057, -2683.9492, -2685.0518]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
Decoder Loss (Test) = tensor([[-2819.7341, -2817.4382, -2815.7461, -2821.6917],
        [-2635.1450, -2636.8267, -2643.2820, -2637.9604],
        [-2385.0762, -2377.5896, -2373.1064, -2373.3862],
        [-2185.3469, -2170.1545, -2159.9646, -2156.7222],
        [-2763.3887, -2761.2561, -2764.1211, -2759.7095],
        [-2729.0474, -2732.4800, -2767.8721, -2725.2856],
        [-2548.0269, -2548.0066, -2549.4031, -2548.3296],
        [-2626.1333, -2604.4561, -2641.8809, -2596.8149],
        [-2560.6743, -2542.7441, -2546.6318, -2544.7700],
        [-2488.0659, -2488.0259, -2477.3564, -2483.8555],
        [-2598.9871, -2564.9941, -2600.6658, -2557.2500],
        [-2542.4268, -2556.4956, -2560.0293, -2560.8633],
        [-2795.4846, -2796.8145, -2796.9697, -2793.2424],
        [-1984.0607, -1973.3203, -1963.8047, -1990.4866],
        [-2711.8123, -2712.0198, -2722.7095, -2714.0312],
        [-2626.1833, -2624.5557, -2687.1768, -2616.7527],
        [-2782.1174, -2779.5969, -2777.1633, -2779.9575],
        [-2772.8997, -2769.3345, -2769.7920, -2770.7368],
        [-2643.7783, -2663.5305, -2673.4443, -2682.9121],
        [-2684.5864, -2682.0354, -2676.3594, -2684.1201],
        [-2376.1401, -2371.1113, -2371.8013, -2373.6279],
        [-2047.8279, -2045.8208, -2042.9312, -2037.0349],
        [-2604.9529, -2599.0312, -2590.2136, -2594.8467],
        [-2561.6008, -2552.4663, -2548.6104, -2553.0757],
        [-2409.8228, -2425.3604, -2449.2578, -2416.7573],
        [-2250.4927, -2253.7300, -2251.8945, -2245.7058],
        [-2556.0532, -2601.6406, -2604.4648, -2574.9966],
        [-2694.7744, -2690.1060, -2693.1558, -2692.3850],
        [-2272.1482, -2279.2864, -2282.8625, -2279.0017],
        [-2653.6812, -2653.5383, -2647.1997, -2650.0669],
        [-2736.8064, -2735.8521, -2751.3713, -2732.6753],
        [-2628.4521, -2619.6877, -2612.5083, -2621.6042],
        [-2383.6797, -2380.9363, -2374.9800, -2380.8938],
        [-2672.6187, -2672.6777, -2672.3823, -2670.7490],
        [-2691.6345, -2692.9126, -2702.8953, -2702.2402],
        [-2756.3169, -2756.2805, -2791.8179, -2749.7832],
        [-2589.6011, -2584.4336, -2583.1011, -2587.5652],
        [-2630.5195, -2610.5557, -2656.6711, -2599.0693],
        [-2674.7617, -2671.0037, -2659.8381, -2666.5115],
        [-2676.8530, -2674.1445, -2667.9453, -2671.2109],
        [-2488.0137, -2481.5691, -2495.1980, -2490.9041],
        [-2519.0898, -2518.4258, -2524.9868, -2517.7317],
        [-2690.5796, -2679.9197, -2684.5872, -2686.2422],
        [-2725.9749, -2731.7996, -2737.9084, -2742.4185],
        [-2610.2295, -2608.9617, -2610.0151, -2608.3687],
        [-2680.0393, -2674.3628, -2668.2881, -2669.8782],
        [-2697.0222, -2693.6030, -2696.7280, -2697.8330],
        [-2451.8276, -2451.1162, -2452.5688, -2458.0085],
        [-2283.4209, -2276.4214, -2274.4097, -2274.4846],
        [-2767.5439, -2760.5005, -2762.2036, -2760.4238],
        [-2452.9038, -2452.8062, -2437.9617, -2451.5752],
        [-2588.3760, -2581.7764, -2587.1118, -2589.4336],
        [-2652.5842, -2648.3167, -2687.8286, -2647.0444],
        [-2679.8257, -2678.3467, -2678.2615, -2685.8528],
        [-2680.3755, -2678.9495, -2687.8564, -2678.5747],
        [-2791.3479, -2791.5786, -2814.0276, -2790.7969],
        [-2663.5403, -2663.9316, -2683.3401, -2673.8853],
        [-2152.7861, -2191.4277, -2122.9468, -2167.9187],
        [-2449.4766, -2453.4075, -2550.2180, -2393.6445],
        [-2604.9651, -2605.0415, -2592.9956, -2600.2000],
        [-2588.1331, -2588.7583, -2589.1465, -2588.3105],
        [-2827.4138, -2825.0710, -2832.4807, -2790.0955],
        [-2635.0222, -2639.4707, -2668.9624, -2630.6731],
        [-2739.4055, -2763.6921, -2770.9219, -2758.8931],
        [-2852.3760, -2855.2461, -2856.2251, -2853.8335],
        [-2256.6477, -2258.3655, -2246.2400, -2249.0173],
        [-2675.1982, -2638.4106, -2700.9961, -2627.8794],
        [-2671.1182, -2670.3547, -2670.5925, -2672.2207],
        [-2505.8906, -2514.0681, -2518.5854, -2516.7720],
        [-2786.9824, -2786.2695, -2789.4087, -2782.7188],
        [-2513.4780, -2534.1968, -2592.8760, -2545.2583],
        [-2678.8325, -2680.4800, -2680.0408, -2682.3621],
        [-2655.7407, -2653.0234, -2655.4756, -2653.1260],
        [-2602.3940, -2578.0190, -2635.4961, -2543.1345],
        [-2790.8127, -2798.5654, -2824.4307, -2807.1602],
        [-2447.4468, -2444.9326, -2444.3318, -2449.2061],
        [-2670.9167, -2670.1987, -2660.2739, -2672.9800],
        [-2659.5479, -2670.3357, -2655.8640, -2646.7603],
        [-2550.8442, -2549.2197, -2558.0557, -2549.3359],
        [-2724.8984, -2724.3638, -2740.5166, -2727.4004],
        [-2351.0684, -2357.6907, -2357.8228, -2375.9448],
        [-2475.1882, -2449.1343, -2438.1890, -2459.0342],
        [-2715.1956, -2705.8574, -2698.0203, -2701.9414],
        [-2681.4644, -2684.1553, -2693.9255, -2692.0083],
        [-2720.7942, -2714.5586, -2716.7378, -2721.1135],
        [-2745.9407, -2743.4290, -2748.8225, -2750.8630],
        [-2618.9058, -2506.7673, -2690.5723, -2438.5337],
        [-2072.0591, -2073.8088, -2053.5359, -2068.7512],
        [-2722.6802, -2720.7769, -2719.0176, -2718.6353],
        [-2214.7109, -2213.0356, -2192.5396, -2207.0610],
        [-2283.2158, -2285.6943, -2293.7905, -2283.8101],
        [-2849.4385, -2829.5581, -2839.0376, -2832.8721],
        [-2617.4819, -2622.1377, -2728.2017, -2592.2271],
        [-2386.8137, -2404.9238, -2391.3843, -2391.1240],
        [-2327.1890, -2332.4966, -2328.2563, -2327.9038],
        [-2608.5835, -2606.0894, -2612.2822, -2598.0410],
        [-2659.7104, -2633.7229, -2655.0647, -2626.9197],
        [-2415.7637, -2416.1907, -2421.9683, -2404.6172],
        [-2668.2324, -2667.8240, -2663.8428, -2670.7983],
        [-2699.7151, -2700.4946, -2701.5679, -2699.8867]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([100, 4, 5]), scale: torch.Size([100, 4, 5]))
=============================================================================================
