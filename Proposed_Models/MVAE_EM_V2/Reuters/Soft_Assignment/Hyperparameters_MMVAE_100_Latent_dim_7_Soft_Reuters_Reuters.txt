Dataset: Reuters
--------------------------------------------
Model Name: MMVAE_100_Latent_dim_7_Soft_Reuters
==================================================================
MMVAE_100_Latent_dim_7_Soft_Reuters Part:
==================================================================
VAE Part:
==================================================================
Random Seed = 100
--------------------------------------------
Device = cuda:0
--------------------------------------------
Number of Epochs = 2000
--------------------------------------------
Learning Rate = 0.0001
--------------------------------------------
Step Size = 20
--------------------------------------------
Input Dimension = 2000
--------------------------------------------
Encoder Hidden Dimension 1 = 500
--------------------------------------------
Encoder Hidden Dimension 2 = 500
--------------------------------------------
Encoder Hidden Dimension 3 = 2000
--------------------------------------------
Latent Dimension = 7
--------------------------------------------
Decoder Hidden Layer 1 = 2000
--------------------------------------------
Decoder Hidden Layer 2 = 500
--------------------------------------------
Decoder Hidden Layer 3 = 500
--------------------------------------------
Output Dimension = 2000
--------------------------------------------
Weight Decay = 1e-05
--------------------------------------------
Batch Size (Training) = 100
--------------------------------------------
Batch Size (Test) = 100
--------------------------------------------
gamma = 0.9
==================================================================

GMM Part:
==================================================================
Number of Components= 4
--------------------------------------------
Covariance Matrix= full
--------------------------------------------
Number of Iterations= 20
--------------------------------------------
Epsilon = 1e-10
--------------------------------------------
Precision = 28
==================================================================

Early Stopping:
--------------------------------------------
Maximum Patience = 2001
--------------------------------------------
Best Test Loss = inf
--------------------------------------------
Patience Counters = 0
==================================================================

Loss Functions:
==================================================================
Decoder Loss (Training) = tensor([[-2547.8354, -2562.7659, -2545.3977, -2512.0142],
        [-2361.6182, -3252.7495, -2883.5105, -3489.0486],
        [-2813.1396, -2856.9746, -3072.5728, -2590.0459],
        [-3296.0007, -2241.8882, -2362.9888, -2917.3970],
        [-3070.8242, -2576.5444, -2609.9297, -3005.8408],
        [-3090.4143, -2559.7266, -2856.3330, -2631.7046],
        [-2761.0359, -2785.7681, -2781.1143, -2804.9209],
        [-2992.1748, -2732.0049, -2939.6729, -2855.0586],
        [-2654.1807, -2953.7778, -2815.8005, -3073.6396],
        [-2904.8450, -2728.8455, -2779.4177, -2817.2026],
        [-2913.5552, -2526.5073, -2694.4263, -2859.1611],
        [-3074.3325, -2920.9448, -2492.9609, -3151.4307],
        [-3243.9165, -2813.4150, -2258.9995, -4065.5542],
        [-2503.8193, -3332.1494, -2783.7261, -3456.3567],
        [-3011.7917, -2666.1465, -2969.1960, -2756.7744],
        [-2875.3281, -2088.0435, -2339.0791, -3101.0864],
        [-2684.6328, -3022.3040, -2001.9205, -2952.9556],
        [-2999.7812, -2630.1309, -3016.9067, -2831.1245],
        [-2624.0703, -2665.6074, -2577.9490, -2889.2402],
        [-2499.0481, -2671.3030, -2698.7783, -2854.1436],
        [-2555.0518, -2856.9756, -2709.4082, -2979.2671],
        [-2865.0210, -2904.8535, -3081.9531, -2653.9407],
        [-2603.1384, -2881.2258, -2691.2749, -2790.0742],
        [-2572.8867, -2837.1487, -2670.7896, -2838.6990],
        [-3371.2920, -2862.5103, -2183.0029, -3363.6235],
        [-2520.1982, -3305.2568, -2408.1846, -2971.1655],
        [-2735.6689, -2841.3296, -2747.4937, -2860.7122],
        [-2890.3794, -2715.5288, -2834.1064, -2752.6584],
        [-2379.9658, -2469.2383, -2860.4395, -2130.4253],
        [-3266.4429, -2400.0015, -2960.3223, -3012.0732],
        [-2630.2810, -2711.3743, -2659.3794, -2825.3965],
        [-3050.3955, -2273.8110, -3156.0723, -2719.0923],
        [-2673.3950, -2826.9280, -2988.9404, -2484.6973],
        [-2847.7322, -3058.6011, -2958.1091, -2444.7498],
        [-2748.5215, -2809.6221, -2760.6455, -2388.8711],
        [-2926.0859, -3107.2402, -2896.7737, -2623.2266],
        [-3111.4297, -2232.5000, -2452.7559, -3144.1721],
        [-3066.2256, -2607.0020, -2858.2397, -2667.3818],
        [-2888.7900, -2571.2607, -3010.8203, -2679.8843],
        [-3139.0093, -2572.9277, -3251.8813, -2663.0103],
        [-2646.5991, -2744.4482, -2659.4329, -2612.9194],
        [-3020.8521, -2501.0100, -2818.2292, -2935.2061],
        [-2987.1982, -2592.1753, -2713.5022, -2700.1514],
        [-3186.3872, -2773.8301, -2292.4907, -3928.0940],
        [-2891.3628, -2898.3735, -2683.3047, -2174.4312],
        [-2852.9958, -2683.6055, -3064.5955, -2536.9165],
        [-2413.9299, -3217.2754, -2788.0159, -2877.5996],
        [-2779.6060, -2902.9648, -2930.2083, -2656.8809],
        [-2942.7048, -2374.3743, -2608.9763, -2849.8687],
        [-2345.6738, -2908.6489, -2470.1360, -3257.2065],
        [-2840.6431, -3215.7119, -2877.8936, -2618.9946],
        [-2711.0667, -2840.2690, -2827.1938, -2957.5583],
        [-2588.4165, -2731.7793, -2639.1833, -2787.2493],
        [-2929.2432, -3429.8638, -3674.3296, -2277.4082],
        [-2927.7979, -2989.2700, -3003.3457, -2077.9751],
        [-2487.2878, -2805.8564, -2548.4980, -2960.7080],
        [-2823.3699, -2761.9316, -2537.0544, -2658.6060],
        [-2792.9214, -3274.2070, -3294.1077, -2469.0547],
        [-2857.4248, -2020.5095, -2169.5000, -3213.4199],
        [-2636.5811, -2950.4067, -2838.6724, -2931.3545],
        [-3245.0942, -3262.8071, -2255.7366, -3619.9043],
        [-2795.1116, -2824.9878, -2811.2317, -2838.0713],
        [-2897.9800, -2604.0571, -2997.2979, -2661.0068],
        [-2939.9175, -3293.4260, -3092.6294, -2373.3923],
        [-2314.9297, -3280.1060, -2614.6606, -3914.8511],
        [-2941.2925, -2559.8928, -2633.4458, -2821.2129],
        [-2567.8877, -3127.9673, -2809.2090, -3008.9436],
        [-2909.5400, -3157.1570, -3193.9346, -2596.2808],
        [-3166.8601, -2589.7461, -3046.6367, -2732.0718],
        [-3142.5615, -2394.2134, -3374.9097, -2727.3477],
        [-2436.0137, -2606.2585, -2498.7964, -2710.8989],
        [-3207.9624, -2206.1357, -2901.7820, -2608.1462],
        [-3153.5686, -2955.6382, -2081.4170, -3211.4431],
        [-2830.9272, -2996.7959, -3441.6611, -2515.9810],
        [-3057.6511, -2564.2317, -2640.0784, -2863.3433],
        [-2458.4121, -3426.2061, -3037.6060, -2231.9336],
        [-2939.6680, -2683.4883, -2827.7720, -2740.1313],
        [-3136.5352, -2607.7676, -2900.7295, -2723.4473],
        [-2696.7949, -2434.3196, -2664.1970, -2644.5261],
        [-3084.2217, -2573.6616, -2975.3120, -2586.7969],
        [-2811.0913, -2829.3992, -2831.6145, -2535.4604],
        [-3178.7090, -2514.1035, -3113.6206, -2815.4771],
        [-2633.1365, -2886.9868, -2801.0908, -3092.1265],
        [-2786.0242, -3007.7344, -2650.0249, -2914.4592],
        [-2952.7505, -2577.1987, -2685.3335, -2729.2068],
        [-2832.8447, -3202.5356, -2944.2754, -2397.3096],
        [-2286.3972, -3172.7236, -2811.4946, -2751.6990],
        [-2952.7178, -2713.6426, -2857.8384, -2821.3027],
        [-2791.1802, -2846.8284, -2807.0955, -2852.8687],
        [-2834.1602, -3622.8594, -2881.9473, -2402.4194],
        [-2863.9883, -2621.1074, -2925.9463, -2610.9644],
        [-2750.0850, -2946.6987, -3137.2737, -2599.1921],
        [-2615.2043, -2967.3750, -2823.6328, -2865.6138],
        [-2826.0342, -2690.7830, -2938.7017, -2701.0000],
        [-3012.5903, -2616.1895, -2923.3721, -2718.2241],
        [-3192.4380, -2521.1909, -2740.5410, -2810.1387],
        [-3418.3958, -2966.9990, -2272.9575, -4307.5083],
        [-2828.2246, -2612.1931, -2660.5720, -2877.1689],
        [-2717.2861, -2812.0225, -2781.0540, -2854.0249],
        [-3111.9426, -2661.4692, -2923.8833, -2761.5837]], device='cuda:0',
       grad_fn=<SumBackward1>)
=============================================================================================
Encoder Loss (Training) = Normal(loc: torch.Size([100, 4, 7]), scale: torch.Size([100, 4, 7]))
=============================================================================================
Decoder Loss (Test) = tensor([[-2818.9722, -2806.3308, -2812.2507, -2813.0664],
        [-2857.1406, -2652.5962, -2930.0591, -2686.8174],
        [-3198.7339, -2635.8823, -2321.9453, -3176.2212],
        [-2423.9043, -2464.2266, -2741.1318, -2154.7612],
        [-2783.2427, -2794.3252, -2782.2061, -2785.5786],
        [-2720.0464, -2832.8181, -2764.5713, -2756.4766],
        [-2833.3882, -2520.3816, -2851.9836, -2587.3884],
        [-2564.6689, -3118.5430, -2832.7620, -3231.6133],
        [-2839.6782, -2946.3440, -3716.5303, -2504.6299],
        [-3169.9609, -2454.4551, -2473.6519, -3132.8179],
        [-2549.2573, -3027.7017, -2943.2405, -3267.6240],
        [-2873.9541, -3258.0591, -2960.6421, -2518.5220],
        [-2770.8237, -2833.7009, -2802.5066, -2958.9990],
        [-3095.9893, -1968.6686, -2620.3186, -3248.7046],
        [-2699.2654, -2733.7192, -2689.1455, -2738.3716],
        [-2641.0947, -2983.5649, -2740.4856, -2796.4062],
        [-2828.8384, -2850.8821, -2824.8340, -2757.3535],
        [-2815.0977, -2776.1584, -2817.6382, -2812.1982],
        [-2854.3865, -3188.0735, -2671.5088, -2594.4927],
        [-2686.4556, -2754.3584, -2688.1570, -2783.1423],
        [-3257.6260, -2373.9463, -2990.0430, -2828.1687],
        [-2949.0029, -2033.9453, -2178.4092, -3192.0352],
        [-2685.0010, -2903.5073, -3071.4653, -2568.4253],
        [-3085.5186, -2559.7959, -3236.8760, -2958.4697],
        [-2952.8101, -3579.7854, -3499.2854, -2417.4587],
        [-3251.8958, -2531.9919, -2242.5508, -3235.1855],
        [-2903.3523, -3205.0669, -3145.3665, -2505.6138],
        [-3137.5615, -2669.1230, -2739.1226, -2844.4441],
        [-3013.1025, -2485.7671, -2580.2275, -3020.1562],
        [-2786.1445, -2811.1855, -3077.2947, -2652.1758],
        [-2721.4409, -2843.7129, -2817.1406, -2990.6733],
        [-2939.6250, -2594.6455, -2682.2917, -2931.5317],
        [-3140.5317, -2376.0793, -3139.5293, -2872.7834],
        [-3127.0037, -2675.5081, -2970.6030, -2816.9900],
        [-2951.9038, -3101.0679, -3174.4038, -2590.3579],
        [-2675.4924, -2782.0947, -2721.5691, -2807.3384],
        [-3051.7417, -2568.5215, -2890.3877, -3012.3833],
        [-2624.1760, -2734.5210, -2686.3582, -3003.1489],
        [-2640.5146, -2790.1226, -2765.3145, -3179.3464],
        [-2600.3511, -2741.7280, -2639.8364, -2821.1279],
        [-3142.4062, -2652.2092, -2492.8489, -3400.0093],
        [-3117.0166, -3054.6890, -2571.3179, -3048.8867],
        [-2772.0044, -2679.0515, -2757.6812, -2682.9956],
        [-2863.8716, -2937.5715, -2956.0261, -2714.0474],
        [-2621.0229, -2618.3547, -2621.1267, -2620.7573],
        [-2970.3027, -2608.4280, -2893.9648, -2793.4202],
        [-2704.8828, -2717.6553, -2702.7659, -2767.0869],
        [-2577.2329, -2584.9849, -3024.2827, -2490.7668],
        [-3073.1079, -2276.8772, -3312.6348, -2977.5496],
        [-2755.5171, -2741.6633, -2737.1875, -2734.8914],
        [-2833.2026, -3225.4639, -3189.3057, -2441.1519],
        [-2787.7217, -2577.9150, -2621.6174, -2982.8906],
        [-2632.5938, -2799.0850, -2652.5981, -2808.7117],
        [-2882.5254, -3064.1934, -3140.4143, -2683.9233],
        [-2637.6948, -2752.1272, -2680.6628, -2708.0640],
        [-2752.9709, -2772.1221, -2757.4072, -2808.2251],
        [-2725.8540, -2929.6270, -2754.7393, -2654.6731],
        [-3091.2769, -3105.3577, -3252.2549, -2114.2061],
        [-3285.4160, -3073.4839, -2401.8901, -3450.1743],
        [-2529.4033, -3199.6104, -2871.5376, -3135.5005],
        [-2965.7583, -2598.5159, -2732.5452, -2730.6716],
        [-2737.3948, -2894.1411, -2805.7991, -2875.0781],
        [-2612.5103, -3045.4846, -2726.8813, -2979.4414],
        [-2957.2461, -2981.7212, -3020.1338, -2669.8633],
        [-2738.8257, -2935.0327, -2872.6846, -2860.7100],
        [-2955.7583, -3507.2351, -3015.5361, -2245.3672],
        [-2638.8506, -2749.4204, -2665.3113, -2945.5962],
        [-3043.2068, -2645.7441, -2857.1465, -2686.4871],
        [-3044.2539, -3270.1660, -3127.4067, -2481.0994],
        [-2835.1365, -2783.5825, -2835.9434, -2808.5608],
        [-2791.3770, -2990.0745, -3377.7361, -2517.1274],
        [-2785.3264, -2684.2175, -2765.5425, -2705.6367],
        [-3060.9343, -2663.7593, -2895.1638, -2777.7573],
        [-2552.1626, -2870.2473, -2670.1189, -2749.8164],
        [-2642.2197, -3081.1787, -2953.0159, -2587.8625],
        [-2674.9766, -2626.2573, -3047.9507, -2456.7393],
        [-2756.4641, -2895.6934, -2926.3149, -2646.7900],
        [-2881.3569, -2866.3433, -2962.1423, -2714.4810],
        [-2809.2471, -2832.4363, -3194.7285, -2592.8237],
        [-2734.9785, -2849.1313, -2765.3501, -2827.6665],
        [-2924.4651, -3381.8674, -3349.8154, -2284.0574],
        [-3070.8545, -2428.7622, -3126.7205, -2875.3113],
        [-2997.0017, -2895.3474, -2682.0688, -2924.9170],
        [-2835.5505, -3074.2847, -2730.6836, -2619.6089],
        [-2919.4009, -2577.8555, -2728.2285, -2828.2256],
        [-2728.1143, -2788.6875, -2789.0759, -2894.3066],
        [-2417.3433, -3098.3088, -2627.2354, -3227.7593],
        [-2409.2148, -2370.0132, -2733.8823, -2079.1155],
        [-3088.7246, -2854.2896, -2786.5359, -3300.1099],
        [-3296.4365, -2173.5344, -2717.2344, -3302.9692],
        [-3604.0076, -3403.1035, -2267.2485, -3676.9868],
        [-2870.0811, -2790.2996, -2835.6963, -2850.0410],
        [-2579.7935, -2871.0317, -2698.6699, -2825.8118],
        [-3197.2544, -3075.2480, -2463.3760, -3249.5889],
        [-3067.8445, -2823.4121, -2338.4436, -3733.1460],
        [-2599.3057, -2813.1831, -2761.4954, -3090.7515],
        [-2580.0002, -2943.2944, -2922.2080, -3040.6777],
        [-3219.4160, -2760.1814, -2389.7773, -3804.3330],
        [-2819.1831, -2742.2520, -2908.7939, -2677.6938],
        [-3287.8057, -2691.7405, -2921.9048, -2935.9143]], device='cuda:0')
=============================================================================================
Encoder Loss (Test) = Normal(loc: torch.Size([100, 4, 7]), scale: torch.Size([100, 4, 7]))
=============================================================================================
