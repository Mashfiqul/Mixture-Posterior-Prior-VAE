# Mixtures of variational autoencoder model

## Overview

This repository contains the code for my one of my submitted papers "Mixtures of posterior and prior variational autoencoders for representation learning and cluster analysis in latent space", which is currently under review. 

## Authors

Mashfiqul Huq Chowdhury.

## Supervisors

Yuichi Hirose, Stephen Marsland, Yuan Yao


## Datasets

The datasets used in this research (MNIST, Fashion-MNIST, USPS, and STL-10) are accessible through PyTorch deep learning frameworks. The Reuters and HAR dataset can be accessed in the Dataset folder. Digits dataset is available in Scikit-learn (sklearn) machine learning library in Python.

- HAR Dataset: https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones

- Reuters: https://github.com/slim1017/VaDE/tree/master/dataset/reuters10k


## Models

Our proposed mixtures VAE (MVAE), EM Version of Mixtures VAE (MVAE(EM-V1), MVAE(EM-V2)) and constrained mixtures VAE (Beta_MVAE(EM-V2), Scheduled Beta_MVAE(EM-V2)) can be accessed in 'Proposed_Models' folder. The evaluated models (VAE, VADE, k-DVAE) can be accessed in 'Evaluated_Models' folder.


### Contact

For additional code or information, please contact Mashfiqul Huq Chowdhury (Email: mashfiq@mbstu.ac.bd).



## Citation

If you use this code in your research, please cite the following paper:

```bash
@article{chowdhury2024deep,
  title={Mixtures of posterior and prior variational autoencoders for representation learning and cluster analysis in latent space},
  author={Chowdhury, Mashfiqul Huq},
  year={2024},
  journal={(Under Review)},
  note={Manuscript under review.},
}
```

## Note

This code is part of an ongoing research project, and the associated paper is currently under review. Feel free to reach out for any inquiries or collaboration opportunities.

Contributions to the repository are welcome, and any questions can be sent to mashfiq@mbstu.ac.bd.

We appreciate your interest and hope that this code proves valuable in your research endeavors.

Best regards,

The Authors


